{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cb1d97",
   "metadata": {},
   "source": [
    "# v 3.0 xgBoost with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "d3e22053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn import preprocessing\n",
    "import geopy\n",
    "from geopy.distance import geodesic\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import dask.dataframe as dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "d3ff9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../references')  # Add the references folder to the system path\n",
    "model_specs = 'xgBoost with SMOTE sacle_pos_weight=5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ab52f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_notebook = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "c56c046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save the figures \n",
    "\n",
    "input_src_dir = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/data/raw'\n",
    "output_dir_figures_train = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/figures/train_figures'\n",
    "output_dir_figures_test = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/figures/test_figures'\n",
    "reports_output_dir = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "e37d27bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    }
   ],
   "source": [
    "# Define which dataset to use\n",
    "use_test_data = False  # Set to True when using fraudtest.csv\n",
    "\n",
    "# Determine dataset type based on the variable\n",
    "dataset_type = 'Test' if use_test_data else 'Train'\n",
    "print(dataset_type)\n",
    "\n",
    "# Load the appropriate dataset\n",
    "\n",
    "if use_test_data:\n",
    "    output_dir_figures = output_dir_figures_test\n",
    "else:\n",
    "    output_dir_figures = output_dir_figures_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "bd0bc5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the preprocess file name dynamically\n",
    "# Get the current timestamp\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")  # Format: YYYYMMDD_HHMMSS\n",
    "\n",
    "logfile_title = 'LogFile'\n",
    "logfile_name = f\"{model_specs}_{dataset_type}_{logfile_title.replace(',', '').lower().split('.')[0]}_{timestamp}.txt\"\n",
    "\n",
    "logfile_path = os.path.join(reports_output_dir, logfile_name)\n",
    "\n",
    "# Function to log times to a file\n",
    "def log_time(step_name, start_time):\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    log_message = (f\"{step_name} completed at {time.ctime(end_time)}. \"\n",
    "                   f\"Elapsed time: {elapsed_time // 60:.0f} minutes and {elapsed_time % 60:.2f} seconds\\n\")\n",
    "    \n",
    "    # Append log to file\n",
    "    with open(logfile_path, 'a') as f:\n",
    "        f.write(log_message)\n",
    "    \n",
    "    # Print the message to the console as well\n",
    "    print(log_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "383fd5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgBoost with SMOTE sacle_pos_weight=5_Test Notebook  started at...  completed at Fri Nov  1 18:39:05 2024. Elapsed time: 0 minutes and 0.03 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(f\"{model_specs}_{dataset_type} Notebook  started at... \", start_time_notebook)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a17fc867",
   "metadata": {},
   "source": [
    "# Load the appropriate dataset\n",
    "\n",
    "if use_test_data:\n",
    "    df = pd.read_csv(f\"{input_src_dir}/fraudTest.csv\")  # Concatenate the directory with the filename\n",
    "else:\n",
    "    df = pd.read_csv(f\"{input_src_dir}/fraudTrain.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "0b58eee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the DASK Client completed at Fri Nov  1 18:39:05 2024. Elapsed time: 0 minutes and 0.01 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 53797 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import dask.dataframe as dd\n",
    "\n",
    "log_time(\"Starting the DASK Client\", start_time)\n",
    "\n",
    "# Start Dask client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "a5b76b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the dataset directly into Dask\n",
    "if use_test_data:\n",
    "    df_pre = dd.read_csv(f\"{input_src_dir}/fraudTest.csv\", assume_missing=True)\n",
    "else:\n",
    "    df_pre = dd.read_csv(f\"{input_src_dir}/fraudTrain.csv\", assume_missing=True)\n",
    "\n",
    "\n",
    "# Now proceed with preprocessing, feature engineering, and model training on `df`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "abe2bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 5\n",
      "Columns: Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud'],\n",
      "      dtype='object')\n",
      "   Unnamed: 0      TransactionTime  CreditCardNumber  \\\n",
      "0         0.0  2020-06-21 12:14:25      2.291164e+15   \n",
      "1         1.0  2020-06-21 12:14:33      3.573030e+15   \n",
      "2         2.0  2020-06-21 12:14:53      3.598215e+15   \n",
      "3         3.0  2020-06-21 12:15:15      3.591920e+15   \n",
      "4         4.0  2020-06-21 12:15:17      3.526826e+15   \n",
      "\n",
      "                               merchant        category  TransactionAmount  \\\n",
      "0                 fraud_Kirlin and Sons   personal_care               2.86   \n",
      "1                  fraud_Sporer-Keebler   personal_care              29.84   \n",
      "2  fraud_Swaniawski, Nitzsche and Welch  health_fitness              41.28   \n",
      "3                     fraud_Haley Group        misc_pos              60.05   \n",
      "4                 fraud_Johnston-Casper          travel               3.19   \n",
      "\n",
      "    first      last gender                       street  ...      lat  \\\n",
      "0    Jeff   Elliott      M            351 Darlene Green  ...  33.9659   \n",
      "1  Joanne  Williams      F             3638 Marsh Union  ...  40.3207   \n",
      "2  Ashley     Lopez      F         9333 Valentine Point  ...  40.6729   \n",
      "3   Brian  Williams      M  32941 Krystal Mill Apt. 552  ...  28.5697   \n",
      "4  Nathan    Massey      M     5783 Evan Roads Apt. 465  ...  44.2529   \n",
      "\n",
      "       long  city_pop                     job  DateOfBirth  \\\n",
      "0  -80.9355  333497.0     Mechanical engineer   1968-03-19   \n",
      "1 -110.4360     302.0  Sales professional, IT   1990-01-17   \n",
      "2  -73.5365   34496.0       Librarian, public   1970-10-21   \n",
      "3  -80.8191   54767.0            Set designer   1987-07-25   \n",
      "4  -85.0170    1126.0      Furniture designer   1955-07-06   \n",
      "\n",
      "                          trans_num     unix_time  merch_lat  merch_long  \\\n",
      "0  2da90c7d74bd46a0caf3777415b3ebd3  1.371817e+09  33.986391  -81.200714   \n",
      "1  324cc204407e99f51b0d6ca0055005e7  1.371817e+09  39.450498 -109.960431   \n",
      "2  c81755dbbbea9d5c77f094348a7579be  1.371817e+09  40.495810  -74.196111   \n",
      "3  2159175b9efe66dc301f149d3d5abf8c  1.371817e+09  28.812398  -80.883061   \n",
      "4  57ff021bd3f328f8738bb535c302a31b  1.371817e+09  44.959148  -85.884734   \n",
      "\n",
      "   is_fraud  \n",
      "0       0.0  \n",
      "1       0.0  \n",
      "2       0.0  \n",
      "3       0.0  \n",
      "4       0.0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Optionally repartition the dataset if necessary\n",
    "df_pre = df_pre.repartition(npartitions=5)\n",
    "\n",
    "\n",
    "# Strip whitespace from column names and rename\n",
    "df_pre.columns = df_pre.columns.str.strip()\n",
    "df_pre = df_pre.rename(columns={'amt': 'TransactionAmount', 'cc_num': 'CreditCardNumber', 'dob': 'DateOfBirth', 'trans_date_trans_time': 'TransactionTime'})\n",
    "\n",
    "# Repartition to ensure consistency\n",
    "df_pre = df_pre.repartition(npartitions=5)\n",
    "\n",
    "# Preview without converting to Pandas\n",
    "print(\"Number of partitions:\", df_pre.npartitions)\n",
    "print(\"Columns:\", df_pre.columns)\n",
    "print(df_pre.head())  # No .compute() here\n",
    "\n",
    "# Final transformations or computations\n",
    "# Only use .compute() at the very end if needed, for example:\n",
    "df = df_pre.compute()  # Converts to Pandas for the entire dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "6a11b824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud'],\n",
      "      dtype='object')\n",
      "(555719, 23)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9042c84c",
   "metadata": {},
   "source": [
    "df.columns = df.columns.str.strip()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6829be1",
   "metadata": {},
   "source": [
    "df = df.rename(columns={'amt': 'TransactionAmount', 'cc_num': 'CreditCardNumber', 'dob': 'DateOfBirth', 'trans_date_trans_time': 'TransactionTime'})\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7500a890",
   "metadata": {},
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "73a7c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas and generate TransactionID\n",
    "df = df_pre.compute()\n",
    "df = df.reset_index(drop=True)\n",
    "df['TransactionID'] = df.index + 1  # Unique TransactionID\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "5c847926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02327c04",
   "metadata": {},
   "source": [
    "#debug\n",
    "import pandas as pd\n",
    "df_check = pd.read_csv(f\"{input_src_dir}/fraudTrain.csv\")\n",
    "print(\"Original CSV Row Count:\", len(df_check))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96d16888",
   "metadata": {},
   "source": [
    "#debug\n",
    "df = dd.read_csv(f\"{input_src_dir}/fraudTrain.csv\", assume_missing=True)\n",
    "row_count = df.size.compute() // len(df.columns)  # Approximate row count\n",
    "print(\"Row count after loading:\", row_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "d3babc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " Unnamed: 0           0\n",
      "TransactionTime      0\n",
      "CreditCardNumber     0\n",
      "merchant             0\n",
      "category             0\n",
      "TransactionAmount    0\n",
      "first                0\n",
      "last                 0\n",
      "gender               0\n",
      "street               0\n",
      "city                 0\n",
      "state                0\n",
      "zip                  0\n",
      "lat                  0\n",
      "long                 0\n",
      "city_pop             0\n",
      "job                  0\n",
      "DateOfBirth          0\n",
      "trans_num            0\n",
      "unix_time            0\n",
      "merch_lat            0\n",
      "merch_long           0\n",
      "is_fraud             0\n",
      "TransactionID        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values)\n",
    "\n",
    "# no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "5dc29333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_fraud\n",
      "0.0    553574\n",
      "1.0      2145\n",
      "Name: count, dtype: int64\n",
      "is_fraud\n",
      "0.0    99.614014\n",
      "1.0     0.385986\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Count of fraud and non-fraud transactions\n",
    "fraud_counts = df['is_fraud'].value_counts()\n",
    "print(fraud_counts)\n",
    "\n",
    "# Optionally, you can get it in percentage terms\n",
    "fraud_percentage = df['is_fraud'].value_counts(normalize=True) * 100\n",
    "print(fraud_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "5c60dd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "924"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many unique credit cards in the data set ??\n",
    "df['CreditCardNumber'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "391577c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "75b3c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TransactionTime to datetime\n",
    "df['TransactionTime'] = pd.to_datetime(df['TransactionTime'])\n",
    "\n",
    "# Optional: Convert DateOfBirth to datetime, if needed\n",
    "df['DateOfBirth'] = pd.to_datetime(df['DateOfBirth'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "52941f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2020-06-21 12:14:25', '2020-06-21 12:14:33',\n",
      "               '2020-06-21 12:14:53', '2020-06-21 12:15:15',\n",
      "               '2020-06-21 12:15:17', '2020-06-21 12:15:37',\n",
      "               '2020-06-21 12:15:44', '2020-06-21 12:15:50',\n",
      "               '2020-06-21 12:16:10', '2020-06-21 12:16:11',\n",
      "               ...\n",
      "               '2020-12-31 23:57:18', '2020-12-31 23:57:50',\n",
      "               '2020-12-31 23:57:56', '2020-12-31 23:58:04',\n",
      "               '2020-12-31 23:58:34', '2020-12-31 23:59:07',\n",
      "               '2020-12-31 23:59:09', '2020-12-31 23:59:15',\n",
      "               '2020-12-31 23:59:24', '2020-12-31 23:59:34'],\n",
      "              dtype='datetime64[ns]', name='TransactionTime', length=555719, freq=None)\n"
     ]
    }
   ],
   "source": [
    "# Set 'TransactionTime' as the index permanently\n",
    "df.set_index('TransactionTime', inplace=True)\n",
    "\n",
    "# Verify the index\n",
    "print(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "55397673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Transaction Time: 2020-06-21 12:14:25\n",
      "Maximum Transaction Time: 2020-12-31 23:59:34\n"
     ]
    }
   ],
   "source": [
    "# Get the minimum and maximum transaction times from the index\n",
    "min_time = df.index.min()\n",
    "max_time = df.index.max()\n",
    "\n",
    "print(f\"Minimum Transaction Time: {min_time}\")\n",
    "print(f\"Maximum Transaction Time: {max_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "a1fc7ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CreditCardNumber</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>TransactionAmount</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>DateOfBirth</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>TransactionID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-06-21 12:14:25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.291164e+15</td>\n",
       "      <td>fraud_Kirlin and Sons</td>\n",
       "      <td>personal_care</td>\n",
       "      <td>2.86</td>\n",
       "      <td>Jeff</td>\n",
       "      <td>Elliott</td>\n",
       "      <td>M</td>\n",
       "      <td>351 Darlene Green</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>...</td>\n",
       "      <td>-80.9355</td>\n",
       "      <td>333497.0</td>\n",
       "      <td>Mechanical engineer</td>\n",
       "      <td>1968-03-19</td>\n",
       "      <td>2da90c7d74bd46a0caf3777415b3ebd3</td>\n",
       "      <td>1.371817e+09</td>\n",
       "      <td>33.986391</td>\n",
       "      <td>-81.200714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-21 12:14:33</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.573030e+15</td>\n",
       "      <td>fraud_Sporer-Keebler</td>\n",
       "      <td>personal_care</td>\n",
       "      <td>29.84</td>\n",
       "      <td>Joanne</td>\n",
       "      <td>Williams</td>\n",
       "      <td>F</td>\n",
       "      <td>3638 Marsh Union</td>\n",
       "      <td>Altonah</td>\n",
       "      <td>...</td>\n",
       "      <td>-110.4360</td>\n",
       "      <td>302.0</td>\n",
       "      <td>Sales professional, IT</td>\n",
       "      <td>1990-01-17</td>\n",
       "      <td>324cc204407e99f51b0d6ca0055005e7</td>\n",
       "      <td>1.371817e+09</td>\n",
       "      <td>39.450498</td>\n",
       "      <td>-109.960431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-21 12:14:53</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.598215e+15</td>\n",
       "      <td>fraud_Swaniawski, Nitzsche and Welch</td>\n",
       "      <td>health_fitness</td>\n",
       "      <td>41.28</td>\n",
       "      <td>Ashley</td>\n",
       "      <td>Lopez</td>\n",
       "      <td>F</td>\n",
       "      <td>9333 Valentine Point</td>\n",
       "      <td>Bellmore</td>\n",
       "      <td>...</td>\n",
       "      <td>-73.5365</td>\n",
       "      <td>34496.0</td>\n",
       "      <td>Librarian, public</td>\n",
       "      <td>1970-10-21</td>\n",
       "      <td>c81755dbbbea9d5c77f094348a7579be</td>\n",
       "      <td>1.371817e+09</td>\n",
       "      <td>40.495810</td>\n",
       "      <td>-74.196111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-21 12:15:15</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.591920e+15</td>\n",
       "      <td>fraud_Haley Group</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>60.05</td>\n",
       "      <td>Brian</td>\n",
       "      <td>Williams</td>\n",
       "      <td>M</td>\n",
       "      <td>32941 Krystal Mill Apt. 552</td>\n",
       "      <td>Titusville</td>\n",
       "      <td>...</td>\n",
       "      <td>-80.8191</td>\n",
       "      <td>54767.0</td>\n",
       "      <td>Set designer</td>\n",
       "      <td>1987-07-25</td>\n",
       "      <td>2159175b9efe66dc301f149d3d5abf8c</td>\n",
       "      <td>1.371817e+09</td>\n",
       "      <td>28.812398</td>\n",
       "      <td>-80.883061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-21 12:15:17</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.526826e+15</td>\n",
       "      <td>fraud_Johnston-Casper</td>\n",
       "      <td>travel</td>\n",
       "      <td>3.19</td>\n",
       "      <td>Nathan</td>\n",
       "      <td>Massey</td>\n",
       "      <td>M</td>\n",
       "      <td>5783 Evan Roads Apt. 465</td>\n",
       "      <td>Falmouth</td>\n",
       "      <td>...</td>\n",
       "      <td>-85.0170</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>Furniture designer</td>\n",
       "      <td>1955-07-06</td>\n",
       "      <td>57ff021bd3f328f8738bb535c302a31b</td>\n",
       "      <td>1.371817e+09</td>\n",
       "      <td>44.959148</td>\n",
       "      <td>-85.884734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Unnamed: 0  CreditCardNumber  \\\n",
       "TransactionTime                                     \n",
       "2020-06-21 12:14:25         0.0      2.291164e+15   \n",
       "2020-06-21 12:14:33         1.0      3.573030e+15   \n",
       "2020-06-21 12:14:53         2.0      3.598215e+15   \n",
       "2020-06-21 12:15:15         3.0      3.591920e+15   \n",
       "2020-06-21 12:15:17         4.0      3.526826e+15   \n",
       "\n",
       "                                                 merchant        category  \\\n",
       "TransactionTime                                                             \n",
       "2020-06-21 12:14:25                 fraud_Kirlin and Sons   personal_care   \n",
       "2020-06-21 12:14:33                  fraud_Sporer-Keebler   personal_care   \n",
       "2020-06-21 12:14:53  fraud_Swaniawski, Nitzsche and Welch  health_fitness   \n",
       "2020-06-21 12:15:15                     fraud_Haley Group        misc_pos   \n",
       "2020-06-21 12:15:17                 fraud_Johnston-Casper          travel   \n",
       "\n",
       "                     TransactionAmount   first      last gender  \\\n",
       "TransactionTime                                                   \n",
       "2020-06-21 12:14:25               2.86    Jeff   Elliott      M   \n",
       "2020-06-21 12:14:33              29.84  Joanne  Williams      F   \n",
       "2020-06-21 12:14:53              41.28  Ashley     Lopez      F   \n",
       "2020-06-21 12:15:15              60.05   Brian  Williams      M   \n",
       "2020-06-21 12:15:17               3.19  Nathan    Massey      M   \n",
       "\n",
       "                                          street        city  ...      long  \\\n",
       "TransactionTime                                               ...             \n",
       "2020-06-21 12:14:25            351 Darlene Green    Columbia  ...  -80.9355   \n",
       "2020-06-21 12:14:33             3638 Marsh Union     Altonah  ... -110.4360   \n",
       "2020-06-21 12:14:53         9333 Valentine Point    Bellmore  ...  -73.5365   \n",
       "2020-06-21 12:15:15  32941 Krystal Mill Apt. 552  Titusville  ...  -80.8191   \n",
       "2020-06-21 12:15:17     5783 Evan Roads Apt. 465    Falmouth  ...  -85.0170   \n",
       "\n",
       "                     city_pop                     job  DateOfBirth  \\\n",
       "TransactionTime                                                      \n",
       "2020-06-21 12:14:25  333497.0     Mechanical engineer   1968-03-19   \n",
       "2020-06-21 12:14:33     302.0  Sales professional, IT   1990-01-17   \n",
       "2020-06-21 12:14:53   34496.0       Librarian, public   1970-10-21   \n",
       "2020-06-21 12:15:15   54767.0            Set designer   1987-07-25   \n",
       "2020-06-21 12:15:17    1126.0      Furniture designer   1955-07-06   \n",
       "\n",
       "                                            trans_num     unix_time  \\\n",
       "TransactionTime                                                       \n",
       "2020-06-21 12:14:25  2da90c7d74bd46a0caf3777415b3ebd3  1.371817e+09   \n",
       "2020-06-21 12:14:33  324cc204407e99f51b0d6ca0055005e7  1.371817e+09   \n",
       "2020-06-21 12:14:53  c81755dbbbea9d5c77f094348a7579be  1.371817e+09   \n",
       "2020-06-21 12:15:15  2159175b9efe66dc301f149d3d5abf8c  1.371817e+09   \n",
       "2020-06-21 12:15:17  57ff021bd3f328f8738bb535c302a31b  1.371817e+09   \n",
       "\n",
       "                     merch_lat  merch_long  is_fraud  TransactionID  \n",
       "TransactionTime                                                      \n",
       "2020-06-21 12:14:25  33.986391  -81.200714       0.0              1  \n",
       "2020-06-21 12:14:33  39.450498 -109.960431       0.0              2  \n",
       "2020-06-21 12:14:53  40.495810  -74.196111       0.0              3  \n",
       "2020-06-21 12:15:15  28.812398  -80.883061       0.0              4  \n",
       "2020-06-21 12:15:17  44.959148  -85.884734       0.0              5  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "10777762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Steps Completed File Loading, Describe, Date Conversions etc..   completed at Fri Nov  1 18:39:23 2024. Elapsed time: 0 minutes and 17.79 seconds\n",
      "\n",
      "--------------------------------------------------- ------------------   completed at Fri Nov  1 18:39:23 2024. Elapsed time: 0 minutes and 17.79 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Initial Steps Completed File Loading, Describe, Date Conversions etc..  \", start_time)\n",
    "log_time(\"--------------------------------------------------- ------------------  \", start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b32d0d",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "ca771ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log pre-process time at various steps\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "a2a6ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START - Feature Engineering .....   completed at Fri Nov  1 18:39:23 2024. Elapsed time: 0 minutes and 0.01 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"START - Feature Engineering .....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "31252f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clip outliers if necessary\n",
    "df['TransactionAmount'] = df['TransactionAmount'].clip(upper=df['TransactionAmount'].quantile(0.99))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "bf9e3c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/1075701793.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['TransactionAmount'].replace([np.inf, -np.inf], np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace inf values with NaN (in case they exist in the 'TransactionAmount' column)\n",
    "df['TransactionAmount'].replace([np.inf, -np.inf], np.nan, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b21308e",
   "metadata": {},
   "source": [
    "# next type of VIZ via transaction id vs transaction count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "70594f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour from TransactionTime\n",
    "df['Hour'] = df.index.hour  # Since TransactionTime is already set as the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "ea729e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Risk Hours: [22, 23, 3, 0, 2, 1]\n",
      "                     Hour  HighRiskHour\n",
      "TransactionTime                        \n",
      "2020-06-21 12:14:25    12             0\n",
      "2020-06-21 12:14:33    12             0\n",
      "2020-06-21 12:14:53    12             0\n",
      "2020-06-21 12:15:15    12             0\n",
      "2020-06-21 12:15:17    12             0\n",
      "...                   ...           ...\n",
      "2020-12-31 23:59:07    23             1\n",
      "2020-12-31 23:59:09    23             1\n",
      "2020-12-31 23:59:15    23             1\n",
      "2020-12-31 23:59:24    23             1\n",
      "2020-12-31 23:59:34    23             1\n",
      "\n",
      "[555719 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate fraud rate by hour\n",
    "fraud_rate_by_hour = df.groupby('Hour')['is_fraud'].mean()\n",
    "\n",
    "# Sort by fraud rate in descending order\n",
    "fraud_rate_by_hour = fraud_rate_by_hour.sort_values(ascending=False)\n",
    "\n",
    "# Define a threshold for high-risk hours (adjust as needed)\n",
    "threshold = fraud_rate_by_hour.mean()  # Mean fraud rate across all hours\n",
    "\n",
    "# Dynamically identify high-risk hours based on the threshold\n",
    "high_risk_hours = fraud_rate_by_hour[fraud_rate_by_hour > threshold].index.tolist()\n",
    "\n",
    "# Print high-risk hours for reference\n",
    "print(\"High-Risk Hours:\", high_risk_hours)\n",
    "\n",
    "# Create the HighRiskHour flag based on dynamically identified high-risk hours\n",
    "df['HighRiskHour'] = df['Hour'].apply(lambda x: 1 if x in high_risk_hours else 0)\n",
    "\n",
    "# Print a sample of the DataFrame to verify the new column\n",
    "print(df[['Hour', 'HighRiskHour']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75736fa",
   "metadata": {},
   "source": [
    "1. Time-Based Analysis:\n",
    "Already explored daily and hourly trends in transaction volumes, but now dive deeper into fraud patterns based on time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "a61c0eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Weekday vs. Weekend: Is fraud more common on weekdays or weekends?\n",
    "df['DayOfWeek'] = df.index.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "fraud_by_day = df[df['is_fraud'] == 1]['DayOfWeek'].value_counts().sort_index()\n",
    "non_fraud_by_day = df[df['is_fraud'] == 0]['DayOfWeek'].value_counts().sort_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "f0d77fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the correct day order\n",
    "day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "\n",
    "\n",
    "df['DayName'] = df.index.day_name()\n",
    "# Convert the 'DayName' column to a categorical type with the correct order\n",
    "df['DayName'] = pd.Categorical(df['DayName'], categories=day_order, ordered=True)\n",
    "\n",
    "fraud_by_day = df[df['is_fraud'] == 1]['DayName'].value_counts().sort_index()\n",
    "non_fraud_by_day = df[df['is_fraud'] == 0]['DayName'].value_counts().sort_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "3edc7f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of fraud on weekends: 29.84%\n",
      "Percentage of non-fraud on weekends: 27.95%\n"
     ]
    }
   ],
   "source": [
    "df['IsWeekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "weekend_fraud = df[df['is_fraud'] == 1]['IsWeekend'].mean()\n",
    "weekend_non_fraud = df[df['is_fraud'] == 0]['IsWeekend'].mean()\n",
    "\n",
    "print(f\"Percentage of fraud on weekends: {weekend_fraud * 100:.2f}%\")\n",
    "print(f\"Percentage of non-fraud on weekends: {weekend_non_fraud * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "67a62065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part1 - TrxAmount, Hour, DayOfWeeek etc.. completed at Fri Nov  1 18:39:24 2024. Elapsed time: 0 minutes and 1.13 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part1 - TrxAmount, Hour, DayOfWeeek etc..\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b51d0f2e",
   "metadata": {},
   "source": [
    "# Calculate distance between cardholder and merchant\n",
    "df['distance'] = df.apply(lambda row: geodesic((row['lat'], row['long']), (row['merch_lat'], row['merch_long'])).km, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "1f04c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'v_2.0_RandomForest_Credit_Card_Fraud_Detection.ipynb', 'v_2.1_RandomForest_Balanced_SMOTE_Credit_Card_Fraud_Detection.ipynb', 'v_2.2_RandomForest_Balanced_SMOTE_GridSearch_Credit_Card_Fraud_Detection.ipynb', 'v_0.0_LogisticRegression_Credit_Card_Fraud_Detection.ipynb', '.gitkeep', 'v_2.1_RandomForest_Balanced_SMOTE_Credit_Card_Fraud_Detection.ipynb copy', '__pycache__', 'v_0.1_LogisticRegression_Balanced_Credit_Card_Fraud_Detection.ipynb', '.ipynb_checkpoints', 'v_3.1_xgBoost_Credit_Card_Fraud_Detection.ipynb', 'v_1.0_DecisionTrees_Credit_Card_Fraud_Detection.ipynb', 'v_1.1_DecisionTrees_Credit_Card_Fraud_Detection.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())  # List all files in the current directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a9514",
   "metadata": {},
   "source": [
    "# MULTIPROCESSING : distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "40c0bbf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part2 - Distance Calculation with Multiprocessing (4 cores) completed at Fri Nov  1 18:40:04 2024. Elapsed time: 0 minutes and 39.93 seconds\n",
      "\n",
      "                         lat      long  merch_lat  merch_long    distance\n",
      "TransactionTime                                                          \n",
      "2020-06-21 12:14:25  33.9659  -80.9355  33.986391  -81.200714   24.613746\n",
      "2020-06-21 12:14:33  40.3207 -110.4360  39.450498 -109.960431  104.834043\n",
      "2020-06-21 12:14:53  40.6729  -73.5365  40.495810  -74.196111   59.204796\n",
      "2020-06-21 12:15:15  28.5697  -80.8191  28.812398  -80.883061   27.615117\n",
      "2020-06-21 12:15:17  44.2529  -85.0170  44.959148  -85.884734  104.423175\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from distance_calculation import calculate_distance_chunk\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Add the current working directory to the system path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Multiprocessing function to split the dataframe and apply the distance calculation\n",
    "def parallel_distance_calculation(df, num_partitions=None):\n",
    "    if num_partitions is None:\n",
    "        num_partitions = mp.cpu_count()  # Use all available CPU cores\n",
    "    \n",
    "    # Split the dataframe into chunks\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    \n",
    "    # Create a multiprocessing Pool\n",
    "    with mp.Pool(num_partitions) as pool:\n",
    "        # Apply the calculate_distance_chunk function to each chunk in parallel\n",
    "        result = pool.map(calculate_distance_chunk, df_split)\n",
    "    \n",
    "    # Concatenate the results back into a single dataframe\n",
    "    return pd.concat(result)\n",
    "\n",
    "# Main block to ensure multiprocessing works correctly\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Assuming df has the columns ['lat', 'long', 'merch_lat', 'merch_long']\n",
    "    \n",
    "    # Run with limited number of cores (e.g., 4 cores)\n",
    "    df = parallel_distance_calculation(df, num_partitions=4)  # Use 4 cores instead of all available cores\n",
    "\n",
    "    # Log the time taken for distance calculation with multiprocessing\n",
    "    log_time(\"Part2 - Distance Calculation with Multiprocessing (4 cores)\", start_time)\n",
    "\n",
    "    # Check the first few rows to verify the result\n",
    "    print(df[['lat', 'long', 'merch_lat', 'merch_long', 'distance']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "7c72a4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/Desktop/coding/ZHAW_Project/ML_BigData_Repo_1/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())  # This will print the current working directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f985dcd",
   "metadata": {},
   "source": [
    "log_time(\"Part2 -  Distance Calculation\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "50a155e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check unique values in the 'is_fraud' column\n",
    "df['is_fraud'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "09d464f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud vs Non-Fraud by Merchant Category\n",
    "fraud_by_category = df[df['is_fraud'] == 1]['category'].value_counts().head(10)\n",
    "non_fraud_by_category = df[df['is_fraud'] == 0]['category'].value_counts().head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "07ec224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Fraudulent Merchant Categories: ['shopping_net', 'grocery_pos', 'misc_net', 'shopping_pos', 'gas_transport']\n"
     ]
    }
   ],
   "source": [
    "# Top 5 categories with the highest fraud counts\n",
    "top_fraud_merchant_categories = df[df['is_fraud'] == 1]['category'].value_counts().head(5).index.tolist()\n",
    "\n",
    "# Print top fraudulent categories\n",
    "print(\"Top Fraudulent Merchant Categories:\", top_fraud_merchant_categories)\n",
    "\n",
    "# Create HighRiskMerchantCategory flag\n",
    "df['HighRiskMerchantCategory'] = df['category'].apply(lambda x: 1 if x in top_fraud_merchant_categories else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "7780b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HighRiskMerchantCategory\n",
      "0    327859\n",
      "1    227860\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the count of 1s and 0s in HighRiskMerchantCategory\n",
    "print(df['HighRiskMerchantCategory'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df499da3",
   "metadata": {},
   "source": [
    "# Potential Additional Features:\n",
    "Transaction Frequency:\n",
    "    Feature: How often a credit card has been used within a specific time frame (e.g., last hour or day).\n",
    "    Why: Fraudsters often make rapid successive transactions within short periods. You could create a rolling window to calculate transaction frequency.\n",
    "    How: You could calculate the number of transactions within the past X hours/days using a rolling window on the TransactionTime feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828d3a0e",
   "metadata": {},
   "source": [
    "#age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "221b8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure 'DateOfBirth' is in datetime format\n",
    "df['DateOfBirth'] = pd.to_datetime(df['DateOfBirth'], errors='coerce')  # Handle errors during conversion\n",
    "\n",
    "# Step 1: Calculate Age\n",
    "# Calculate age in years\n",
    "df['Age'] = (pd.Timestamp.now() - df['DateOfBirth']).dt.days // 365  # Age in years\n",
    "\n",
    "# Step 2: Create Age Groups\n",
    "# Define age bins and labels\n",
    "bins = [0, 18, 25, 35, 45, 55, 65, 100]  # Define your age bins, ensuring to cover all possible ages\n",
    "labels = ['0-18', '19-25', '26-35', '36-45', '46-55', '56-65', '66+']  # Corresponding labels\n",
    "\n",
    "# Create age group feature, include NaN values handling\n",
    "df['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "\n",
    "# Verify the new features without truncating DataFrame\n",
    "#print(df[['DateOfBirth', 'Age', 'AgeGroup']].head(10))  # Display the first 10 entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "296a6aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part3 - Merchant Categories & Age group completed at Fri Nov  1 18:40:05 2024. Elapsed time: 0 minutes and 40.51 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part3 - Merchant Categories & Age group\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19dc4c6f",
   "metadata": {},
   "source": [
    "RapidTransactionFlag"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e07efaf",
   "metadata": {},
   "source": [
    "def count_transactions_within_last_hour(group):\n",
    "    # Create an empty list to hold the frequencies\n",
    "    frequencies = []\n",
    "    \n",
    "    # Loop over each transaction time in the group\n",
    "    for time in group.index:\n",
    "        # Count the number of transactions within the last hour\n",
    "        count = group[(group.index >= (time - pd.Timedelta(hours=1))) & (group.index <= time)].shape[0]\n",
    "        frequencies.append(count)\n",
    "    \n",
    "    return frequencies\n",
    "\n",
    "# Apply the function to each group\n",
    "df['TransactionFrequency'] = df.groupby('CreditCardNumber').apply(count_transactions_within_last_hour).reset_index(drop=True)\n",
    "print(df[['TransactionFrequency']].head(10))  # Display the first 10 entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc72de",
   "metadata": {},
   "source": [
    "# MULTIPROCESSING : count_transactions_within_last_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "620555a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part4 - TransactionFrequency Multiprocessing completed at Fri Nov  1 18:41:14 2024. Elapsed time: 1 minutes and 8.60 seconds\n",
      "\n",
      "                    TransactionFrequency\n",
      "TransactionTime                         \n",
      "2020-06-21 12:14:25                  NaN\n",
      "2020-06-21 12:14:33                  NaN\n",
      "2020-06-21 12:14:53                  NaN\n",
      "2020-06-21 12:15:15                  NaN\n",
      "2020-06-21 12:15:17                  NaN\n",
      "2020-06-21 12:15:37                  NaN\n",
      "2020-06-21 12:15:44                  NaN\n",
      "2020-06-21 12:15:50                  NaN\n",
      "2020-06-21 12:16:10                  NaN\n",
      "2020-06-21 12:16:11                  NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import time\n",
    "from transaction_frequency import process_chunk  # Import from the .py file\n",
    "\n",
    "# Multiprocessing function to parallelize the transaction counting\n",
    "def parallel_count_transactions(df, num_partitions=None):\n",
    "    if num_partitions is None:\n",
    "        num_partitions = mp.cpu_count()  # Use all available CPU cores\n",
    "    \n",
    "    # Ensure the index is a datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Split the dataframe into chunks based on the number of partitions (CPU cores)\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    \n",
    "    # Create a multiprocessing Pool\n",
    "    with mp.Pool(num_partitions) as pool:\n",
    "        # Apply the processing function to each chunk in parallel\n",
    "        result = pool.map(process_chunk, df_split)\n",
    "    \n",
    "    # Combine the results from each chunk into a single series, reset index for consistency\n",
    "    return pd.concat(result).reset_index(drop=True)\n",
    "\n",
    "# Assuming df has 'CreditCardNumber' as a column and transaction times are indexed\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Apply the parallel processing for transaction frequency counting\n",
    "    df['TransactionFrequency'] = parallel_count_transactions(df, num_partitions=4)  # Adjust num_partitions as needed\n",
    "\n",
    "    # Log the time taken for transaction frequency calculation with multiprocessing\n",
    "    log_time(\"Part4 - TransactionFrequency Multiprocessing\", start_time)\n",
    "\n",
    "    # Check the first 10 rows\n",
    "    print(df[['TransactionFrequency']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "f3e571f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2020-06-21 12:14:25', '2020-06-21 12:14:33',\n",
      "               '2020-06-21 12:14:53', '2020-06-21 12:15:15',\n",
      "               '2020-06-21 12:15:17', '2020-06-21 12:15:37',\n",
      "               '2020-06-21 12:15:44', '2020-06-21 12:15:50',\n",
      "               '2020-06-21 12:16:10', '2020-06-21 12:16:11',\n",
      "               ...\n",
      "               '2020-12-31 23:57:18', '2020-12-31 23:57:50',\n",
      "               '2020-12-31 23:57:56', '2020-12-31 23:58:04',\n",
      "               '2020-12-31 23:58:34', '2020-12-31 23:59:07',\n",
      "               '2020-12-31 23:59:09', '2020-12-31 23:59:15',\n",
      "               '2020-12-31 23:59:24', '2020-12-31 23:59:34'],\n",
      "              dtype='datetime64[ns]', name='TransactionTime', length=555719, freq=None)\n"
     ]
    }
   ],
   "source": [
    "df.index = pd.to_datetime(df.index)\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96249ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "dadb160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/1490275857.py:2: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  transaction_counts_hourly = df.resample('H').size()\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/1490275857.py:6: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  transaction_counts = df.groupby('CreditCardNumber').resample('H').size().reset_index(name='TransactionCount')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CreditCardNumber     TransactionTime  TransactionCount\n",
      "0      6.041621e+10 2020-06-21 13:00:00                 1\n",
      "1      6.041621e+10 2020-06-21 14:00:00                 0\n",
      "2      6.041621e+10 2020-06-21 15:00:00                 0\n",
      "3      6.041621e+10 2020-06-21 16:00:00                 1\n",
      "4      6.041621e+10 2020-06-21 17:00:00                 0\n",
      "5      6.041621e+10 2020-06-21 18:00:00                 0\n",
      "6      6.041621e+10 2020-06-21 19:00:00                 0\n",
      "7      6.041621e+10 2020-06-21 20:00:00                 0\n",
      "8      6.041621e+10 2020-06-21 21:00:00                 0\n",
      "9      6.041621e+10 2020-06-21 22:00:00                 0\n"
     ]
    }
   ],
   "source": [
    "# Resample the data to count transactions every hour\n",
    "transaction_counts_hourly = df.resample('H').size()\n",
    "transaction_counts_daily = df.resample('D').size()\n",
    "\n",
    "# Combine with CreditCardNumber if necessary\n",
    "transaction_counts = df.groupby('CreditCardNumber').resample('H').size().reset_index(name='TransactionCount')\n",
    "print(transaction_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "97c19713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CreditCardNumber  TotalTransactionCount\n",
      "0      6.041621e+10                    678\n",
      "1      6.042293e+10                    669\n",
      "2      6.042310e+10                    228\n",
      "3      6.042785e+10                    215\n",
      "4      6.048700e+10                    239\n",
      "5      6.049060e+10                    455\n",
      "6      6.049559e+10                    224\n",
      "7      5.018030e+11                    635\n",
      "8      5.018282e+11                    218\n",
      "9      5.018311e+11                    439\n"
     ]
    }
   ],
   "source": [
    "total_transactions = df.groupby('CreditCardNumber').size().reset_index(name='TotalTransactionCount')\n",
    "print(total_transactions.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "aef71d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-21    1908\n",
      "2020-06-22    3834\n",
      "2020-06-23    3602\n",
      "2020-06-24    1352\n",
      "2020-06-25    1578\n",
      "              ... \n",
      "2020-12-27    5183\n",
      "2020-12-28    6212\n",
      "2020-12-29    6123\n",
      "2020-12-30    2630\n",
      "2020-12-31    3044\n",
      "Length: 194, dtype: int64\n",
      "          Unnamed: 0  CreditCardNumber  TransactionAmount            zip  \\\n",
      "count  478827.000000      4.788270e+05      478827.000000  478827.000000   \n",
      "mean   282352.474282      4.156977e+17          63.671204   48855.657540   \n",
      "min         1.000000      6.041621e+10           1.000000    1257.000000   \n",
      "25%    139140.500000      1.800429e+14           9.510000   26292.000000   \n",
      "50%    283432.000000      3.521417e+15          46.370000   48174.000000   \n",
      "75%    429493.500000      4.635331e+15          82.260000   72042.000000   \n",
      "max    555718.000000      4.992346e+18         519.854600   99921.000000   \n",
      "std    163611.709333      1.306822e+18          78.874224   26858.296762   \n",
      "\n",
      "                 lat           long      city_pop  \\\n",
      "count  478827.000000  478827.000000  4.788270e+05   \n",
      "mean       38.539326     -90.234740  8.885169e+04   \n",
      "min        20.027100    -165.672300  2.300000e+01   \n",
      "25%        34.668900     -96.798000  7.430000e+02   \n",
      "50%        39.371600     -87.476900  2.435000e+03   \n",
      "75%        41.894800     -80.175200  2.032800e+04   \n",
      "max        65.689900     -67.950300  2.906700e+06   \n",
      "std         5.065466      13.723696  3.021841e+05   \n",
      "\n",
      "                         DateOfBirth     unix_time      merch_lat  \\\n",
      "count                         478827  4.788270e+05  478827.000000   \n",
      "mean   1974-02-03 13:41:54.331898544  1.380814e+09      38.538876   \n",
      "min              1924-10-30 00:00:00  1.371817e+09      19.027849   \n",
      "25%              1963-02-09 00:00:00  1.376035e+09      34.747276   \n",
      "50%              1976-01-02 00:00:00  1.380965e+09      39.373398   \n",
      "75%              1987-05-23 00:00:00  1.386140e+09      41.952671   \n",
      "max              2005-01-29 00:00:00  1.388534e+09      66.674714   \n",
      "std                              NaN  5.291347e+06       5.100154   \n",
      "\n",
      "          merch_long       is_fraud  TransactionID           Hour  \\\n",
      "count  478827.000000  478827.000000  478827.000000  478827.000000   \n",
      "mean      -90.234807       0.003678  282353.474282      13.161858   \n",
      "min      -166.671575       0.000000       2.000000       0.000000   \n",
      "25%       -96.907974       0.000000  139141.500000       8.000000   \n",
      "50%       -87.446584       0.000000  283433.000000      14.000000   \n",
      "75%       -80.267973       0.000000  429494.500000      19.000000   \n",
      "max       -66.952352       1.000000  555719.000000      23.000000   \n",
      "std        13.734943       0.060533  163611.709333       6.736161   \n",
      "\n",
      "        HighRiskHour      DayOfWeek      IsWeekend       distance  \\\n",
      "count  478827.000000  478827.000000  478827.000000  478827.000000   \n",
      "mean        0.229705       2.656360       0.278873      76.103409   \n",
      "min         0.000000       0.000000       0.000000       0.124180   \n",
      "25%         0.000000       1.000000       0.000000      55.305724   \n",
      "50%         0.000000       2.000000       0.000000      78.192864   \n",
      "75%         0.000000       5.000000       1.000000      98.501460   \n",
      "max         1.000000       6.000000       1.000000     150.673743   \n",
      "std         0.420644       2.220847       0.448445      29.101638   \n",
      "\n",
      "       HighRiskMerchantCategory            Age  \n",
      "count             478827.000000  478827.000000  \n",
      "mean                   0.393127      50.285143  \n",
      "min                    0.000000      19.000000  \n",
      "25%                    0.000000      37.000000  \n",
      "50%                    0.000000      48.000000  \n",
      "75%                    1.000000      61.000000  \n",
      "max                    1.000000     100.000000  \n",
      "std                    0.488445      17.411733  \n"
     ]
    }
   ],
   "source": [
    "# Calculate the time difference between consecutive transactions\n",
    "time_diff = df.index.to_series().diff().dt.total_seconds()\n",
    "# Flag rapid transactions (within 5 minutes)\n",
    "df['RapidTransactionFlag'] = time_diff < 60  # For a 1-minute threshold\n",
    "\n",
    "# Create a temporary DataFrame for rapid transactions\n",
    "rapid_transactions = df[df['RapidTransactionFlag']]\n",
    "\n",
    "# Group by date and count the number of rapid transactions\n",
    "rapid_transaction_counts = rapid_transactions.groupby(rapid_transactions.index.date).size()\n",
    "print(rapid_transaction_counts)\n",
    "\n",
    "# Get a summary of the rapid transactions\n",
    "rapid_transactions_summary = rapid_transactions.describe()\n",
    "print(rapid_transactions_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "64e05375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'CreditCardNumber', 'merchant', 'category',\n",
      "       'TransactionAmount', 'first', 'last', 'gender', 'street', 'city',\n",
      "       'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID', 'Hour', 'HighRiskHour', 'DayOfWeek', 'DayName',\n",
      "       'IsWeekend', 'distance', 'HighRiskMerchantCategory', 'Age', 'AgeGroup',\n",
      "       'TransactionFrequency', 'RapidTransactionFlag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "bca5e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part5 - RapidTransactionFlag completed at Fri Nov  1 18:41:20 2024. Elapsed time: 1 minutes and 14.95 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part5 - RapidTransactionFlag\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b200c",
   "metadata": {},
   "source": [
    "Transaction Amount Features:\n",
    "Log Transaction Amount: Normalize the TransactionAmount by taking its logarithm to reduce skewness.\n",
    "Transaction Amount Flags: Create binary flags for high-value transactions (e.g., if TransactionAmount exceeds a certain threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "da75a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     TransactionAmount  LogTransactionAmount  \\\n",
      "TransactionTime                                                \n",
      "2020-06-21 12:14:25               2.86              1.350667   \n",
      "2020-06-21 12:14:33              29.84              3.428813   \n",
      "2020-06-21 12:14:53              41.28              3.744314   \n",
      "2020-06-21 12:15:15              60.05              4.111693   \n",
      "2020-06-21 12:15:17               3.19              1.432701   \n",
      "2020-06-21 12:15:37              19.55              3.022861   \n",
      "2020-06-21 12:15:44             133.93              4.904756   \n",
      "2020-06-21 12:15:50              10.37              2.430978   \n",
      "2020-06-21 12:16:10               4.37              1.680828   \n",
      "2020-06-21 12:16:11              66.54              4.212720   \n",
      "\n",
      "                     HighValueTransactionFlag  \n",
      "TransactionTime                                \n",
      "2020-06-21 12:14:25                     False  \n",
      "2020-06-21 12:14:33                     False  \n",
      "2020-06-21 12:14:53                     False  \n",
      "2020-06-21 12:15:15                     False  \n",
      "2020-06-21 12:15:17                     False  \n",
      "2020-06-21 12:15:37                     False  \n",
      "2020-06-21 12:15:44                      True  \n",
      "2020-06-21 12:15:50                     False  \n",
      "2020-06-21 12:16:10                     False  \n",
      "2020-06-21 12:16:11                     False  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample DataFrame creation\n",
    "# Assume 'df' is your DataFrame and has a 'TransactionAmount' column\n",
    "# df = pd.read_csv('your_data.csv')  # Load your actual data\n",
    "\n",
    "# Step 1: Log Transaction Amount\n",
    "# Calculate the log of TransactionAmount\n",
    "df['LogTransactionAmount'] = np.log1p(df['TransactionAmount'])  # Use log1p for stability with 0 values\n",
    "\n",
    "# Step 2: Create Transaction Amount Flags\n",
    "# Define a threshold for high-value transactions\n",
    "threshold = 100  # Adjust the threshold based on your data context\n",
    "\n",
    "# Create a flag for high-value transactions\n",
    "df['HighValueTransactionFlag'] = df['TransactionAmount'] > threshold\n",
    "\n",
    "# Verify the new features\n",
    "print(df[['TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag']].head(10))  # Display the first 10 entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217bc43",
   "metadata": {},
   "source": [
    "Behavioral Features:\n",
    "Count of Transactions in Last X Days: Count how many transactions have occurred in the last 7, 14, or 30 days.\n",
    "Average Transaction Amount in Last X Days: Calculate the average transaction amount over the same periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "25aeefda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     TransactionCountLast7Days  TransactionCountLast14Days  \\\n",
      "TransactionTime                                                              \n",
      "2020-06-21 13:05:42                        1.0                         1.0   \n",
      "2020-06-21 16:25:36                        2.0                         2.0   \n",
      "2020-06-22 07:58:33                        3.0                         3.0   \n",
      "2020-06-22 15:32:31                        4.0                         4.0   \n",
      "2020-06-23 12:28:54                        5.0                         5.0   \n",
      "2020-06-23 14:24:48                        6.0                         6.0   \n",
      "2020-06-23 16:39:40                        7.0                         7.0   \n",
      "2020-06-23 19:07:05                        8.0                         8.0   \n",
      "2020-06-23 22:45:57                        9.0                         9.0   \n",
      "2020-06-24 04:22:17                       10.0                        10.0   \n",
      "\n",
      "                     TransactionCountLast30Days  \\\n",
      "TransactionTime                                   \n",
      "2020-06-21 13:05:42                         1.0   \n",
      "2020-06-21 16:25:36                         2.0   \n",
      "2020-06-22 07:58:33                         3.0   \n",
      "2020-06-22 15:32:31                         4.0   \n",
      "2020-06-23 12:28:54                         5.0   \n",
      "2020-06-23 14:24:48                         6.0   \n",
      "2020-06-23 16:39:40                         7.0   \n",
      "2020-06-23 19:07:05                         8.0   \n",
      "2020-06-23 22:45:57                         9.0   \n",
      "2020-06-24 04:22:17                        10.0   \n",
      "\n",
      "                     AverageTransactionAmountLast7Days  \\\n",
      "TransactionTime                                          \n",
      "2020-06-21 13:05:42                         124.660000   \n",
      "2020-06-21 16:25:36                         101.590000   \n",
      "2020-06-22 07:58:33                          89.476667   \n",
      "2020-06-22 15:32:31                          89.042500   \n",
      "2020-06-23 12:28:54                         100.838000   \n",
      "2020-06-23 14:24:48                          84.503333   \n",
      "2020-06-23 16:39:40                          73.468571   \n",
      "2020-06-23 19:07:05                          65.056250   \n",
      "2020-06-23 22:45:57                          58.426667   \n",
      "2020-06-24 04:22:17                          57.279000   \n",
      "\n",
      "                     AverageTransactionAmountLast14Days  \\\n",
      "TransactionTime                                           \n",
      "2020-06-21 13:05:42                          124.660000   \n",
      "2020-06-21 16:25:36                          101.590000   \n",
      "2020-06-22 07:58:33                           89.476667   \n",
      "2020-06-22 15:32:31                           89.042500   \n",
      "2020-06-23 12:28:54                          100.838000   \n",
      "2020-06-23 14:24:48                           84.503333   \n",
      "2020-06-23 16:39:40                           73.468571   \n",
      "2020-06-23 19:07:05                           65.056250   \n",
      "2020-06-23 22:45:57                           58.426667   \n",
      "2020-06-24 04:22:17                           57.279000   \n",
      "\n",
      "                     AverageTransactionAmountLast30Days  \n",
      "TransactionTime                                          \n",
      "2020-06-21 13:05:42                          124.660000  \n",
      "2020-06-21 16:25:36                          101.590000  \n",
      "2020-06-22 07:58:33                           89.476667  \n",
      "2020-06-22 15:32:31                           89.042500  \n",
      "2020-06-23 12:28:54                          100.838000  \n",
      "2020-06-23 14:24:48                           84.503333  \n",
      "2020-06-23 16:39:40                           73.468571  \n",
      "2020-06-23 19:07:05                           65.056250  \n",
      "2020-06-23 22:45:57                           58.426667  \n",
      "2020-06-24 04:22:17                           57.279000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'TransactionTime' is already set as the index and in datetime format\n",
    "\n",
    "# Step 1: Count of Transactions in Last X Days\n",
    "for days in [7, 14, 30]:\n",
    "    # Sort data by CreditCardNumber and TransactionTime to ensure rolling works properly\n",
    "    df = df.sort_values(by=['CreditCardNumber', 'TransactionTime'])\n",
    "    \n",
    "    # Apply rolling and count the number of transactions for each card\n",
    "    df[f'TransactionCountLast{days}Days'] = (\n",
    "        df.groupby('CreditCardNumber')['CreditCardNumber']\n",
    "        .rolling(f'{days}D')\n",
    "        .count()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Step 2: Average Transaction Amount in Last X Days\n",
    "for days in [7, 14, 30]:\n",
    "    # Sort data by CreditCardNumber and TransactionTime to ensure rolling works properly\n",
    "    df = df.sort_values(by=['CreditCardNumber', 'TransactionTime'])\n",
    "    \n",
    "    # Calculate the average transaction amount for each credit card in the last X days\n",
    "    df[f'AverageTransactionAmountLast{days}Days'] = (\n",
    "        df.groupby('CreditCardNumber')['TransactionAmount']\n",
    "        .rolling(f'{days}D')\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Verify the new features\n",
    "print(df[['TransactionCountLast7Days', 'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
    "           'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast14Days', 'AverageTransactionAmountLast30Days']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "ebcd77cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'CreditCardNumber', 'merchant', 'category',\n",
      "       'TransactionAmount', 'first', 'last', 'gender', 'street', 'city',\n",
      "       'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID', 'Hour', 'HighRiskHour', 'DayOfWeek', 'DayName',\n",
      "       'IsWeekend', 'distance', 'HighRiskMerchantCategory', 'Age', 'AgeGroup',\n",
      "       'TransactionFrequency', 'RapidTransactionFlag', 'LogTransactionAmount',\n",
      "       'HighValueTransactionFlag', 'TransactionCountLast7Days',\n",
      "       'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
      "       'AverageTransactionAmountLast7Days',\n",
      "       'AverageTransactionAmountLast14Days',\n",
      "       'AverageTransactionAmountLast30Days'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)  # Display all columns in the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "bc639c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part6 - TransactionCountLast_X_Days & AverageTrxAmountLast_X_Days completed at Fri Nov  1 18:41:25 2024. Elapsed time: 0 minutes and 4.90 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part6 - TransactionCountLast_X_Days & AverageTrxAmountLast_X_Days\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a24a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b4aaef3",
   "metadata": {},
   "source": [
    "# Graph Construction with NetworkX:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18793de2",
   "metadata": {},
   "source": [
    "Highlight Fraudulent Nodes: Overlay of fraudulent and non-fraudulent credit cards on this degree distribution to see if there’s a difference in their degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "e26bd1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique credit card nodes: 924\n",
      "Number of unique merchant nodes: 693\n",
      "Number of credit card nodes with degrees: 924\n",
      "Number of merchant nodes with degrees: 693\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges between credit cards and merchants, including transaction amount as an edge attribute\n",
    "for idx, row in df.iterrows():\n",
    "    credit_card = str(row['CreditCardNumber'])\n",
    "    merchant = str(row['merchant'])\n",
    "    transaction_amount = row['TransactionAmount']  # Ensure TransactionAmount exists in your dataframe\n",
    "    \n",
    "    # Add an edge with the transaction amount as an attribute\n",
    "    G.add_edge(credit_card, merchant, transaction_amount=transaction_amount)\n",
    "\n",
    "\n",
    "# Calculate degrees for all nodes in the graph\n",
    "degrees = dict(G.degree())\n",
    "\n",
    "# Filter degrees for credit cards and merchants\n",
    "credit_card_nodes = df['CreditCardNumber'].astype(str).unique()\n",
    "merchant_nodes = df['merchant'].astype(str).unique()\n",
    "\n",
    "credit_card_degrees = {node: degrees[node] for node in credit_card_nodes if node in degrees}\n",
    "merchant_degrees = {node: degrees[node] for node in merchant_nodes if node in degrees}\n",
    "\n",
    "# Debugging: Print counts to ensure correctness\n",
    "print(f\"Number of unique credit card nodes: {len(credit_card_nodes)}\")\n",
    "print(f\"Number of unique merchant nodes: {len(merchant_nodes)}\")\n",
    "print(f\"Number of credit card nodes with degrees: {len(credit_card_degrees)}\")\n",
    "print(f\"Number of merchant nodes with degrees: {len(merchant_degrees)}\")\n",
    "\n",
    "# Create a new DataFrame for easier plotting\n",
    "degree_df = pd.DataFrame({\n",
    "    'CreditCardDegree': pd.Series(credit_card_degrees),\n",
    "    'MerchantDegree': pd.Series(merchant_degrees)\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "a55584fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add degree information back to the original DataFrame\n",
    "df['degree'] = df['CreditCardNumber'].astype(str).map(credit_card_degrees)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "6a8f6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check edges and their attributes\n",
    "#for edge in G.edges(data=True):\n",
    "#    print(edge)\n",
    "\n",
    "#do NOT print this, huge list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "a9c525e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CreditCardNumber'] = df['CreditCardNumber'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "5115524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_mapping = df.set_index('CreditCardNumber')['is_fraud'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "f9e261bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part7 - NetworkX Start Step completed at Fri Nov  1 18:42:06 2024. Elapsed time: 0 minutes and 41.03 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part7 - NetworkX Start Step\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "167571cb",
   "metadata": {},
   "source": [
    "betweenness_centrality"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0093ce9f",
   "metadata": {},
   "source": [
    "# Calculate betweenness centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac75df",
   "metadata": {},
   "source": [
    "# MULTIPROCESSING : betweenness_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "50d69fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part8 - Betweenness Centrality Calculation with Multiprocessing completed at Fri Nov  1 18:44:10 2024. Elapsed time: 2 minutes and 3.70 seconds\n",
      "\n",
      "[('60416207185.0', 6.866934985630104e-05), ('fraud_Kutch-Ferry', 0.0003782526235637667), ('fraud_Halvorson Group', 0.00022372276499079655), ('fraud_Conroy-Cruickshank', 0.00043926880614731214), ('fraud_Larkin Ltd', 0.0003300758225045274), ('fraud_Leffler-Goldner', 0.0002759436321997116), ('fraud_Kihn, Abernathy and Douglas', 0.0002977530309658762), ('fraud_Altenwerth, Cartwright and Koss', 0.0003741107388775957), ('fraud_Cartwright PLC', 0.000338769069184188), ('fraud_Ritchie, Oberbrunner and Cremin', 7.542017831392305e-05)]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import time\n",
    "from networkx_graph_betweeness_centrality import parallel_betweenness_centrality\n",
    "\n",
    "# Assuming G is your graph\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate betweenness centrality using parallel processing\n",
    "    betweenness_centrality = parallel_betweenness_centrality(G, num_partitions=4)  # Adjust number of cores if needed\n",
    "\n",
    "    # Log the time taken for betweenness centrality calculation with multiprocessing\n",
    "    log_time(\"Part8 - Betweenness Centrality Calculation with Multiprocessing\", start_time)\n",
    "\n",
    "    # Check a few centrality values\n",
    "    print(list(betweenness_centrality.items())[:10])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e7b4462",
   "metadata": {},
   "source": [
    "import os\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "85bd6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['betweenness_centrality'] = df['CreditCardNumber'].map(betweenness_centrality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "fb8bf772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    5.557190e+05\n",
      "mean     7.601584e-05\n",
      "std      3.431355e-05\n",
      "min      7.500703e-09\n",
      "25%      4.970571e-05\n",
      "50%      7.859341e-05\n",
      "75%      1.056097e-04\n",
      "max      1.412551e-04\n",
      "Name: betweenness_centrality, dtype: float64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df['betweenness_centrality'].describe())\n",
    "print(df['betweenness_centrality'].isna().sum())  # Check for missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "e5e4b947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60416207185: None\n",
      "fraud_Kutch-Ferry: 0.0003782526235637667\n"
     ]
    }
   ],
   "source": [
    "# Check betweenness centrality for specific credit card numbers\n",
    "sample_nodes = ['60416207185', 'fraud_Kutch-Ferry']  # Replace with actual nodes\n",
    "for node in sample_nodes:\n",
    "    print(f\"{node}: {betweenness_centrality.get(node)}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a33a0b35",
   "metadata": {},
   "source": [
    "log_time(\"Part8 - Feature Engineering -- NetworkX betweenness_centrality\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439124c6",
   "metadata": {},
   "source": [
    "1. Investigate Nodes with High Betweenness Centrality:\n",
    "\n",
    "Now that you’ve visualized nodes with high betweenness centrality, you can:\n",
    "\n",
    "    Examine if fraudulent nodes tend to have high betweenness centrality. This might indicate that these nodes are acting as \"connectors\" between different parts of the network, which could be a sign of suspicious behavior.\n",
    "    Compare centrality between fraud and non-fraud nodes to see if there's a pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13fdd8e",
   "metadata": {},
   "source": [
    "2. Visualize Communities in the Network:\n",
    "\n",
    "You could apply community detection to uncover fraud rings or clusters of merchants targeted by fraudsters. The Louvain algorithm is great for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "b5f148a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community.community_louvain as community_louvain\n",
    "\n",
    "\n",
    "# Apply Louvain method for community detection\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee872fb",
   "metadata": {},
   "source": [
    "Fraud Node Highlighting:\n",
    "\n",
    "    Fraudulent nodes (from df['is_fraud'] == 1) are colored red to make them stand out. The rest of the nodes are still colored based on their communities.\n",
    "    This should help you easily spot any fraudulent nodes in the network.\n",
    "\n",
    "Top 10 Most Central Nodes:\n",
    "\n",
    "    We calculate betweenness centrality and extract the top 10 most central nodes.\n",
    "    These nodes are visualized with their connections, which should help declutter the graph and focus on the key players in the transaction network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "8fd26394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply Louvain method for community detection\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "# Create positions for nodes using a spring layout\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Add the community information to the DataFrame\n",
    "df['community'] = df['CreditCardNumber'].map(partition)\n",
    "\n",
    "# Highlight fraud nodes separately\n",
    "fraud_nodes = df[df['is_fraud'] == 1]['CreditCardNumber'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "95613930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the community information to the DataFrame\n",
    "df['community'] = df['CreditCardNumber'].map(partition)\n",
    "\n",
    "# Calculate the percentage of fraud in each community\n",
    "community_fraud = df.groupby('community')['is_fraud'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "d9407a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community\n",
      "0    0.002863\n",
      "1    0.006427\n",
      "2    0.003589\n",
      "3    0.005148\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print fraud rate per community\n",
    "print(community_fraud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "17572504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community\n",
      "0    218636\n",
      "1     63793\n",
      "2    191120\n",
      "3     82170\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "community_size = df.groupby('community').size()\n",
    "print(community_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "43cca311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fraud rates and community sizes into a single DataFrame\n",
    "fraud_vs_size = pd.concat([community_fraud, df.groupby('community').size()], axis=1)\n",
    "fraud_vs_size.columns = ['FraudRate', 'CommunitySize']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "accbc68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community\n",
      "1    0.006427\n",
      "3    0.005148\n",
      "2    0.003589\n",
      "0    0.002863\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "top_fraud_communities = community_fraud.sort_values(ascending=False).head(5)\n",
    "print(top_fraud_communities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "4a875aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the community labels of the top fraud communities\n",
    "top_community_labels = top_fraud_communities.index.tolist()\n",
    "\n",
    "# Filter the DataFrame for only the top fraud communities\n",
    "top_communities_df = df[df['community'].isin(top_community_labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "88b0bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merchant\n",
      "fraud_Romaguera, Cruickshank and Greenholt    0.021739\n",
      "fraud_Lemke-Gutmann                           0.021505\n",
      "fraud_Mosciski, Ziemann and Farrell           0.020690\n",
      "fraud_Heathcote, Yost and Kertzmann           0.020482\n",
      "fraud_Rodriguez, Yost and Jenkins             0.019960\n",
      "fraud_Medhurst PLC                            0.019430\n",
      "fraud_Bashirian Group                         0.018987\n",
      "fraud_Kris-Weimann                            0.018939\n",
      "fraud_Heathcote LLC                           0.018703\n",
      "fraud_Bednar Group                            0.018519\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Show Only the Top Merchants by Fraud Rate:\n",
    "# Instead of displaying all merchants, you can filter the plot to show only the top 10 or 20 merchants with the highest fraud rates.\n",
    "\n",
    "# Calculate fraud rate by merchant in the top fraud communities\n",
    "merchant_fraud_rate = top_communities_df.groupby('merchant')['is_fraud'].mean()\n",
    "\n",
    "# Sort merchants by fraud rate in descending order\n",
    "top_merchants = merchant_fraud_rate.sort_values(ascending=False).head(10)\n",
    "\n",
    "# Print top 10 merchants with highest fraud rate\n",
    "print(top_merchants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "bbd1dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'category' is a column representing merchant categories\n",
    "merchantcategory_fraud = top_communities_df.groupby('category')['is_fraud'].mean()\n",
    "\n",
    "# Sort the fraud rate by merchant category in descending order\n",
    "merchantcategory_fraud_sorted = merchantcategory_fraud.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "2e7c000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part9 - Community & Top Merchants completed at Fri Nov  1 18:45:15 2024. Elapsed time: 3 minutes and 8.95 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log_time(\"Part9 - Community & Top Merchants\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "154aa31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Density: 0.2513486042481799\n"
     ]
    }
   ],
   "source": [
    "# Check the density of the graph (a measure of sparsity)\n",
    "density = nx.density(G)\n",
    "print(f\"Graph Density: {density}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "5e360c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Degree of Nodes: 406.17934446505876\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average degree\n",
    "degree_sequence = [degree for node, degree in G.degree()]\n",
    "average_degree = sum(degree_sequence) / len(degree_sequence)\n",
    "print(f\"Average Degree of Nodes: {average_degree}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "5fa3b21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part10 - Density completed at Fri Nov  1 18:45:15 2024. Elapsed time: 0 minutes and 0.02 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part10 - Density\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "452804a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'CreditCardNumber', 'merchant', 'category',\n",
      "       'TransactionAmount', 'first', 'last', 'gender', 'street', 'city',\n",
      "       'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID', 'Hour', 'HighRiskHour', 'DayOfWeek', 'DayName',\n",
      "       'IsWeekend', 'distance', 'HighRiskMerchantCategory', 'Age', 'AgeGroup',\n",
      "       'TransactionFrequency', 'RapidTransactionFlag', 'LogTransactionAmount',\n",
      "       'HighValueTransactionFlag', 'TransactionCountLast7Days',\n",
      "       'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
      "       'AverageTransactionAmountLast7Days',\n",
      "       'AverageTransactionAmountLast14Days',\n",
      "       'AverageTransactionAmountLast30Days', 'degree',\n",
      "       'betweenness_centrality', 'community'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "d5be3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag',\n",
    "    'TransactionCountLast7Days', 'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
    "    'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast14Days', 'AverageTransactionAmountLast30Days',\n",
    "    'Hour', 'HighRiskHour', 'DayOfWeek', 'IsWeekend', 'TransactionFrequency', 'RapidTransactionFlag',\n",
    "    'lat', 'long', 'merch_lat', 'merch_long', 'distance', 'city_pop',\n",
    "    'Age', 'AgeGroup', 'gender', 'state', 'city',\n",
    "    'degree', 'betweenness_centrality', 'community'\n",
    "]\n",
    "\n",
    "df_selected_features = df[selected_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f21f1e",
   "metadata": {},
   "source": [
    "# Page rank as new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "6fda32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PageRank for each node in the graph\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "# Map the PageRank values to the 'CreditCardNumber' in the DataFrame\n",
    "df['pagerank'] = df['CreditCardNumber'].map(pagerank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "53596e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in the pagerank column\n",
    "print(df['pagerank'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "f5e2620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    555719.000000\n",
      "mean          0.000636\n",
      "std           0.000139\n",
      "min           0.000099\n",
      "25%           0.000548\n",
      "50%           0.000669\n",
      "75%           0.000751\n",
      "max           0.000850\n",
      "Name: pagerank, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check descriptive statistics of pagerank values\n",
    "print(df['pagerank'].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "a525047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with zero PageRank: 0\n"
     ]
    }
   ],
   "source": [
    "# Check how many nodes have a PageRank of zero\n",
    "zero_pagerank_count = (df['pagerank'] == 0).sum()\n",
    "print(f\"Number of nodes with zero PageRank: {zero_pagerank_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "c97f80b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PageRank for Fraud: 0.0005267197095238786\n",
      "Average PageRank for Non-Fraud: 0.0006366062405655646\n"
     ]
    }
   ],
   "source": [
    "# Compare the average PageRank for fraud and non-fraud transactions\n",
    "fraud_avg_pagerank = df[df['is_fraud'] == 1]['pagerank'].mean()\n",
    "non_fraud_avg_pagerank = df[df['is_fraud'] == 0]['pagerank'].mean()\n",
    "\n",
    "print(f\"Average PageRank for Fraud: {fraud_avg_pagerank}\")\n",
    "print(f\"Average PageRank for Non-Fraud: {non_fraud_avg_pagerank}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "47121274",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features.append('pagerank')\n",
    "df_selected_features = df[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "439c0ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag',\n",
      "       'TransactionCountLast7Days', 'TransactionCountLast14Days',\n",
      "       'TransactionCountLast30Days', 'AverageTransactionAmountLast7Days',\n",
      "       'AverageTransactionAmountLast14Days',\n",
      "       'AverageTransactionAmountLast30Days', 'Hour', 'HighRiskHour',\n",
      "       'DayOfWeek', 'IsWeekend', 'TransactionFrequency',\n",
      "       'RapidTransactionFlag', 'lat', 'long', 'merch_lat', 'merch_long',\n",
      "       'distance', 'city_pop', 'Age', 'AgeGroup', 'gender', 'state', 'city',\n",
      "       'degree', 'betweenness_centrality', 'community', 'pagerank'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_selected_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "ae9a67f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(555719, 46)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "a89b08a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part11 - PageRank completed at Fri Nov  1 18:45:17 2024. Elapsed time: 0 minutes and 1.69 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part11 - PageRank\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "e17cecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "56237852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END - Feature Engineering .....   completed at Fri Nov  1 18:45:17 2024. Elapsed time: 0 minutes and 0.01 seconds\n",
      "\n",
      "--------------------------------------------------- ------------------   completed at Fri Nov  1 18:45:17 2024. Elapsed time: 0 minutes and 0.00 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"END - Feature Engineering .....  \", start_time)\n",
    "start_time = time.time()\n",
    "log_time(\"--------------------------------------------------- ------------------  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac908b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "163f30c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgBoost with SMOTE sacle_pos_weight=5_Test START Model ....   completed at Fri Nov  1 18:45:17 2024. Elapsed time: 0 minutes and 0.01 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(f\"{model_specs}_{dataset_type} START Model ....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef615d1",
   "metadata": {},
   "source": [
    "# Define features\n",
    "optimized_features = [\n",
    "    'LogTransactionAmount', 'merch_lat', 'AverageTransactionAmountLast14Days',\n",
    "    'TransactionAmount', 'distance', 'merch_long', \n",
    "    'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast30Days',\n",
    "    'TransactionCountLast30Days', 'TransactionCountLast7Days'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0b0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9886b177",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Evaluation on test set\n",
    "def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "    y_test_probs = model.predict_proba(X_test)[:, 1]\n",
    "    y_test_pred = (y_test_probs >= threshold).astype(int)\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    \n",
    "    test_clf = classification_report(y_test, y_test_pred)\n",
    "    print(\"Classification Report (Test):\\n\", test_clf)\n",
    "    \n",
    "    \n",
    "    # Evaluation metrics\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_pred_adjusted)\n",
    "    print(\"Test ROC AUC:\", test_roc_auc)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_test_pred_adjusted)\n",
    "    test_auc_pr = auc(recall, precision)  # Renamed to avoid conflict\n",
    "    \n",
    "    print(\"Test Precision-Recall AUC:\", auc(recall, precision))\n",
    "    plt.plot(recall, precision, label=\"Test Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Test Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return best_model, X_test, y_test, test_accuracy, test_clf,  test_roc_auc, test_auc_pr # return balanced dataset for report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173afd65",
   "metadata": {},
   "source": [
    "optimized_features_v2 = [\n",
    "    'LogTransactionAmount', 'TransactionAmount', 'HighValueTransactionFlag',\n",
    "    'TransactionCountLast7Days', 'TransactionCountLast30Days', \n",
    "    'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast14Days', \n",
    "    'AverageTransactionAmountLast30Days', 'Hour', 'HighRiskHour', 'IsWeekend', \n",
    "    'RapidTransactionFlag', 'lat', 'long', 'merch_lat', 'merch_long', \n",
    "    'distance', 'city_pop', 'AgeGroup', 'degree', 'betweenness_centrality', 'pagerank'\n",
    "]\n",
    "# First, ensure X contains only the selected features in optimized_features_v2\n",
    "# df_optimized_features_v2 = df[optimized_features_v2]\n",
    "\n",
    "# Filter only numerical features\n",
    "df_optimized_features_v2 = df[optimized_features_v2].select_dtypes(include=['number']).columns\n",
    "print(\"Numerical features selected for interactions:\", numerical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "208e91db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from itertools import combinations\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define optimized features\n",
    "optimized_features_v2 = [\n",
    "    'LogTransactionAmount', 'TransactionAmount', 'HighValueTransactionFlag',\n",
    "    'TransactionCountLast7Days', 'TransactionCountLast30Days', \n",
    "    'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast14Days', \n",
    "    'AverageTransactionAmountLast30Days', 'Hour', 'HighRiskHour', 'IsWeekend', \n",
    "    'RapidTransactionFlag', 'lat', 'long', 'merch_lat', 'merch_long', \n",
    "    'distance', 'city_pop', 'AgeGroup', 'degree', 'betweenness_centrality', 'pagerank'\n",
    "]\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess(df, sample_fraction=1.0):\n",
    "    # Select only the optimized features\n",
    "    X = df[optimized_features_v2].sample(frac=sample_fraction)\n",
    "    y = df['is_fraud'].reset_index(drop=True)\n",
    "    X = X.reset_index(drop=True)\n",
    "    \n",
    "    # Identify and encode categorical columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "    X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Filter numerical features for interaction terms\n",
    "    numerical_features = X.select_dtypes(include=['number']).columns\n",
    "    print(\"Numerical features selected for interactions:\", numerical_features)\n",
    "    \n",
    "    # Add interaction terms for numerical features only\n",
    "    for feature1, feature2 in combinations(numerical_features, 2):\n",
    "        interaction_name = f\"{feature1}_x_{feature2}\"\n",
    "        X[interaction_name] = X[feature1] * X[feature2]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Train XGBoost model with SMOTE\n",
    "def train_xgboost_with_smote(X, y):\n",
    "    # Balance the dataset using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "    # Initialize XGBoost with custom scale_pos_weight\n",
    "    #The scale_pos_weight parameter was set to 10, which may be impacting the balance. \n",
    "    #Lowering this to values around\n",
    "    #model = XGBClassifier(scale_pos_weight=10, random_state=42)  # Adjust scale_pos_weight as needed\n",
    "    \n",
    "    model = XGBClassifier(scale_pos_weight=5, random_state=42)  \n",
    "    model.fit(X_smote, y_smote)\n",
    "\n",
    "    # Evaluate on training data\n",
    "    y_train_pred = model.predict(X_smote)\n",
    "    accuracy = accuracy_score(y_smote, y_train_pred)\n",
    "    clf = classification_report(y_smote, y_train_pred)\n",
    "    roc_auc = roc_auc_score(y_smote, y_train_pred)\n",
    "    precision, recall, _ = precision_recall_curve(y_smote, y_train_pred)\n",
    "    auc_pr = auc(recall, precision)\n",
    "    \n",
    "    print(\"Train Accuracy:\", accuracy)\n",
    "    print(\"Classification Report (Train):\\n\", clf)\n",
    "    print(\"Train ROC AUC:\", roc_auc)\n",
    "    print(\"Train Precision-Recall AUC:\", auc_pr)\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    plt.plot(recall, precision, label=\"Train Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return model, accuracy, clf, roc_auc, auc_pr\n",
    "\n",
    "# Report generation and saving\n",
    "def generate_and_save_report(model, accuracy, clf, roc_auc, auc_pr, report_dir=\"reports\", file_name=\"classification_report.txt\"):\n",
    "    report_path = os.path.join(report_dir, file_name)\n",
    "    \n",
    "    # Create the report content\n",
    "    full_report = (\n",
    "        f\"Accuracy: {accuracy:.4f}\\n\\n\"\n",
    "        f\"Classification Report:\\n{clf}\\n\\n\"\n",
    "        f\"ROC AUC: {roc_auc:.4f}\\n\\n\"\n",
    "        f\"Precision-Recall AUC: {auc_pr:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Write the report to a file\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(full_report)\n",
    "    print(f\"Report saved to {report_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "05abfd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, threshold=0.2):\n",
    "    # Predict probabilities and apply threshold\n",
    "    y_test_probs = model.predict_proba(X_test)[:, 1]\n",
    "    y_test_pred = (y_test_probs >= threshold).astype(int)\n",
    "    \n",
    "    # Evaluate accuracy and classification report\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    clf = classification_report(y_test, y_test_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_test_probs)\n",
    "    auc_pr = auc(recall, precision)\n",
    "    \n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    print(\"Classification Report (Test):\\n\", clf)\n",
    "    print(\"Test ROC AUC:\", roc_auc)\n",
    "    print(\"Test Precision-Recall AUC:\", auc_pr)\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    plt.plot(recall, precision, label=\"Test Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy, clf, roc_auc, auc_pr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "fc39e4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features selected for interactions: Index(['LogTransactionAmount', 'TransactionAmount',\n",
      "       'TransactionCountLast7Days', 'TransactionCountLast30Days',\n",
      "       'AverageTransactionAmountLast7Days',\n",
      "       'AverageTransactionAmountLast14Days',\n",
      "       'AverageTransactionAmountLast30Days', 'Hour', 'HighRiskHour',\n",
      "       'IsWeekend', 'lat', 'long', 'merch_lat', 'merch_long', 'distance',\n",
      "       'city_pop', 'degree', 'betweenness_centrality', 'pagerank'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_20530/3784813503.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X[interaction_name] = X[feature1] * X[feature2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4106553851856784\n",
      "Classification Report (Test):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.41      0.58    553574\n",
      "         1.0       0.00      0.59      0.01      2145\n",
      "\n",
      "    accuracy                           0.41    555719\n",
      "   macro avg       0.50      0.50      0.29    555719\n",
      "weighted avg       0.99      0.41      0.58    555719\n",
      "\n",
      "Test ROC AUC: 0.4991559290881513\n",
      "Test Precision-Recall AUC: 0.003910979744480943\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4W0lEQVR4nO3de1xUdf7H8fdwmwEUXES5KCjeUjMtYTU109Rw1Z+tu1mabkqpRVamlKVrecui2rxkirlet83Kh5f6WZFJbqmpvzZv5YZliYYKhHgBFAWB8/vDnA1Bg2Fg4Ph6Ph7zeDDf+Z5zPufL5bz5njNnLIZhGAIAADAJN1cXAAAA4EyEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCqEGwAAYCoeri6guhUXFystLU1169aVxWJxdTkAAKAcDMNQbm6uQkND5eZ27bmZ6y7cpKWlKSwszNVlAAAABxw9elSNGze+Zp/rLtzUrVtX0qXB8fPzc3E1AACgPHJychQWFmY/jl/LdRduLp+K8vPzI9wAAFDLlOeSEi4oBgAApkK4AQAApkK4AQAApnLdXXMD4PpWVFSkixcvuroMAGXw8vL6zbd5lwfhBsB1wTAMZWRk6MyZM64uBcBVuLm5KSIiQl5eXpVaD+EGwHXhcrBp2LChfHx8uIknUMNcvsluenq6wsPDK/U7SrgBYHpFRUX2YFO/fn1XlwPgKho0aKC0tDQVFhbK09PT4fVwQTEA07t8jY2Pj4+LKwFwLZdPRxUVFVVqPYQbANcNTkUBNZuzfkcJNwAAwFRcGm62bt2qgQMHKjQ0VBaLRe+///5vLrNlyxZFRkbKZrOpWbNmeuONN6q+UAAAUGu4NNycO3dOHTp00IIFC8rV//Dhw+rfv7+6d++uvXv36q9//avGjRundevWVXGlAIDqcOTIEVksFu3bt8+pfc3g888/l8Visd/OYOXKlapXr55La6qpXBpu+vXrp1mzZunPf/5zufq/8cYbCg8P17x589SmTRuNHj1aDz74oF599dUqrvS3FRUbOnY6T8dO57m6FAAmYLFYrvmIiYlxeN1NmzbVvHnzytXv8vZ8fHzUrl07LV682OHtlkdYWJjS09PVrl07p/atrF+PfZ06ddShQwetXLmyyrfrDJ999pn69++v+vXry8fHR23bttWTTz6p48ePu7q0KlOrrrnZuXOnoqOjS7T17dtXu3btuuodR/Pz85WTk1PiURVOnsvXbS9/pttf+axK1g/g+pKenm5/zJs3T35+fiXaXnvttWqpY+bMmUpPT9c333yjQYMGKTY2VqtXry6zb0FBQaW35+7uruDgYHl4/PadSirS1xlWrFih9PR0ff311xoyZIgeeOABffLJJ9WybUctXrxYffr0UXBwsNatW6fk5GS98cYbys7O1uzZsx1erzO+11WpVoWbjIwMBQUFlWgLCgpSYWGhsrKyylwmPj5e/v7+9kdYWFh1lAqgBjMMQ3kFhS55GIZRrhqDg4PtD39/f1kslhJtW7duLXH94YwZM1RYWGhffvr06QoPD5fValVoaKjGjRsnSerZs6d++uknTZgwwT4TcS1169ZVcHCwWrRooVmzZqlly5b26yN79uypxx57THFxcQoMDNSdd94pSUpOTlb//v1Vp04dBQUF6f777y/xN7q4uFgvv/yyWrRoIavVqvDwcL3wwguSSp9qOn36tIYPH64GDRrI29tbLVu21IoVK8rsK126LrNTp06yWq0KCQnRpEmTSoxLz549NW7cOD399NMKCAhQcHCwpk+fXq7vSb169RQcHKzmzZvrr3/9qwICArRp0yb769nZ2XrooYfUsGFD+fn5qVevXvr6669LrGPDhg2KioqSzWZTYGBgiTMXb731lqKiouxjPmzYMGVmZpartrIcO3ZM48aN07hx47R8+XL17NlTTZs21e23366lS5dq6tSpki79rNx8880llp03b56aNm1qfx4TE6NBgwYpPj5eoaGhatWqlSZPnqxbb7211Hbbt2+vadOm2Z+vWLFCbdq0kc1mU+vWrZWQkODwPpVXrbuJ35W/iJf/UFztF3Ty5MmKi4uzP8/JySHgANe58xeL1Haqa/7jTp7ZVz5elfvT+8knn+gvf/mL5s+fr+7du+vQoUN66KGHJEnTpk3T2rVrNXfuXL377ru68cYblZGRYT/Irl+/Xh06dNBDDz2kMWPGVHjbNputxEz5P/7xDz3yyCPavn27DMNQenq6evTooTFjxmjOnDk6f/68nnnmGd17773617/+JenS3+UlS5Zo7ty5uu2225Senq7vvvuuzO0999xzSk5O1scff6zAwED9+OOPOn/+fJl9jx8/rv79+ysmJkZvvvmmvvvuO40ZM0Y2m61EgPnHP/6huLg4ffnll9q5c6diYmLUrVs3ezj7LUVFRVq3bp1OnTplv9GcYRgaMGCAAgIClJiYKH9/fy1evFi9e/fWwYMHFRAQoI8++kh//vOfNWXKFP3zn/9UQUGBPvroI/t6CwoK9Pzzz+uGG25QZmamJkyYoJiYGCUmJparriutWbNGBQUFevrpp8t8vaLX62zevFl+fn5KSkqyH3tfeuklHTp0SM2bN5ckffvtt9q/f7/Wrl0rSVqyZImmTZumBQsW6JZbbtHevXs1ZswY+fr6auTIkQ7tV3nUqnATHBysjIyMEm2ZmZny8PC46l1HrVarrFZrdZQHANXihRde0KRJk+wHh2bNmun555/X008/rWnTpik1NVXBwcHq06ePPD09FR4erk6dOkmSAgIC5O7ubp8dKK/CwkK99dZb2r9/vx555BF7e4sWLfTKK6/Yn0+dOlUdO3bUiy++aG9bvny5wsLCdPDgQYWEhOi1117TggUL7PU3b95ct912W5nbTU1N1S233KKoqChJKjGbcKWEhASFhYVpwYIFslgsat26tdLS0vTMM89o6tSp9g9k/PXMQsuWLbVgwQJt3rz5N8PNfffdJ3d3d124cEFFRUUKCAjQ6NGjJV26rmX//v3KzMy0H3NeffVVvf/++1q7dq0eeughvfDCCxo6dKhmzJhhX2eHDh3sXz/44IP2r5s1a6b58+erU6dOOnv2rOrUqXPN2sryww8/yM/PTyEhIRVetiy+vr5aunRpic99at++vd5++20999xzkqRVq1bp97//vVq1aiVJev755zV79mz7DFVERISSk5O1ePFiws1lXbp00QcffFCibdOmTYqKiqrUbZoBXF+8Pd2VPLOvy7ZdWbt379ZXX31lP5UjXZpNuHDhgvLy8nTPPfdo3rx5atasmf7whz+of//+GjhwoEPXpjzzzDN69tlnlZ+fLy8vL02cOFEPP/yw/fXLoePXtX322WdlHowPHTqkM2fOKD8/X7179y7X9h955BHdfffd2rNnj6KjozVo0CB17dq1zL4HDhxQly5dSszkd+vWTWfPntWxY8cUHh4u6dIB+ddCQkLsp39iY2P11ltv2V87e/as/eu5c+eqT58+Onr0qOLi4jRhwgS1aNHCvt9nz54t9Y/2+fPndejQIUnSvn37rjlbtnfvXk2fPl379u3TqVOnVFxcLOlSwGvbtu21B6oMhmE49caVN910U6kPtBw+fLiWL1+u5557ToZh6J133tH48eMlSSdOnNDRo0c1atSoEvtdWFgof39/p9VVFpeGm7Nnz+rHH3+0Pz98+LD27dungIAAhYeHa/LkyTp+/LjefPNNSZd+6BYsWKC4uDiNGTNGO3fu1LJly/TOO++4ahcA1EIWi6XSp4Zcqbi4WDNmzCjznaY2m01hYWH6/vvvlZSUpE8//VRjx47V3/72N23ZsqXC/whOnDhRMTEx8vHxUUhISKmDpa+vb6naBg4cqJdffrnUukJCQpSSklKh7ffr108//fSTPvroI3366afq3bu3Hn300TLfJVvWwbysSxeuHAOLxWIPEjNnztRTTz1VZi2Xrz1q0aKF1qxZY59Ratu2rYqLixUSEqLPP/+81HKXT/94e3tfdT/PnTun6OhoRUdH66233lKDBg2Umpqqvn37OnzxbqtWrZSdna309PRrzt64ubmVuhasrDfpXPm9lqRhw4Zp0qRJ2rNnj86fP6+jR49q6NChkmQf0yVLlqhz584llnN3r3zIvxaX/nbv2rVLd9xxh/355WtjRo4cqZUrVyo9PV2pqan21yMiIpSYmKgJEyZo4cKFCg0N1fz583X33XdXe+0A4CodO3bU999/b581KIu3t7fuuusu3XXXXXr00UfVunVr7d+/Xx07dpSXl1e5P7snMDDwmtspq7Z169apadOmZc4UtWzZUt7e3tq8ebP9lM5vadCggWJiYhQTE6Pu3btr4sSJZYabtm3bat26dSVCzo4dO1S3bl01atSoXNtq2LChGjZs+Jv9WrRoobvvvluTJ0/W//7v/6pjx47KyMiQh4fHVU+dtW/fXps3b9YDDzxQ6rXvvvtOWVlZeumll+zXhe7atatcNV/N4MGDNWnSJL3yyiuaO3duqdfPnDmjevXqqUGDBsrIyCgxbuW9d1Djxo11++23a9WqVTp//rz69Oljf+NPUFCQGjVqpJSUFA0fPrxS+1JRLg03PXv2vOY7B8q6h0CPHj20Z8+eKqwKAGq2qVOn6n/+538UFhame+65R25ubvrmm2+0f/9+zZo1SytXrlRRUZE6d+4sHx8f/fOf/5S3t7eaNGki6dJ1K1u3btXQoUNltVoVGBjotNoeffRRLVmyRPfdd58mTpxovwj43Xff1ZIlS2Sz2fTMM8/o6aeflpeXl7p166YTJ07o22+/1ahRo8rc18jISN14443Kz8/Xhx9+qDZt2pS57bFjx2revHl6/PHH9dhjj+n777/XtGnTFBcXZ7/expmefPJJdejQQbt27VKfPn3UpUsXDRo0SC+//LJuuOEGpaWlKTExUYMGDVJUVJSmTZum3r17q3nz5ho6dKgKCwv18ccf6+mnn1Z4eLi8vLz0+uuvKzY2Vv/5z3/0/PPPV6q+sLAwzZ07V4899phycnI0YsQINW3aVMeOHdObb76pOnXqaPbs2erZs6dOnDihV155RYMHD9bGjRv18ccfy8/Pr1zbGT58uKZPn66CgoJSIWr69OkaN26c/Pz81K9fP+Xn52vXrl06ffp0iTf7OFuteis4AODS/b0+/PBDJSUl6fe//71uvfVWzZkzxx5e6tWrpyVLlqhbt2722YIPPvjAfj3IzJkzdeTIETVv3lwNGjRwam2hoaHavn27ioqK1LdvX7Vr105PPPGE/P397QHjueee05NPPqmpU6eqTZs2GjJkyFXf8uzl5aXJkyerffv2uv322+Xu7q533323zL6NGjVSYmKi/v3vf6tDhw6KjY3VqFGj9Oyzzzp1Hy+76aab1KdPH02dOlUWi0WJiYm6/fbb9eCDD6pVq1YaOnSojhw5Yp/J6Nmzp9asWaMNGzbo5ptvVq9evfTll19KujQ7tXLlSq1Zs0Zt27bVSy+95JQb1I4dO1abNm3S8ePH9ac//UmtW7fW6NGj5efnZz/91qZNGyUkJGjhwoXq0KGD/v3vf1/11FxZ7rnnHp08eVJ5eXkaNGhQiddGjx6tpUuXauXKlbrpppvUo0cPrVy5UhEREZXet2uxGOW96YJJ5OTkyN/fX9nZ2eVOpeWRmXtBnV7YLDeLlBI/wGnrBVB5Fy5c0OHDhxURESGbzebqcgBcxbV+Vyty/GbmBgAAmArhBgAAmArhBgAAmArhBsB14zq7xBCodZz1O0q4AWB6l2/alpeX5+JKAFzL5RsWVvYmf7X3Fp0AUE7u7u6qV6+e/e3GPj4+Tr0tPYDKKy4u1okTJ+Tj4+PQR4X8GuEGwHXh8odEXu1+KgBcz83NTeHh4ZX+54NwA+C6YLFYFBISooYNG5b5uTkAXM/Ly8spd5Mm3AC4rri7u1f5h/YBcC0uKAYAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKZCuAEAAKbi8nCTkJCgiIgI2Ww2RUZGatu2bdfsv2rVKnXo0EE+Pj4KCQnRAw88oJMnT1ZTtQAAoKZzabhZvXq1xo8frylTpmjv3r3q3r27+vXrp9TU1DL7f/HFFxoxYoRGjRqlb7/9VmvWrNFXX32l0aNHV3PlAACgpnJpuJkzZ45GjRql0aNHq02bNpo3b57CwsK0aNGiMvv/3//9n5o2bapx48YpIiJCt912mx5++GHt2rXrqtvIz89XTk5OiQcAADAvl4WbgoIC7d69W9HR0SXao6OjtWPHjjKX6dq1q44dO6bExEQZhqGff/5Za9eu1YABA666nfj4ePn7+9sfYWFhTt0PAABQs7gs3GRlZamoqEhBQUEl2oOCgpSRkVHmMl27dtWqVas0ZMgQeXl5KTg4WPXq1dPrr79+1e1MnjxZ2dnZ9sfRo0eduh8AAKBmcfkFxRaLpcRzwzBKtV2WnJyscePGaerUqdq9e7c2btyow4cPKzY29qrrt1qt8vPzK/EAAADm5eGqDQcGBsrd3b3ULE1mZmap2ZzL4uPj1a1bN02cOFGS1L59e/n6+qp79+6aNWuWQkJCqrxuAABQs7ls5sbLy0uRkZFKSkoq0Z6UlKSuXbuWuUxeXp7c3EqW7O7uLunSjA8AAIBLT0vFxcVp6dKlWr58uQ4cOKAJEyYoNTXVfppp8uTJGjFihL3/wIEDtX79ei1atEgpKSnavn27xo0bp06dOik0NNRVuwEAAGoQl52WkqQhQ4bo5MmTmjlzptLT09WuXTslJiaqSZMmkqT09PQS97yJiYlRbm6uFixYoCeffFL16tVTr1699PLLL7tqFwAAQA1jMa6z8zk5OTny9/dXdna2Uy8uzsy9oE4vbJabRUqJv/pb0wEAQMVV5Pjt8ndLAQAAOBPhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmArhBgAAmIrLw01CQoIiIiJks9kUGRmpbdu2XbN/fn6+pkyZoiZNmshqtap58+Zavnx5NVULAABqOg9Xbnz16tUaP368EhIS1K1bNy1evFj9+vVTcnKywsPDy1zm3nvv1c8//6xly5apRYsWyszMVGFhYTVXDgAAaiqXhps5c+Zo1KhRGj16tCRp3rx5+uSTT7Ro0SLFx8eX6r9x40Zt2bJFKSkpCggIkCQ1bdr0mtvIz89Xfn6+/XlOTo7zdgAAANQ4LjstVVBQoN27dys6OrpEe3R0tHbs2FHmMhs2bFBUVJReeeUVNWrUSK1atdJTTz2l8+fPX3U78fHx8vf3tz/CwsKcuh8AAKBmcdnMTVZWloqKihQUFFSiPSgoSBkZGWUuk5KSoi+++EI2m03vvfeesrKyNHbsWJ06deqq191MnjxZcXFx9uc5OTkEHAAATMylp6UkyWKxlHhuGEaptsuKi4tlsVi0atUq+fv7S7p0amvw4MFauHChvL29Sy1jtVpltVqdXzgAAKiRXHZaKjAwUO7u7qVmaTIzM0vN5lwWEhKiRo0a2YONJLVp00aGYejYsWNVWi8AAKgdXBZuvLy8FBkZqaSkpBLtSUlJ6tq1a5nLdOvWTWlpaTp79qy97eDBg3Jzc1Pjxo2rtF4AAFA7uPQ+N3FxcVq6dKmWL1+uAwcOaMKECUpNTVVsbKykS9fLjBgxwt5/2LBhql+/vh544AElJydr69atmjhxoh588MEyT0kBAIDrj0uvuRkyZIhOnjypmTNnKj09Xe3atVNiYqKaNGkiSUpPT1dqaqq9f506dZSUlKTHH39cUVFRql+/vu69917NmjXLVbsAAABqGIthGIari6hOOTk58vf3V3Z2tvz8/Jy23szcC+r0wma5WaSU+AFOWy8AAKjY8duhmZtz587ppZde0ubNm5WZmani4uISr6ekpDiyWgAAgEpzKNyMHj1aW7Zs0f3336+QkJCrvnUbAACgujkUbj7++GN99NFH6tatm7PrAQAAqBSH3i31u9/9zv7ZTgAAADWJQ+Hm+eef19SpU5WXl+fsegAAACrFodNSs2fP1qFDhxQUFKSmTZvK09OzxOt79uxxSnEAAAAV5VC4GTRokJPLAAAAcA6Hws20adOcXQcAAIBTVOoOxbt379aBAwdksVjUtm1b3XLLLc6qCwAAwCEOhZvMzEwNHTpUn3/+uerVqyfDMJSdna077rhD7777rho0aODsOgEAAMrFoXdLPf7448rJydG3336rU6dO6fTp0/rPf/6jnJwcjRs3ztk1AgAAlJtDMzcbN27Up59+qjZt2tjb2rZtq4ULFyo6OtppxQEAAFSUQzM3xcXFpd7+LUmenp6lPmcKAACgOjkUbnr16qUnnnhCaWlp9rbjx49rwoQJ6t27t9OKAwAAqCiHws2CBQuUm5urpk2bqnnz5mrRooUiIiKUm5ur119/3dk1AgAAlJtD19yEhYVpz549SkpK0nfffSfDMNS2bVv16dPH2fUBAABUSKXuc3PnnXfqzjvvdFYtAAAAlVbucDN//nw99NBDstlsmj9//jX78nZwAADgKuUON3PnztXw4cNls9k0d+7cq/azWCyEGwAA4DLlDjeHDx8u82sAAICaxKF3S12pqKhI+/bt0+nTp52xOgAAAIc5FG7Gjx+vZcuWSboUbG6//XZ17NhRYWFh+vzzz51ZHwAAQIU4FG7Wrl2rDh06SJI++OADHTlyRN99953Gjx+vKVOmOLVAAACAinAo3GRlZSk4OFiSlJiYqHvuuUetWrXSqFGjtH//fqcWCAAAUBEOhZugoCAlJyerqKhIGzdutN+8Ly8vT+7u7k4tEAAAoCIcuonfAw88oHvvvVchISGyWCz2G/l9+eWXat26tVMLBAAAqAiHws306dPVrl07HT16VPfcc4+sVqskyd3dXZMmTXJqgQAAABXh8McvDB48uFTbyJEjK1UMAABAZfHxCwAAwFT4+AUAAGAqfPwCAAAwFad8/AIAAEBN4VC4GTx4sF566aVS7X/72990zz33VLooAAAARzkUbrZs2aIBAwaUav/DH/6grVu3VrooAAAARzkUbs6ePSsvL69S7Z6ensrJyal0UQAAAI5yKNy0a9dOq1evLtX+7rvvqm3btpUuCgAAwFEO3cTvueee0913361Dhw6pV69ekqTNmzfrnXfe0Zo1a5xaIAAAQEU4FG7uuusuvf/++3rxxRe1du1aeXt7q3379vr000/Vo0cPZ9cIAABQbg5//MKAAQPKvKgYAADAlRy+z82ZM2e0dOlS/fWvf9WpU6ckSXv27NHx48edVhwAAEBFOTRz880336hPnz7y9/fXkSNHNHr0aAUEBOi9997TTz/9pDfffNPZdQIAAJSLQzM3cXFxiomJ0Q8//CCbzWZv79evH/e5AQAALuVQuPnqq6/08MMPl2pv1KiRMjIyKl0UAACAoxwKNzabrcyb9X3//fdq0KBBpYsCAABwlEPh5o9//KNmzpypixcvSpIsFotSU1M1adIk3X333U4tEAAAoCIcCjevvvqqTpw4oYYNG+r8+fPq0aOHWrRoobp16+qFF15wdo0AAADl5tC7pfz8/PTFF1/oX//6l/bs2aPi4mJ17NhRffr0cXZ9AAAAFVLhcFNYWCibzaZ9+/apV69e9o9fAAAAqAkqfFrKw8NDTZo0UVFRUVXUAwAAUCkOXXPz7LPPavLkyfY7EwMAANQUDl1zM3/+fP34448KDQ1VkyZN5OvrW+L1PXv2OKU4AACAinIo3AwaNEgWi0WGYTi7HgAAgEqpULjJy8vTxIkT9f777+vixYvq3bu3Xn/9dQUGBlZVfQAAABVSoWtupk2bppUrV2rAgAG677779Omnn+qRRx6pqtoAAAAqrEIzN+vXr9eyZcs0dOhQSdLw4cPVrVs3FRUVyd3dvUoKBAAAqIgKzdwcPXpU3bt3tz/v1KmTPDw8lJaW5vTCAAAAHFGhcFNUVCQvL68SbR4eHiosLHS4gISEBEVERMhmsykyMlLbtm0r13Lbt2+Xh4eHbr75Zoe3DQAAzKdCp6UMw1BMTIysVqu97cKFC4qNjS3xdvD169eXa32rV6/W+PHjlZCQoG7dumnx4sXq16+fkpOTFR4eftXlsrOzNWLECPXu3Vs///xzRXYBAACYXIVmbkaOHKmGDRvK39/f/vjLX/6i0NDQEm3lNWfOHI0aNUqjR49WmzZtNG/ePIWFhWnRokXXXO7hhx/WsGHD1KVLl4qUDwAArgMVmrlZsWKF0zZcUFCg3bt3a9KkSSXao6OjtWPHjmvWcOjQIb311luaNWvWb24nPz9f+fn59uc5OTmOFw0AAGo8hz5+wRmysrJUVFSkoKCgEu1BQUHKyMgoc5kffvhBkyZN0qpVq+ThUb5cFh8fX2JWKSwsrNK1AwCAmstl4eYyi8VS4rlhGKXapEsXMw8bNkwzZsxQq1atyr3+yZMnKzs72/44evRopWsGAAA1l0Mfv+AMgYGBcnd3LzVLk5mZWWo2R5Jyc3O1a9cu7d27V4899pgkqbi4WIZhyMPDQ5s2bVKvXr1KLWe1WktcAA0AAMzNZTM3Xl5eioyMVFJSUon2pKQkde3atVR/Pz8/7d+/X/v27bM/YmNjdcMNN2jfvn3q3LlzdZUOAABqMJfN3EhSXFyc7r//fkVFRalLly76+9//rtTUVMXGxkq6dErp+PHjevPNN+Xm5qZ27dqVWL5hw4ay2Wyl2gEAwPXLpeFmyJAhOnnypGbOnKn09HS1a9dOiYmJatKkiSQpPT1dqampriwRAADUMhbDMAxXF1GdcnJy5O/vr+zsbPn5+TltvZm5F9Tphc1ys0gp8QOctl4AAFCx47fL3y0FAADgTIQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKoQbAABgKi4PNwkJCYqIiJDNZlNkZKS2bdt21b7r16/XnXfeqQYNGsjPz09dunTRJ598Uo3VAgCAms6l4Wb16tUaP368pkyZor1796p79+7q16+fUlNTy+y/detW3XnnnUpMTNTu3bt1xx13aODAgdq7d281Vw4AAGoqi2EYhqs23rlzZ3Xs2FGLFi2yt7Vp00aDBg1SfHx8udZx4403asiQIZo6dWq5+ufk5Mjf31/Z2dny8/NzqO6yZOZeUKcXNsvNIqXED3DaegEAQMWO3y6buSkoKNDu3bsVHR1doj06Olo7duwo1zqKi4uVm5urgICAq/bJz89XTk5OiQcAADAvl4WbrKwsFRUVKSgoqER7UFCQMjIyyrWO2bNn69y5c7r33nuv2ic+Pl7+/v72R1hYWKXqBgAANZvLLyi2WCwlnhuGUaqtLO+8846mT5+u1atXq2HDhlftN3nyZGVnZ9sfR48erXTNAACg5vJw1YYDAwPl7u5eapYmMzOz1GzOlVavXq1Ro0ZpzZo16tOnzzX7Wq1WWa3WStcLAABqB5fN3Hh5eSkyMlJJSUkl2pOSktS1a9erLvfOO+8oJiZGb7/9tgYM4MJdAABQkstmbiQpLi5O999/v6KiotSlSxf9/e9/V2pqqmJjYyVdOqV0/Phxvfnmm5IuBZsRI0botdde06233mqf9fH29pa/v7/L9gMAANQcLg03Q4YM0cmTJzVz5kylp6erXbt2SkxMVJMmTSRJ6enpJe55s3jxYhUWFurRRx/Vo48+am8fOXKkVq5cWd3lAwCAGsil97lxBe5zAwBA7VMr7nMDAABQFQg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3TlZsSE0nfaSLRcXad/SMCouKXV0SAADXFZeHm4SEBEVERMhmsykyMlLbtm27Zv8tW7YoMjJSNptNzZo10xtvvFFNlVZMyykfa9DC7Wox5WM1nfSR7vv7/2nC6n06dOIsgQcAgCrk4cqNr169WuPHj1dCQoK6deumxYsXq1+/fkpOTlZ4eHip/ocPH1b//v01ZswYvfXWW9q+fbvGjh2rBg0a6O6773bBHpTfzpSTkqT39h4v0f6XW8P108k8FRYZavQ7bxUbhny83BVU16Yz5y+qrs1Dnu5uyrlwUcF+NmWfv6g6Vg/5eHno/MUi5eUXqp6vl87lF+rCxSLVtXmquNhQYbGhn06eU4j/pXUWG5faUk/mKbSeTe5ul3Lt0VN5ahlUR6fPFah+Hau8Pd1VZBg6fa5AIf42ublZ9HPOBYX4e8v4Vd0/Z19QSD2bLLLIYpGKf1kmsI5VhiTDkAwZMn5Z6PI6pEvt+qWPJGWfvygvdzd5e7n/dxvGf7dWUGQo5/xFBda1yvJLm8UiWWTR6bwC1bF6yMvjvznd+HWhv2zvRG6+gv1sJdotlv9+nXO+UF4ebvL2dC9R49XW+XNOvoL8rLqSUarlktN5BfLxdJftl/X/lmJDOpFb9jautR1JOpdfKMOQ6toq/ut9IjdfgXWtcrOUNY6/+tr4dfvVxyr7/EVZPS59b6/FIss1X/8554KC/W3X7FMROecvyt3NIh+v0mN05f7Y268y6Nf6XmTl5svP21Ne7tfev/Ku71rSzlxQaD3njdG1ZOXmq67Ns8TvXXkZVxvIckrPvqDQet6VWoejTp4rUF2rh6yerp0XOJN3UV4ebvL5jd8rV7FIuq1lA0UE+rquBqOyP2mV0LlzZ3Xs2FGLFi2yt7Vp00aDBg1SfHx8qf7PPPOMNmzYoAMHDtjbYmNj9fXXX2vnzp3l2mZOTo78/f2VnZ0tPz+/yu/ELzJzL6jTC5udtj4AAGqr1sF1tXH87U5dZ0WO3y6buSkoKNDu3bs1adKkEu3R0dHasWNHmcvs3LlT0dHRJdr69u2rZcuW6eLFi/L09Cy1TH5+vvLz8+3Pc3JynFB9aUXF/82Ib43qrNtaBpbqYxiGig3pw2/StPCzH3VDsJ/OXriowmJDVg93nc4rkKe7RXWsHko5cU6Bda2SIZ3KK1CzQF+dOJuvgsJiBfnZdOFikX7OuaDmDerobH6hss9fVJP6PnJ3c5OHm0WHs84pItBXNk93ubtJHm5uOnY6Tw39bLJ5uMtikX7IPKum9X10OOucfL085O3lrotFxTp04pxaNKyjYsPQt8ez1TKortx++cfTIosOZOSoZcM6svwy9VFUbOj7jFy1Dq77y2yIxT4rYpGUnJ6jVkF15fZL46VZl0t+zrkgLw831ff97wyFfVmLdL6gSGlnLqhZA1/7f86XZ4RST+XJ39tTft6lv++/9u3xbLUN9b/yu2H/6viZyzV4/beGX/W0lPin26LktGy1CfGz7/+Vrmw+fvq8fK3u8vf2KrN/aYa+TcvRjaVq/m0nz136GQlxYJbjx8yzCgvwkbXUf+Mld+jK/btyFC6/nnbmgrw93VXP59rfn9/ybVqO2oY68R+RnAtyd7MosE7Zs2JS6X20t19lhqms/j+dzFNgXavqWCv+3/VvzWRd6UB6jlqH1K3wco5IPZWn3/l6OTQzWFkH0nN0w6/+llSno6cv/b1xxX7/2uXfq9/5Vu736krO+tk5djpPXx/L1i3h9ZyyPke57LuUlZWloqIiBQUFlWgPCgpSRkZGmctkZGSU2b+wsFBZWVkKCQkptUx8fLxmzJjhvMKvIqiuTbc2C1BgHWuZwUaSLBaL3C3SH29upD/e3KjKawIA4Hrk8guKr/zv1zCMq/5HfLX+ZbVfNnnyZGVnZ9sfR48erWTFZXNzs+jdh7powbCOVbJ+AABQPi6buQkMDJS7u3upWZrMzMxSszOXBQcHl9nfw8ND9evXL3MZq9Uqq/Xq088AAMBcXDZz4+XlpcjISCUlJZVoT0pKUteuXctcpkuXLqX6b9q0SVFRUWVebwMAAK4/Lj0tFRcXp6VLl2r58uU6cOCAJkyYoNTUVMXGxkq6dEppxIgR9v6xsbH66aefFBcXpwMHDmj58uVatmyZnnrqKVftAgAAqGFcetn3kCFDdPLkSc2cOVPp6elq166dEhMT1aRJE0lSenq6UlNT7f0jIiKUmJioCRMmaOHChQoNDdX8+fNr/D1uAABA9XHpfW5coarucwMAAKpORY7fLn+3FAAAgDMRbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKkQbgAAgKm49OMXXOHyDZlzcnJcXAkAACivy8ft8nywwnUXbnJzcyVJYWFhLq4EAABUVG5urvz9/a/Z57r7bKni4mKlpaWpbt26slgsTl13Tk6OwsLCdPToUT63qgoxztWDca4ejHP1YayrR1WNs2EYys3NVWhoqNzcrn1VzXU3c+Pm5qbGjRtX6Tb8/Pz4xakGjHP1YJyrB+NcfRjr6lEV4/xbMzaXcUExAAAwFcINAAAwFcKNE1mtVk2bNk1Wq9XVpZga41w9GOfqwThXH8a6etSEcb7uLigGAADmxswNAAAwFcINAAAwFcINAAAwFcINAAAwFcJNBSUkJCgiIkI2m02RkZHatm3bNftv2bJFkZGRstlsatasmd54441qqrR2q8g4r1+/XnfeeacaNGggPz8/denSRZ988kk1Vlt7VfTn+bLt27fLw8NDN998c9UWaBIVHef8/HxNmTJFTZo0kdVqVfPmzbV8+fJqqrb2qug4r1q1Sh06dJCPj49CQkL0wAMP6OTJk9VUbe20detWDRw4UKGhobJYLHr//fd/cxmXHAcNlNu7775reHp6GkuWLDGSk5ONJ554wvD19TV++umnMvunpKQYPj4+xhNPPGEkJycbS5YsMTw9PY21a9dWc+W1S0XH+YknnjBefvll49///rdx8OBBY/LkyYanp6exZ8+eaq68dqnoOF925swZo1mzZkZ0dLTRoUOH6im2FnNknO+66y6jc+fORlJSknH48GHjyy+/NLZv316NVdc+FR3nbdu2GW5ubsZrr71mpKSkGNu2bTNuvPFGY9CgQdVcee2SmJhoTJkyxVi3bp0hyXjvvfeu2d9Vx0HCTQV06tTJiI2NLdHWunVrY9KkSWX2f/rpp43WrVuXaHv44YeNW2+9tcpqNIOKjnNZ2rZta8yYMcPZpZmKo+M8ZMgQ49lnnzWmTZtGuCmHio7zxx9/bPj7+xsnT56sjvJMo6Lj/Le//c1o1qxZibb58+cbjRs3rrIazaY84cZVx0FOS5VTQUGBdu/erejo6BLt0dHR2rFjR5nL7Ny5s1T/vn37ateuXbp48WKV1VqbOTLOVyouLlZubq4CAgKqokRTcHScV6xYoUOHDmnatGlVXaIpODLOGzZsUFRUlF555RU1atRIrVq10lNPPaXz589XR8m1kiPj3LVrVx07dkyJiYkyDEM///yz1q5dqwEDBlRHydcNVx0Hr7sPznRUVlaWioqKFBQUVKI9KChIGRkZZS6TkZFRZv/CwkJlZWUpJCSkyuqtrRwZ5yvNnj1b586d07333lsVJZqCI+P8ww8/aNKkSdq2bZs8PPjTUR6OjHNKSoq++OIL2Ww2vffee8rKytLYsWN16tQprru5CkfGuWvXrlq1apWGDBmiCxcuqLCwUHfddZdef/316ij5uuGq4yAzNxVksVhKPDcMo1Tbb/Uvqx0lVXScL3vnnXc0ffp0rV69Wg0bNqyq8kyjvONcVFSkYcOGacaMGWrVqlV1lWcaFfl5Li4ulsVi0apVq9SpUyf1799fc+bM0cqVK5m9+Q0VGefk5GSNGzdOU6dO1e7du7Vx40YdPnxYsbGx1VHqdcUVx0H+/SqnwMBAubu7l/ovIDMzs1QqvSw4OLjM/h4eHqpfv36V1VqbOTLOl61evVqjRo3SmjVr1KdPn6oss9ar6Djn5uZq165d2rt3rx577DFJlw7ChmHIw8NDmzZtUq9evaql9trEkZ/nkJAQNWrUSP7+/va2Nm3ayDAMHTt2TC1btqzSmmsjR8Y5Pj5e3bp108SJEyVJ7du3l6+vr7p3765Zs2Yxs+4krjoOMnNTTl5eXoqMjFRSUlKJ9qSkJHXt2rXMZbp06VKq/6ZNmxQVFSVPT88qq7U2c2ScpUszNjExMXr77bc5Z14OFR1nPz8/7d+/X/v27bM/YmNjdcMNN2jfvn3q3LlzdZVeqzjy89ytWzelpaXp7Nmz9raDBw/Kzc1NjRs3rtJ6aytHxjkvL09ubiUPge7u7pL+O7OAynPZcbBKL1c2mctvNVy2bJmRnJxsjB8/3vD19TWOHDliGIZhTJo0ybj//vvt/S+/BW7ChAlGcnKysWzZMt4KXg4VHee3337b8PDwMBYuXGikp6fbH2fOnHHVLtQKFR3nK/FuqfKp6Djn5uYajRs3NgYPHmx8++23xpYtW4yWLVsao0ePdtUu1AoVHecVK1YYHh4eRkJCgnHo0CHjiy++MKKiooxOnTq5ahdqhdzcXGPv3r3G3r17DUnGnDlzjL1799rfcl9TjoOEmwpauHCh0aRJE8PLy8vo2LGjsWXLFvtrI0eONHr06FGi/+eff27ccssthpeXl9G0aVNj0aJF1Vxx7VSRce7Ro4chqdRj5MiR1V94LVPRn+dfI9yUX0XH+cCBA0afPn0Mb29vo3HjxkZcXJyRl5dXzVXXPhUd5/nz5xtt27Y1vL29jZCQEGP48OHGsWPHqrnq2uWzzz675t/bmnIctBgG828AAMA8uOYGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGACQ1bdpU8+bNsz+3WCx6//33XVYPAMcRbgC4XExMjCwWiywWizw8PBQeHq5HHnlEp0+fdnVpAGohwg2AGuEPf/iD0tPTdeTIES1dulQffPCBxo4d6+qyANRChBsANYLValVwcLAaN26s6OhoDRkyRJs2bbK/vmLFCrVp00Y2m02tW7dWQkJCieWPHTumoUOHKiAgQL6+voqKitKXX34pSTp06JD++Mc/KigoSHXq1NHvf/97ffrpp9W6fwCqj4erCwCAK6WkpGjjxo3y9PSUJC1ZskTTpk3TggULdMstt2jv3r0aM2aMfH19NXLkSJ09e1Y9evRQo0aNtGHDBgUHB2vPnj0qLi6WJJ09e1b9+/fXrFmzZLPZ9I9//EMDBw7U999/r/DwcFfuKoAqQLgBUCN8+OGHqlOnjoqKinThwgVJ0pw5cyRJzz//vGbPnq0///nPkqSIiAglJydr8eLFGjlypN5++22dOHFCX331lQICAiRJLVq0sK+7Q4cO6tChg/35rFmz9N5772nDhg167LHHqmsXAVQTwg2AGuGOO+7QokWLlJeXp6VLl+rgwYN6/PHHdeLECR09elSjRo3SmDFj7P0LCwvl7+8vSdq3b59uueUWe7C50rlz5zRjxgx9+OGHSktLU2Fhoc6fP6/U1NRq2TcA1YtwA6BG8PX1tc+2zJ8/X3fccYdmzJhhn1lZsmSJOnfuXGIZd3d3SZK3t/c11z1x4kR98sknevXVV9WiRQt5e3tr8ODBKigoqII9AeBqhBsANdK0adPUr18/PfLII2rUqJFSUlI0fPjwMvu2b99eS5cu1alTp8qcvdm2bZtiYmL0pz/9SdKla3COHDlSleUDcCHeLQWgRurZs6duvPFGvfjii5o+fbri4+P12muv6eDBg9q/f79WrFhhvybnvvvuU3BwsAYNGqTt27crJSVF69at086dOyVduv5m/fr12rdvn77++msNGzbMfrExAPMh3ACoseLi4rRkyRL17dtXS5cu1cqVK3XTTTepR48eWrlypSIiIiRJXl5e2rRpkxo2bKj+/fvrpptu0ksvvWQ/bTV37lz97ne/U9euXTVw4ED17dtXHTt2dOWuAahCFsMwDFcXAQAA4CzM3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFP5f610PSNYY9IBAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report saved to /Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/xgBoost with SMOTE sacle_pos_weight=5_Test_Report_20241101_185010.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate a timestamp for the report filename\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "# Define the report title and output filename\n",
    "classification_report_title = \"Report\"\n",
    "classification_report_filename = f\"{model_specs}_{dataset_type}_{classification_report_title}_{timestamp}.txt\"\n",
    "\n",
    "# Execute training and evaluation\n",
    "if use_test_data:\n",
    "    # Load and preprocess the test data\n",
    "    X_test, y_test = load_and_preprocess(df)  # Ensure `df` is the test DataFrame\n",
    "    # Evaluate the model (make sure `evaluate_model` is defined)\n",
    "    accuracy, clf, roc_auc, auc_pr = evaluate_model(xgb_model, X_test, y_test)\n",
    "    # Generate and save the evaluation report\n",
    "    generate_and_save_report(xgb_model, accuracy, clf, roc_auc, auc_pr, report_dir=reports_output_dir, file_name=classification_report_filename)\n",
    "\n",
    "else:\n",
    "    # Assuming `df` is the training DataFrame\n",
    "    X_train, y_train = load_and_preprocess(df)  # Ensure `df` is the training DataFrame\n",
    "    # Train the XGBoost model with SMOTE\n",
    "    xgb_model, accuracy, clf, roc_auc, auc_pr = train_xgboost_with_smote(X_train, y_train)\n",
    "    # Generate and save the training report\n",
    "    generate_and_save_report(xgb_model, accuracy, clf, roc_auc, auc_pr, report_dir=reports_output_dir, file_name=classification_report_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "d83b384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgBoost with SMOTE sacle_pos_weight=5_Test END Model ....   completed at Fri Nov  1 18:45:20 2024. Elapsed time: 0 minutes and 3.93 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(f\"{model_specs}_{dataset_type} END Model ....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "875d9134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed the DASK Client completed at Fri Nov  1 18:45:21 2024. Elapsed time: 0 minutes and 0.40 seconds\n",
      "\n",
      ".................................................   completed at Fri Nov  1 18:45:21 2024. Elapsed time: 0 minutes and 0.40 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Close Dask client when done\n",
    "client.close()\n",
    "log_time(\"Closed the DASK Client\", start_time)\n",
    "log_time(\".................................................  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "4713ea62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/models/xgboost_with_smote_sacle_pos_weight=5_Test.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Specify the output directory\n",
    "output_dir_model = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/models'\n",
    "if not os.path.exists(output_dir_model):\n",
    "    os.makedirs(output_dir_model)  # Ensure the directory exists\n",
    "\n",
    "# Define model filename\n",
    "model_outputfilename = f\"{model_specs.replace(' ', '_').replace(',', '').lower()}_{dataset_type}.pkl\"\n",
    "\n",
    "# Assuming best_rf_model is your final or best model\n",
    "try:\n",
    "    # Save the model\n",
    "    with open(os.path.join(output_dir_model, model_outputfilename), 'wb') as model_file:\n",
    "        pickle.dump(xgb_model, model_file)\n",
    "\n",
    "    print(f\"Model saved to {os.path.join(output_dir_model, model_outputfilename)}\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"Error: 'best_rf_model' is not defined. Please ensure the model is assigned before saving.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "0d57811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook ended at: Fri Nov  1 18:45:21 2024\n",
      "Total execution time: 6 minutes and 15.46 seconds\n",
      "xgBoost with SMOTE sacle_pos_weight=5_Test Notebook Ended at...  completed at Fri Nov  1 18:45:21 2024. Elapsed time: 6 minutes and 15.46 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Assuming start_time is defined earlier in the notebook\n",
    "end_time_notebook = time.time()\n",
    "elapsed_time = end_time_notebook - start_time_notebook\n",
    "\n",
    "# Print and format the notebook end time and total execution time\n",
    "print(f\"Notebook ended at: {time.ctime(end_time_notebook)}\")\n",
    "print(f\"Total execution time: {elapsed_time // 60:.0f} minutes and {elapsed_time % 60:.2f} seconds\")\n",
    "\n",
    "\n",
    "log_time(f\"{model_specs}_{dataset_type} Notebook Ended at... \", start_time_notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e445b3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40c359fe",
   "metadata": {},
   "source": [
    "# moving now to xgBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be1135",
   "metadata": {},
   "source": [
    "#   # Initialize XGBoost with custom scale_pos_weight\n",
    "    #The scale_pos_weight parameter was set to 10, which may be impacting the balance. \n",
    "    #Lowering this to values around\n",
    "    #model = XGBClassifier(scale_pos_weight=10, random_state=42)  # Adjust scale_pos_weight as needed\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6c4bbeb",
   "metadata": {},
   "source": [
    "\n",
    "Train Accuracy: 0.9452449601254762\n",
    "Classification Report (Train):\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.99      0.90      0.94   1289169\n",
    "         1.0       0.91      0.99      0.95   1289169\n",
    "\n",
    "    accuracy                           0.95   2578338\n",
    "   macro avg       0.95      0.95      0.95   2578338\n",
    "weighted avg       0.95      0.95      0.95   2578338\n",
    "\n",
    "Train ROC AUC: 0.9452449601254762\n",
    "Train Precision-Recall AUC: 0.9515018817782082\n",
    "    \n",
    "High Overall Accuracy: 94.4%94.4%\n",
    "High Precision and Recall for Both Classes:\n",
    "\n",
    "    Non-Fraud Class (0): Precision is high at 99%99%, and recall is also strong at 90%90%.\n",
    "    Fraud Class (1): Precision is 91%91%, and recall is 99%99%.\n",
    "\n",
    "Balanced Metrics: The macro and weighted averages for precision, recall, and F1-scores are balanced, indicating that the model is distinguishing well between fraud and non-fraud cases.\n",
    "ROC AUC and Precision-Recall AUC: Both scores are above 0.940.94, reflecting good predictive capability for both true positives and managing false positives.\n",
    "\n",
    "\n",
    "on TEST DATA\n",
    "\n",
    "Test Accuracy: 0.63207844252221 Classification Report (Test): precision recall f1-score support\n",
    "\n",
    "     0.0       1.00      0.63      0.77    553574\n",
    "     1.0       0.00      0.38      0.01      2145\n",
    "\n",
    "accuracy                           0.63    555719\n",
    "macro avg 0.50 0.50 0.39 555719 weighted avg 0.99 0.63 0.77 555719\n",
    "\n",
    "Test ROC AUC: 0.5048790018644095 Test Precision-Recall AUC: 0.0039033652757451153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dddbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d7da0d5",
   "metadata": {},
   "source": [
    "scale_pos_weight = 5 and threshold for test is no set to 0.3\n",
    "\n",
    "Train\n",
    "-------\n",
    "\n",
    "Accuracy: 0.9807\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.99      0.98      0.98   1289169\n",
    "         1.0       0.98      0.99      0.98   1289169\n",
    "\n",
    "    accuracy                           0.98   2578338\n",
    "   macro avg       0.98      0.98      0.98   2578338\n",
    "weighted avg       0.98      0.98      0.98   2578338\n",
    "\n",
    "\n",
    "ROC AUC: 0.9807\n",
    "\n",
    "Precision-Recall AUC: 0.9843\n",
    "\n",
    "\n",
    "Test\n",
    "------\n",
    "\n",
    "Test Accuracy: 0.515015682386242\n",
    "Classification Report (Test):\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "         0.0       1.00      0.52      0.68    553574\n",
    "         1.0       0.00      0.48      0.01      2145\n",
    "\n",
    "    accuracy                           0.52    555719\n",
    "   macro avg       0.50      0.50      0.34    555719\n",
    "weighted avg       0.99      0.52      0.68    555719\n",
    "\n",
    "Test ROC AUC: 0.4988295448008151\n",
    "Test Precision-Recall AUC: 0.0038914212788521436"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5914ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfb3843f",
   "metadata": {},
   "source": [
    "Threshold = 0.2\n",
    "---------------\n",
    "\n",
    "Test Accuracy: 0.4106553851856784\n",
    "Classification Report (Test):\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "         0.0       1.00      0.41      0.58    553574\n",
    "         1.0       0.00      0.59      0.01      2145\n",
    "\n",
    "    accuracy                           0.41    555719\n",
    "   macro avg       0.50      0.50      0.29    555719\n",
    "weighted avg       0.99      0.41      0.58    555719\n",
    "\n",
    "Test ROC AUC: 0.4991559290881513\n",
    "Test Precision-Recall AUC: 0.003910979744480943"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dae3a0",
   "metadata": {},
   "source": [
    "Next Steps to Finalize\n",
    "\n",
    "Given these observations, here’s a plan to move toward your final configuration:\n",
    "\n",
    "    Remove SMOTE and Retrain:\n",
    "        Try training the model without SMOTE, using scale_pos_weight = 5. Removing SMOTE may improve generalization, as SMOTE can sometimes overfit to synthetic patterns, especially with high class imbalance.\n",
    "\n",
    "    Threshold Tuning Post-SMOTE Removal:\n",
    "        Start with a threshold of 0.3 for the test evaluation after removing SMOTE, as this has given a balanced trade-off in previous runs. You can adjust further if needed.\n",
    "\n",
    "    Prepare Results for Presentation:\n",
    "        Document each setting’s results (with and without SMOTE, different thresholds) for your presentation to illustrate your tuning process and rationale for the final model selection.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ZHAW_Project)",
   "language": "python",
   "name": "zhaw_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
