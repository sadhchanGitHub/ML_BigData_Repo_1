{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cb1d97",
   "metadata": {},
   "source": [
    "# v2.1 - Random Forest Balanced SMOTE with DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e22053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn import preprocessing\n",
    "import geopy\n",
    "from geopy.distance import geodesic\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import dask.dataframe as dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ff9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../references')  # Add the references folder to the system path\n",
    "model_specs = 'RandomForest_Balanced_SMOTE_GridSearch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab52f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_notebook = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c56c046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/RandomForest\n"
     ]
    }
   ],
   "source": [
    "# Directory to save the figures \n",
    "\n",
    "input_src_dir = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/data/raw'\n",
    "output_dir_figures_train = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/figures/train_figures'\n",
    "output_dir_figures_test = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/figures/test_figures'\n",
    "#reports_output_dir = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports'\n",
    "\n",
    "\n",
    "reports_output_dir_base = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports'\n",
    "# reports_output_dir for DecisionTrees\n",
    "reports_output_dir = f\"{reports_output_dir_base}/RandomForest\"\n",
    "print(reports_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e37d27bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    }
   ],
   "source": [
    "# Define which dataset to use\n",
    "use_test_data = False  # Set to True when using fraudtest.csv\n",
    "\n",
    "# Determine dataset type based on the variable\n",
    "dataset_type = 'Test' if use_test_data else 'Train'\n",
    "print(dataset_type)\n",
    "\n",
    "# Load the appropriate dataset\n",
    "\n",
    "if use_test_data:\n",
    "    output_dir_figures = output_dir_figures_test\n",
    "else:\n",
    "    output_dir_figures = output_dir_figures_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0bc5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the preprocess file name dynamically\n",
    "# Get the current timestamp\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")  # Format: YYYYMMDD_HHMMSS\n",
    "\n",
    "logfile_title = 'LogFile'\n",
    "logfile_name = f\"{model_specs}_{dataset_type}_{logfile_title.replace(',', '').lower().split('.')[0]}_{timestamp}.txt\"\n",
    "\n",
    "logfile_path = os.path.join(reports_output_dir, logfile_name)\n",
    "\n",
    "# Function to log times to a file\n",
    "def log_time(step_name, start_time):\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    log_message = (f\"{step_name} completed at {time.ctime(end_time)}. \"\n",
    "                   f\"Elapsed time: {elapsed_time // 60:.0f} minutes and {elapsed_time % 60:.2f} seconds\\n\")\n",
    "    \n",
    "    # Append log to file\n",
    "    with open(logfile_path, 'a') as f:\n",
    "        f.write(log_message)\n",
    "    \n",
    "    # Print the message to the console as well\n",
    "    print(log_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "383fd5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest_Balanced_SMOTE_GridSearch_Train Notebook  started at...  completed at Sun Nov  3 18:31:09 2024. Elapsed time: 0 minutes and 0.03 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(f\"{model_specs.replace(',', '')}_{dataset_type} Notebook  started at... \", start_time_notebook)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a17fc867",
   "metadata": {},
   "source": [
    "# Load the appropriate dataset\n",
    "\n",
    "if use_test_data:\n",
    "    df = pd.read_csv(f\"{input_src_dir}/fraudTest.csv\")  # Concatenate the directory with the filename\n",
    "else:\n",
    "    df = pd.read_csv(f\"{input_src_dir}/fraudTrain.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b58eee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the DASK Client completed at Sun Nov  3 18:31:10 2024. Elapsed time: 0 minutes and 0.39 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 60942 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import dask.dataframe as dd\n",
    "\n",
    "log_time(\"Starting the DASK Client\", start_time)\n",
    "\n",
    "# Start Dask client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b76b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the dataset directly into Dask\n",
    "if use_test_data:\n",
    "    df_pre = dd.read_csv(f\"{input_src_dir}/fraudTest.csv\", assume_missing=True)\n",
    "else:\n",
    "    df_pre = dd.read_csv(f\"{input_src_dir}/fraudTrain.csv\", assume_missing=True)\n",
    "\n",
    "\n",
    "# Now proceed with preprocessing, feature engineering, and model training on `df`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abe2bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 5\n",
      "Columns: Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud'],\n",
      "      dtype='object')\n",
      "   Unnamed: 0      TransactionTime  CreditCardNumber  \\\n",
      "0         0.0  2019-01-01 00:00:18      2.703186e+15   \n",
      "1         1.0  2019-01-01 00:00:44      6.304233e+11   \n",
      "2         2.0  2019-01-01 00:00:51      3.885949e+13   \n",
      "3         3.0  2019-01-01 00:01:16      3.534094e+15   \n",
      "4         4.0  2019-01-01 00:03:06      3.755342e+14   \n",
      "\n",
      "                             merchant       category  TransactionAmount  \\\n",
      "0          fraud_Rippin, Kub and Mann       misc_net               4.97   \n",
      "1     fraud_Heller, Gutmann and Zieme    grocery_pos             107.23   \n",
      "2                fraud_Lind-Buckridge  entertainment             220.11   \n",
      "3  fraud_Kutch, Hermiston and Farrell  gas_transport              45.00   \n",
      "4                 fraud_Keeling-Crist       misc_pos              41.96   \n",
      "\n",
      "       first     last gender                        street  ...      lat  \\\n",
      "0   Jennifer    Banks      F                561 Perry Cove  ...  36.0788   \n",
      "1  Stephanie     Gill      F  43039 Riley Greens Suite 393  ...  48.8878   \n",
      "2     Edward  Sanchez      M      594 White Dale Suite 530  ...  42.1808   \n",
      "3     Jeremy    White      M   9443 Cynthia Court Apt. 038  ...  46.2306   \n",
      "4      Tyler   Garcia      M              408 Bradley Rest  ...  38.4207   \n",
      "\n",
      "       long  city_pop                                job  DateOfBirth  \\\n",
      "0  -81.1781    3495.0          Psychologist, counselling   1988-03-09   \n",
      "1 -118.2105     149.0  Special educational needs teacher   1978-06-21   \n",
      "2 -112.2620    4154.0        Nature conservation officer   1962-01-19   \n",
      "3 -112.1138    1939.0                    Patent attorney   1967-01-12   \n",
      "4  -79.4629      99.0     Dance movement psychotherapist   1986-03-28   \n",
      "\n",
      "                          trans_num     unix_time  merch_lat  merch_long  \\\n",
      "0  0b242abb623afc578575680df30655b9  1.325376e+09  36.011293  -82.048315   \n",
      "1  1f76529f8574734946361c461b024d99  1.325376e+09  49.159047 -118.186462   \n",
      "2  a1a22d70485983eac12b5b88dad1cf95  1.325376e+09  43.150704 -112.154481   \n",
      "3  6b849c168bdad6f867558c3793159a81  1.325376e+09  47.034331 -112.561071   \n",
      "4  a41d7549acf90789359a9aa5346dcb46  1.325376e+09  38.674999  -78.632459   \n",
      "\n",
      "   is_fraud  \n",
      "0       0.0  \n",
      "1       0.0  \n",
      "2       0.0  \n",
      "3       0.0  \n",
      "4       0.0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Optionally repartition the dataset if necessary\n",
    "df_pre = df_pre.repartition(npartitions=5)\n",
    "\n",
    "\n",
    "# Strip whitespace from column names and rename\n",
    "df_pre.columns = df_pre.columns.str.strip()\n",
    "df_pre = df_pre.rename(columns={'amt': 'TransactionAmount', 'cc_num': 'CreditCardNumber', 'dob': 'DateOfBirth', 'trans_date_trans_time': 'TransactionTime'})\n",
    "\n",
    "# Repartition to ensure consistency\n",
    "df_pre = df_pre.repartition(npartitions=5)\n",
    "\n",
    "# Preview without converting to Pandas\n",
    "print(\"Number of partitions:\", df_pre.npartitions)\n",
    "print(\"Columns:\", df_pre.columns)\n",
    "print(df_pre.head())  # No .compute() here\n",
    "\n",
    "# Final transformations or computations\n",
    "# Only use .compute() at the very end if needed, for example:\n",
    "df = df_pre.compute()  # Converts to Pandas for the entire dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a11b824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud'],\n",
      "      dtype='object')\n",
      "(1296675, 23)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9042c84c",
   "metadata": {},
   "source": [
    "df.columns = df.columns.str.strip()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6829be1",
   "metadata": {},
   "source": [
    "df = df.rename(columns={'amt': 'TransactionAmount', 'cc_num': 'CreditCardNumber', 'dob': 'DateOfBirth', 'trans_date_trans_time': 'TransactionTime'})\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7500a890",
   "metadata": {},
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73a7c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas and generate TransactionID\n",
    "df = df_pre.compute()\n",
    "df = df.reset_index(drop=True)\n",
    "df['TransactionID'] = df.index + 1  # Unique TransactionID\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c847926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Columns:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02327c04",
   "metadata": {},
   "source": [
    "#debug\n",
    "import pandas as pd\n",
    "df_check = pd.read_csv(f\"{input_src_dir}/fraudTrain.csv\")\n",
    "print(\"Original CSV Row Count:\", len(df_check))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96d16888",
   "metadata": {},
   "source": [
    "#debug\n",
    "df = dd.read_csv(f\"{input_src_dir}/fraudTrain.csv\", assume_missing=True)\n",
    "row_count = df.size.compute() // len(df.columns)  # Approximate row count\n",
    "print(\"Row count after loading:\", row_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3babc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " Unnamed: 0           0\n",
      "TransactionTime      0\n",
      "CreditCardNumber     0\n",
      "merchant             0\n",
      "category             0\n",
      "TransactionAmount    0\n",
      "first                0\n",
      "last                 0\n",
      "gender               0\n",
      "street               0\n",
      "city                 0\n",
      "state                0\n",
      "zip                  0\n",
      "lat                  0\n",
      "long                 0\n",
      "city_pop             0\n",
      "job                  0\n",
      "DateOfBirth          0\n",
      "trans_num            0\n",
      "unix_time            0\n",
      "merch_lat            0\n",
      "merch_long           0\n",
      "is_fraud             0\n",
      "TransactionID        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values)\n",
    "\n",
    "# no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dc29333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_fraud\n",
      "0.0    1289169\n",
      "1.0       7506\n",
      "Name: count, dtype: int64\n",
      "is_fraud\n",
      "0.0    99.421135\n",
      "1.0     0.578865\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Count of fraud and non-fraud transactions\n",
    "fraud_counts = df['is_fraud'].value_counts()\n",
    "print(fraud_counts)\n",
    "\n",
    "# Optionally, you can get it in percentage terms\n",
    "fraud_percentage = df['is_fraud'].value_counts(normalize=True) * 100\n",
    "print(fraud_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c60dd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many unique credit cards in the data set ??\n",
    "df['CreditCardNumber'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "391577c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75b3c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TransactionTime to datetime\n",
    "df['TransactionTime'] = pd.to_datetime(df['TransactionTime'])\n",
    "\n",
    "# Optional: Convert DateOfBirth to datetime, if needed\n",
    "df['DateOfBirth'] = pd.to_datetime(df['DateOfBirth'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52941f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-01 00:00:18', '2019-01-01 00:00:44',\n",
      "               '2019-01-01 00:00:51', '2019-01-01 00:01:16',\n",
      "               '2019-01-01 00:03:06', '2019-01-01 00:04:08',\n",
      "               '2019-01-01 00:04:42', '2019-01-01 00:05:08',\n",
      "               '2019-01-01 00:05:18', '2019-01-01 00:06:01',\n",
      "               ...\n",
      "               '2020-06-21 12:08:42', '2020-06-21 12:09:22',\n",
      "               '2020-06-21 12:10:56', '2020-06-21 12:11:23',\n",
      "               '2020-06-21 12:11:36', '2020-06-21 12:12:08',\n",
      "               '2020-06-21 12:12:19', '2020-06-21 12:12:32',\n",
      "               '2020-06-21 12:13:36', '2020-06-21 12:13:37'],\n",
      "              dtype='datetime64[ns]', name='TransactionTime', length=1296675, freq=None)\n"
     ]
    }
   ],
   "source": [
    "# Set 'TransactionTime' as the index permanently\n",
    "df.set_index('TransactionTime', inplace=True)\n",
    "\n",
    "# Verify the index\n",
    "print(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55397673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Transaction Time: 2019-01-01 00:00:18\n",
      "Maximum Transaction Time: 2020-06-21 12:13:37\n"
     ]
    }
   ],
   "source": [
    "# Get the minimum and maximum transaction times from the index\n",
    "min_time = df.index.min()\n",
    "max_time = df.index.max()\n",
    "\n",
    "print(f\"Minimum Transaction Time: {min_time}\")\n",
    "print(f\"Maximum Transaction Time: {max_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1fc7ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CreditCardNumber</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>TransactionAmount</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>DateOfBirth</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>TransactionID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.703186e+15</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>Moravian Falls</td>\n",
       "      <td>...</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495.0</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1.325376e+09</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.304233e+11</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>Orient</td>\n",
       "      <td>...</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149.0</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1.325376e+09</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:51</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.885949e+13</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>Malad City</td>\n",
       "      <td>...</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154.0</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1.325376e+09</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:01:16</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.534094e+15</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>9443 Cynthia Court Apt. 038</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>...</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>Patent attorney</td>\n",
       "      <td>1967-01-12</td>\n",
       "      <td>6b849c168bdad6f867558c3793159a81</td>\n",
       "      <td>1.325376e+09</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:03:06</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.755342e+14</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>Garcia</td>\n",
       "      <td>M</td>\n",
       "      <td>408 Bradley Rest</td>\n",
       "      <td>Doe Hill</td>\n",
       "      <td>...</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>99.0</td>\n",
       "      <td>Dance movement psychotherapist</td>\n",
       "      <td>1986-03-28</td>\n",
       "      <td>a41d7549acf90789359a9aa5346dcb46</td>\n",
       "      <td>1.325376e+09</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Unnamed: 0  CreditCardNumber  \\\n",
       "TransactionTime                                     \n",
       "2019-01-01 00:00:18         0.0      2.703186e+15   \n",
       "2019-01-01 00:00:44         1.0      6.304233e+11   \n",
       "2019-01-01 00:00:51         2.0      3.885949e+13   \n",
       "2019-01-01 00:01:16         3.0      3.534094e+15   \n",
       "2019-01-01 00:03:06         4.0      3.755342e+14   \n",
       "\n",
       "                                               merchant       category  \\\n",
       "TransactionTime                                                          \n",
       "2019-01-01 00:00:18          fraud_Rippin, Kub and Mann       misc_net   \n",
       "2019-01-01 00:00:44     fraud_Heller, Gutmann and Zieme    grocery_pos   \n",
       "2019-01-01 00:00:51                fraud_Lind-Buckridge  entertainment   \n",
       "2019-01-01 00:01:16  fraud_Kutch, Hermiston and Farrell  gas_transport   \n",
       "2019-01-01 00:03:06                 fraud_Keeling-Crist       misc_pos   \n",
       "\n",
       "                     TransactionAmount      first     last gender  \\\n",
       "TransactionTime                                                     \n",
       "2019-01-01 00:00:18               4.97   Jennifer    Banks      F   \n",
       "2019-01-01 00:00:44             107.23  Stephanie     Gill      F   \n",
       "2019-01-01 00:00:51             220.11     Edward  Sanchez      M   \n",
       "2019-01-01 00:01:16              45.00     Jeremy    White      M   \n",
       "2019-01-01 00:03:06              41.96      Tyler   Garcia      M   \n",
       "\n",
       "                                           street            city  ...  \\\n",
       "TransactionTime                                                    ...   \n",
       "2019-01-01 00:00:18                561 Perry Cove  Moravian Falls  ...   \n",
       "2019-01-01 00:00:44  43039 Riley Greens Suite 393          Orient  ...   \n",
       "2019-01-01 00:00:51      594 White Dale Suite 530      Malad City  ...   \n",
       "2019-01-01 00:01:16   9443 Cynthia Court Apt. 038         Boulder  ...   \n",
       "2019-01-01 00:03:06              408 Bradley Rest        Doe Hill  ...   \n",
       "\n",
       "                         long  city_pop                                job  \\\n",
       "TransactionTime                                                              \n",
       "2019-01-01 00:00:18  -81.1781    3495.0          Psychologist, counselling   \n",
       "2019-01-01 00:00:44 -118.2105     149.0  Special educational needs teacher   \n",
       "2019-01-01 00:00:51 -112.2620    4154.0        Nature conservation officer   \n",
       "2019-01-01 00:01:16 -112.1138    1939.0                    Patent attorney   \n",
       "2019-01-01 00:03:06  -79.4629      99.0     Dance movement psychotherapist   \n",
       "\n",
       "                     DateOfBirth                         trans_num  \\\n",
       "TransactionTime                                                      \n",
       "2019-01-01 00:00:18   1988-03-09  0b242abb623afc578575680df30655b9   \n",
       "2019-01-01 00:00:44   1978-06-21  1f76529f8574734946361c461b024d99   \n",
       "2019-01-01 00:00:51   1962-01-19  a1a22d70485983eac12b5b88dad1cf95   \n",
       "2019-01-01 00:01:16   1967-01-12  6b849c168bdad6f867558c3793159a81   \n",
       "2019-01-01 00:03:06   1986-03-28  a41d7549acf90789359a9aa5346dcb46   \n",
       "\n",
       "                        unix_time  merch_lat  merch_long  is_fraud  \\\n",
       "TransactionTime                                                      \n",
       "2019-01-01 00:00:18  1.325376e+09  36.011293  -82.048315       0.0   \n",
       "2019-01-01 00:00:44  1.325376e+09  49.159047 -118.186462       0.0   \n",
       "2019-01-01 00:00:51  1.325376e+09  43.150704 -112.154481       0.0   \n",
       "2019-01-01 00:01:16  1.325376e+09  47.034331 -112.561071       0.0   \n",
       "2019-01-01 00:03:06  1.325376e+09  38.674999  -78.632459       0.0   \n",
       "\n",
       "                     TransactionID  \n",
       "TransactionTime                     \n",
       "2019-01-01 00:00:18              1  \n",
       "2019-01-01 00:00:44              2  \n",
       "2019-01-01 00:00:51              3  \n",
       "2019-01-01 00:01:16              4  \n",
       "2019-01-01 00:03:06              5  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10777762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Steps Completed File Loading, Describe, Date Conversions etc..   completed at Sun Nov  3 18:31:39 2024. Elapsed time: 0 minutes and 30.08 seconds\n",
      "\n",
      "--------------------------------------------------- ------------------   completed at Sun Nov  3 18:31:39 2024. Elapsed time: 0 minutes and 30.08 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Initial Steps Completed File Loading, Describe, Date Conversions etc..  \", start_time)\n",
    "log_time(\"--------------------------------------------------- ------------------  \", start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b32d0d",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca771ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log pre-process time at various steps\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2a6ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START - Feature Engineering .....   completed at Sun Nov  3 18:31:39 2024. Elapsed time: 0 minutes and 0.01 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"START - Feature Engineering .....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31252f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clip outliers if necessary\n",
    "df['TransactionAmount'] = df['TransactionAmount'].clip(upper=df['TransactionAmount'].quantile(0.99))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf9e3c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_37663/1075701793.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['TransactionAmount'].replace([np.inf, -np.inf], np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace inf values with NaN (in case they exist in the 'TransactionAmount' column)\n",
    "df['TransactionAmount'].replace([np.inf, -np.inf], np.nan, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b21308e",
   "metadata": {},
   "source": [
    "# next type of VIZ via transaction id vs transaction count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70594f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour from TransactionTime\n",
    "df['Hour'] = df.index.hour  # Since TransactionTime is already set as the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea729e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Risk Hours: [22, 23, 1, 0, 2, 3]\n",
      "                     Hour  HighRiskHour\n",
      "TransactionTime                        \n",
      "2019-01-01 00:00:18     0             1\n",
      "2019-01-01 00:00:44     0             1\n",
      "2019-01-01 00:00:51     0             1\n",
      "2019-01-01 00:01:16     0             1\n",
      "2019-01-01 00:03:06     0             1\n",
      "...                   ...           ...\n",
      "2020-06-21 12:12:08    12             0\n",
      "2020-06-21 12:12:19    12             0\n",
      "2020-06-21 12:12:32    12             0\n",
      "2020-06-21 12:13:36    12             0\n",
      "2020-06-21 12:13:37    12             0\n",
      "\n",
      "[1296675 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate fraud rate by hour\n",
    "fraud_rate_by_hour = df.groupby('Hour')['is_fraud'].mean()\n",
    "\n",
    "# Sort by fraud rate in descending order\n",
    "fraud_rate_by_hour = fraud_rate_by_hour.sort_values(ascending=False)\n",
    "\n",
    "# Define a threshold for high-risk hours (adjust as needed)\n",
    "threshold = fraud_rate_by_hour.mean()  # Mean fraud rate across all hours\n",
    "\n",
    "# Dynamically identify high-risk hours based on the threshold\n",
    "high_risk_hours = fraud_rate_by_hour[fraud_rate_by_hour > threshold].index.tolist()\n",
    "\n",
    "# Print high-risk hours for reference\n",
    "print(\"High-Risk Hours:\", high_risk_hours)\n",
    "\n",
    "# Create the HighRiskHour flag based on dynamically identified high-risk hours\n",
    "df['HighRiskHour'] = df['Hour'].apply(lambda x: 1 if x in high_risk_hours else 0)\n",
    "\n",
    "# Print a sample of the DataFrame to verify the new column\n",
    "print(df[['Hour', 'HighRiskHour']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75736fa",
   "metadata": {},
   "source": [
    "1. Time-Based Analysis:\n",
    "Already explored daily and hourly trends in transaction volumes, but now dive deeper into fraud patterns based on time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a61c0eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Weekday vs. Weekend: Is fraud more common on weekdays or weekends?\n",
    "df['DayOfWeek'] = df.index.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "fraud_by_day = df[df['is_fraud'] == 1]['DayOfWeek'].value_counts().sort_index()\n",
    "non_fraud_by_day = df[df['is_fraud'] == 0]['DayOfWeek'].value_counts().sort_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0d77fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the correct day order\n",
    "day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "\n",
    "\n",
    "df['DayName'] = df.index.day_name()\n",
    "# Convert the 'DayName' column to a categorical type with the correct order\n",
    "df['DayName'] = pd.Categorical(df['DayName'], categories=day_order, ordered=True)\n",
    "\n",
    "fraud_by_day = df[df['is_fraud'] == 1]['DayName'].value_counts().sort_index()\n",
    "non_fraud_by_day = df[df['is_fraud'] == 0]['DayName'].value_counts().sort_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3edc7f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of fraud on weekends: 32.55%\n",
      "Percentage of non-fraud on weekends: 34.84%\n"
     ]
    }
   ],
   "source": [
    "df['IsWeekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "weekend_fraud = df[df['is_fraud'] == 1]['IsWeekend'].mean()\n",
    "weekend_non_fraud = df[df['is_fraud'] == 0]['IsWeekend'].mean()\n",
    "\n",
    "print(f\"Percentage of fraud on weekends: {weekend_fraud * 100:.2f}%\")\n",
    "print(f\"Percentage of non-fraud on weekends: {weekend_non_fraud * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67a62065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part1 - TrxAmount, Hour, DayOfWeeek etc.. completed at Sun Nov  3 18:31:42 2024. Elapsed time: 0 minutes and 2.82 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part1 - TrxAmount, Hour, DayOfWeeek etc..\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b51d0f2e",
   "metadata": {},
   "source": [
    "# Calculate distance between cardholder and merchant\n",
    "df['distance'] = df.apply(lambda row: geodesic((row['lat'], row['long']), (row['merch_lat'], row['merch_long'])).km, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f04c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'v_2.0_RandomForest_Credit_Card_Fraud_Detection.ipynb', 'v_1.2_DecisionTrees_OptFeat_Parallelized_GridSearch_Credit_Card_Fraud_Detection.ipynb', 'v_2.1_RandomForest_Balanced_SMOTE_Credit_Card_Fraud_Detection.ipynb', 'v_2.2_RandomForest_Balanced_SMOTE_GridSearch_Credit_Card_Fraud_Detection.ipynb', 'bkp', 'v_0.0_LogisticRegression_Credit_Card_Fraud_Detection.ipynb', 'v_3.2_xgBoost_Credit_Card_Fraud_Detection.ipynb', '.gitkeep', 'v_3.1_xgBoost_SMOTE_Credit_Card_Fraud_Detection.ipynb', '__pycache__', 'v_0.1_LogisticRegression_Balanced_Credit_Card_Fraud_Detection.ipynb', 'v_1.1_DecisionTrees_Parallelized_GridSearch_Credit_Card_Fraud_Detection.ipynb', '.ipynb_checkpoints', 'v_1.0_DecisionTrees_Credit_Card_Fraud_Detection.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())  # List all files in the current directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a9514",
   "metadata": {},
   "source": [
    "# MULTIPROCESSING : distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40c0bbf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part2 - Distance Calculation with Multiprocessing (4 cores) completed at Sun Nov  3 18:33:55 2024. Elapsed time: 2 minutes and 13.05 seconds\n",
      "\n",
      "                         lat      long  merch_lat  merch_long    distance\n",
      "TransactionTime                                                          \n",
      "2019-01-01 00:00:18  36.0788  -81.1781  36.011293  -82.048315   78.773821\n",
      "2019-01-01 00:00:44  48.8878 -118.2105  49.159047 -118.186462   30.216618\n",
      "2019-01-01 00:00:51  42.1808 -112.2620  43.150704 -112.154481  108.102912\n",
      "2019-01-01 00:01:16  46.2306 -112.1138  47.034331 -112.561071   95.685115\n",
      "2019-01-01 00:03:06  38.4207  -79.4629  38.674999  -78.632459   77.702395\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from distance_calculation import calculate_distance_chunk\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Add the current working directory to the system path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Multiprocessing function to split the dataframe and apply the distance calculation\n",
    "def parallel_distance_calculation(df, num_partitions=None):\n",
    "    if num_partitions is None:\n",
    "        num_partitions = mp.cpu_count()  # Use all available CPU cores\n",
    "    \n",
    "    # Split the dataframe into chunks\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    \n",
    "    # Create a multiprocessing Pool\n",
    "    with mp.Pool(num_partitions) as pool:\n",
    "        # Apply the calculate_distance_chunk function to each chunk in parallel\n",
    "        result = pool.map(calculate_distance_chunk, df_split)\n",
    "    \n",
    "    # Concatenate the results back into a single dataframe\n",
    "    return pd.concat(result)\n",
    "\n",
    "# Main block to ensure multiprocessing works correctly\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Assuming df has the columns ['lat', 'long', 'merch_lat', 'merch_long']\n",
    "    \n",
    "    # Run with limited number of cores (e.g., 4 cores)\n",
    "    df = parallel_distance_calculation(df, num_partitions=4)  # Use 4 cores instead of all available cores\n",
    "\n",
    "    # Log the time taken for distance calculation with multiprocessing\n",
    "    log_time(\"Part2 - Distance Calculation with Multiprocessing (4 cores)\", start_time)\n",
    "\n",
    "    # Check the first few rows to verify the result\n",
    "    print(df[['lat', 'long', 'merch_lat', 'merch_long', 'distance']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c72a4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/Desktop/coding/ZHAW_Project/ML_BigData_Repo_1/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())  # This will print the current working directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f985dcd",
   "metadata": {},
   "source": [
    "log_time(\"Part2 -  Distance Calculation\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50a155e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check unique values in the 'is_fraud' column\n",
    "df['is_fraud'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09d464f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud vs Non-Fraud by Merchant Category\n",
    "fraud_by_category = df[df['is_fraud'] == 1]['category'].value_counts().head(10)\n",
    "non_fraud_by_category = df[df['is_fraud'] == 0]['category'].value_counts().head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07ec224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Fraudulent Merchant Categories: ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport']\n"
     ]
    }
   ],
   "source": [
    "# Top 5 categories with the highest fraud counts\n",
    "top_fraud_merchant_categories = df[df['is_fraud'] == 1]['category'].value_counts().head(5).index.tolist()\n",
    "\n",
    "# Print top fraudulent categories\n",
    "print(\"Top Fraudulent Merchant Categories:\", top_fraud_merchant_categories)\n",
    "\n",
    "# Create HighRiskMerchantCategory flag\n",
    "df['HighRiskMerchantCategory'] = df['category'].apply(lambda x: 1 if x in top_fraud_merchant_categories else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7780b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HighRiskMerchantCategory\n",
      "0    763876\n",
      "1    532799\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the count of 1s and 0s in HighRiskMerchantCategory\n",
    "print(df['HighRiskMerchantCategory'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df499da3",
   "metadata": {},
   "source": [
    "# Potential Additional Features:\n",
    "Transaction Frequency:\n",
    "    Feature: How often a credit card has been used within a specific time frame (e.g., last hour or day).\n",
    "    Why: Fraudsters often make rapid successive transactions within short periods. You could create a rolling window to calculate transaction frequency.\n",
    "    How: You could calculate the number of transactions within the past X hours/days using a rolling window on the TransactionTime feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828d3a0e",
   "metadata": {},
   "source": [
    "#age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "221b8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure 'DateOfBirth' is in datetime format\n",
    "df['DateOfBirth'] = pd.to_datetime(df['DateOfBirth'], errors='coerce')  # Handle errors during conversion\n",
    "\n",
    "# Step 1: Calculate Age\n",
    "# Calculate age in years\n",
    "df['Age'] = (pd.Timestamp.now() - df['DateOfBirth']).dt.days // 365  # Age in years\n",
    "\n",
    "# Step 2: Create Age Groups\n",
    "# Define age bins and labels\n",
    "bins = [0, 18, 25, 35, 45, 55, 65, 100]  # Define your age bins, ensuring to cover all possible ages\n",
    "labels = ['0-18', '19-25', '26-35', '36-45', '46-55', '56-65', '66+']  # Corresponding labels\n",
    "\n",
    "# Create age group feature, include NaN values handling\n",
    "df['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "\n",
    "# Verify the new features without truncating DataFrame\n",
    "#print(df[['DateOfBirth', 'Age', 'AgeGroup']].head(10))  # Display the first 10 entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "296a6aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part3 - Merchant Categories & Age group completed at Sun Nov  3 18:33:57 2024. Elapsed time: 2 minutes and 14.57 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part3 - Merchant Categories & Age group\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19dc4c6f",
   "metadata": {},
   "source": [
    "RapidTransactionFlag"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e07efaf",
   "metadata": {},
   "source": [
    "def count_transactions_within_last_hour(group):\n",
    "    # Create an empty list to hold the frequencies\n",
    "    frequencies = []\n",
    "    \n",
    "    # Loop over each transaction time in the group\n",
    "    for time in group.index:\n",
    "        # Count the number of transactions within the last hour\n",
    "        count = group[(group.index >= (time - pd.Timedelta(hours=1))) & (group.index <= time)].shape[0]\n",
    "        frequencies.append(count)\n",
    "    \n",
    "    return frequencies\n",
    "\n",
    "# Apply the function to each group\n",
    "df['TransactionFrequency'] = df.groupby('CreditCardNumber').apply(count_transactions_within_last_hour).reset_index(drop=True)\n",
    "print(df[['TransactionFrequency']].head(10))  # Display the first 10 entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc72de",
   "metadata": {},
   "source": [
    "# MULTIPROCESSING : count_transactions_within_last_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "620555a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part4 - TransactionFrequency Multiprocessing completed at Sun Nov  3 18:38:38 2024. Elapsed time: 4 minutes and 41.06 seconds\n",
      "\n",
      "                    TransactionFrequency\n",
      "TransactionTime                         \n",
      "2019-01-01 00:00:18                  NaN\n",
      "2019-01-01 00:00:44                  NaN\n",
      "2019-01-01 00:00:51                  NaN\n",
      "2019-01-01 00:01:16                  NaN\n",
      "2019-01-01 00:03:06                  NaN\n",
      "2019-01-01 00:04:08                  NaN\n",
      "2019-01-01 00:04:42                  NaN\n",
      "2019-01-01 00:05:08                  NaN\n",
      "2019-01-01 00:05:18                  NaN\n",
      "2019-01-01 00:06:01                  NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import time\n",
    "from transaction_frequency import process_chunk  # Import from the .py file\n",
    "\n",
    "# Multiprocessing function to parallelize the transaction counting\n",
    "def parallel_count_transactions(df, num_partitions=None):\n",
    "    if num_partitions is None:\n",
    "        num_partitions = mp.cpu_count()  # Use all available CPU cores\n",
    "    \n",
    "    # Ensure the index is a datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Split the dataframe into chunks based on the number of partitions (CPU cores)\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    \n",
    "    # Create a multiprocessing Pool\n",
    "    with mp.Pool(num_partitions) as pool:\n",
    "        # Apply the processing function to each chunk in parallel\n",
    "        result = pool.map(process_chunk, df_split)\n",
    "    \n",
    "    # Combine the results from each chunk into a single series, reset index for consistency\n",
    "    return pd.concat(result).reset_index(drop=True)\n",
    "\n",
    "# Assuming df has 'CreditCardNumber' as a column and transaction times are indexed\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Apply the parallel processing for transaction frequency counting\n",
    "    df['TransactionFrequency'] = parallel_count_transactions(df, num_partitions=4)  # Adjust num_partitions as needed\n",
    "\n",
    "    # Log the time taken for transaction frequency calculation with multiprocessing\n",
    "    log_time(\"Part4 - TransactionFrequency Multiprocessing\", start_time)\n",
    "\n",
    "    # Check the first 10 rows\n",
    "    print(df[['TransactionFrequency']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3e571f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-01 00:00:18', '2019-01-01 00:00:44',\n",
      "               '2019-01-01 00:00:51', '2019-01-01 00:01:16',\n",
      "               '2019-01-01 00:03:06', '2019-01-01 00:04:08',\n",
      "               '2019-01-01 00:04:42', '2019-01-01 00:05:08',\n",
      "               '2019-01-01 00:05:18', '2019-01-01 00:06:01',\n",
      "               ...\n",
      "               '2020-06-21 12:08:42', '2020-06-21 12:09:22',\n",
      "               '2020-06-21 12:10:56', '2020-06-21 12:11:23',\n",
      "               '2020-06-21 12:11:36', '2020-06-21 12:12:08',\n",
      "               '2020-06-21 12:12:19', '2020-06-21 12:12:32',\n",
      "               '2020-06-21 12:13:36', '2020-06-21 12:13:37'],\n",
      "              dtype='datetime64[ns]', name='TransactionTime', length=1296675, freq=None)\n"
     ]
    }
   ],
   "source": [
    "df.index = pd.to_datetime(df.index)\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96249ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dadb160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_37663/1490275857.py:2: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  transaction_counts_hourly = df.resample('H').size()\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_37663/1490275857.py:6: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  transaction_counts = df.groupby('CreditCardNumber').resample('H').size().reset_index(name='TransactionCount')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CreditCardNumber     TransactionTime  TransactionCount\n",
      "0      6.041621e+10 2019-01-01 12:00:00                 1\n",
      "1      6.041621e+10 2019-01-01 13:00:00                 0\n",
      "2      6.041621e+10 2019-01-01 14:00:00                 0\n",
      "3      6.041621e+10 2019-01-01 15:00:00                 0\n",
      "4      6.041621e+10 2019-01-01 16:00:00                 0\n",
      "5      6.041621e+10 2019-01-01 17:00:00                 0\n",
      "6      6.041621e+10 2019-01-01 18:00:00                 0\n",
      "7      6.041621e+10 2019-01-01 19:00:00                 0\n",
      "8      6.041621e+10 2019-01-01 20:00:00                 0\n",
      "9      6.041621e+10 2019-01-01 21:00:00                 0\n"
     ]
    }
   ],
   "source": [
    "# Resample the data to count transactions every hour\n",
    "transaction_counts_hourly = df.resample('H').size()\n",
    "transaction_counts_daily = df.resample('D').size()\n",
    "\n",
    "# Combine with CreditCardNumber if necessary\n",
    "transaction_counts = df.groupby('CreditCardNumber').resample('H').size().reset_index(name='TransactionCount')\n",
    "print(transaction_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97c19713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CreditCardNumber  TotalTransactionCount\n",
      "0      6.041621e+10                   1518\n",
      "1      6.042293e+10                   1531\n",
      "2      6.042310e+10                    510\n",
      "3      6.042785e+10                    528\n",
      "4      6.048700e+10                    496\n",
      "5      6.049060e+10                   1010\n",
      "6      6.049559e+10                    518\n",
      "7      5.018030e+11                   1559\n",
      "8      5.018181e+11                      8\n",
      "9      5.018282e+11                    515\n"
     ]
    }
   ],
   "source": [
    "total_transactions = df.groupby('CreditCardNumber').size().reset_index(name='TotalTransactionCount')\n",
    "print(total_transactions.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aef71d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01    1960\n",
      "2019-01-02     606\n",
      "2019-01-03     701\n",
      "2019-01-04     952\n",
      "2019-01-05     879\n",
      "              ... \n",
      "2020-06-17    1327\n",
      "2020-06-18    1547\n",
      "2020-06-19    1966\n",
      "2020-06-20    1910\n",
      "2020-06-21    1142\n",
      "Length: 537, dtype: int64\n",
      "         Unnamed: 0  CreditCardNumber  TransactionAmount           zip  \\\n",
      "count  1.061083e+06      1.061083e+06       1.061083e+06  1.061083e+06   \n",
      "mean   6.541777e+05      4.168739e+17       6.463842e+01  4.881377e+04   \n",
      "min    1.000000e+00      6.041621e+10       1.000000e+00  1.257000e+03   \n",
      "25%    3.399115e+05      1.800429e+14       9.500000e+00  2.623700e+04   \n",
      "50%    6.543810e+05      3.521417e+15       4.635000e+01  4.817400e+04   \n",
      "75%    9.639775e+05      4.642255e+15       8.223000e+01  7.204200e+04   \n",
      "max    1.296674e+06      4.992346e+18       5.459926e+02  9.978300e+04   \n",
      "std    3.688933e+05      1.308438e+18       8.254229e+01  2.689496e+04   \n",
      "\n",
      "                lat          long      city_pop  \\\n",
      "count  1.061083e+06  1.061083e+06  1.061083e+06   \n",
      "mean   3.853009e+01 -9.022821e+01  8.979136e+04   \n",
      "min    2.002710e+01 -1.656723e+02  2.300000e+01   \n",
      "25%    3.459060e+01 -9.679800e+01  7.430000e+02   \n",
      "50%    3.935430e+01 -8.747690e+01  2.470000e+03   \n",
      "75%    4.194040e+01 -8.015800e+01  2.047800e+04   \n",
      "max    6.669330e+01 -6.795030e+01  2.906700e+06   \n",
      "std    5.078796e+00  1.374857e+01  3.041541e+05   \n",
      "\n",
      "                         DateOfBirth     unix_time     merch_lat  \\\n",
      "count                        1061083  1.061083e+06  1.061083e+06   \n",
      "mean   1974-01-07 01:53:43.193284640  1.349452e+09  3.853026e+01   \n",
      "min              1924-10-30 00:00:00  1.325376e+09  1.902779e+01   \n",
      "25%              1962-12-06 00:00:00  1.339269e+09  3.471879e+01   \n",
      "50%              1975-12-28 00:00:00  1.349484e+09  3.935917e+01   \n",
      "75%              1987-05-05 00:00:00  1.359010e+09  4.195454e+01   \n",
      "max              2005-01-29 00:00:00  1.371817e+09  6.751027e+01   \n",
      "std                              NaN  1.264150e+07  5.112969e+00   \n",
      "\n",
      "         merch_long      is_fraud  TransactionID          Hour  HighRiskHour  \\\n",
      "count  1.061083e+06  1.061083e+06   1.061083e+06  1.061083e+06  1.061083e+06   \n",
      "mean  -9.022814e+01  5.652715e-03   6.541787e+05  1.323944e+01  2.301988e-01   \n",
      "min   -1.666701e+02  0.000000e+00   2.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%   -9.689874e+01  0.000000e+00   3.399125e+05  8.000000e+00  0.000000e+00   \n",
      "50%   -8.744732e+01  0.000000e+00   6.543820e+05  1.400000e+01  0.000000e+00   \n",
      "75%   -8.024346e+01  0.000000e+00   9.639785e+05  1.900000e+01  0.000000e+00   \n",
      "max   -6.695090e+01  1.000000e+00   1.296675e+06  2.300000e+01  1.000000e+00   \n",
      "std    1.376070e+01  7.497178e-02   3.688933e+05  6.727723e+00  4.209602e-01   \n",
      "\n",
      "          DayOfWeek     IsWeekend      distance  HighRiskMerchantCategory  \\\n",
      "count  1.061083e+06  1.061083e+06  1.061083e+06              1.061083e+06   \n",
      "mean   3.092014e+00  3.698118e-01  7.611910e+01              3.897716e-01   \n",
      "min    0.000000e+00  0.000000e+00  2.227351e-02              0.000000e+00   \n",
      "25%    1.000000e+00  0.000000e+00  5.535721e+01              0.000000e+00   \n",
      "50%    3.000000e+00  0.000000e+00  7.827691e+01              0.000000e+00   \n",
      "75%    5.000000e+00  1.000000e+00  9.849232e+01              1.000000e+00   \n",
      "max    6.000000e+00  1.000000e+00  1.518682e+02              1.000000e+00   \n",
      "std    2.263057e+00  4.827538e-01  2.909755e+01              4.876986e-01   \n",
      "\n",
      "                Age  \n",
      "count  1.061083e+06  \n",
      "mean   5.036491e+01  \n",
      "min    1.900000e+01  \n",
      "25%    3.700000e+01  \n",
      "50%    4.800000e+01  \n",
      "75%    6.100000e+01  \n",
      "max    1.000000e+02  \n",
      "std    1.738604e+01  \n"
     ]
    }
   ],
   "source": [
    "# Calculate the time difference between consecutive transactions\n",
    "time_diff = df.index.to_series().diff().dt.total_seconds()\n",
    "# Flag rapid transactions (within 5 minutes)\n",
    "df['RapidTransactionFlag'] = time_diff < 60  # For a 1-minute threshold\n",
    "\n",
    "# Create a temporary DataFrame for rapid transactions\n",
    "rapid_transactions = df[df['RapidTransactionFlag']]\n",
    "\n",
    "# Group by date and count the number of rapid transactions\n",
    "rapid_transaction_counts = rapid_transactions.groupby(rapid_transactions.index.date).size()\n",
    "print(rapid_transaction_counts)\n",
    "\n",
    "# Get a summary of the rapid transactions\n",
    "rapid_transactions_summary = rapid_transactions.describe()\n",
    "print(rapid_transactions_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64e05375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'CreditCardNumber', 'merchant', 'category',\n",
      "       'TransactionAmount', 'first', 'last', 'gender', 'street', 'city',\n",
      "       'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID', 'Hour', 'HighRiskHour', 'DayOfWeek', 'DayName',\n",
      "       'IsWeekend', 'distance', 'HighRiskMerchantCategory', 'Age', 'AgeGroup',\n",
      "       'TransactionFrequency', 'RapidTransactionFlag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bca5e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part5 - RapidTransactionFlag completed at Sun Nov  3 18:38:51 2024. Elapsed time: 4 minutes and 54.45 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part5 - RapidTransactionFlag\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b200c",
   "metadata": {},
   "source": [
    "Transaction Amount Features:\n",
    "Log Transaction Amount: Normalize the TransactionAmount by taking its logarithm to reduce skewness.\n",
    "Transaction Amount Flags: Create binary flags for high-value transactions (e.g., if TransactionAmount exceeds a certain threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da75a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     TransactionAmount  LogTransactionAmount  \\\n",
      "TransactionTime                                                \n",
      "2019-01-01 00:00:18               4.97              1.786747   \n",
      "2019-01-01 00:00:44             107.23              4.684259   \n",
      "2019-01-01 00:00:51             220.11              5.398660   \n",
      "2019-01-01 00:01:16              45.00              3.828641   \n",
      "2019-01-01 00:03:06              41.96              3.760269   \n",
      "2019-01-01 00:04:08              94.63              4.560487   \n",
      "2019-01-01 00:04:42              44.54              3.818591   \n",
      "2019-01-01 00:05:08              71.65              4.285653   \n",
      "2019-01-01 00:05:18               4.27              1.662030   \n",
      "2019-01-01 00:06:01             198.39              5.295263   \n",
      "\n",
      "                     HighValueTransactionFlag  \n",
      "TransactionTime                                \n",
      "2019-01-01 00:00:18                     False  \n",
      "2019-01-01 00:00:44                      True  \n",
      "2019-01-01 00:00:51                      True  \n",
      "2019-01-01 00:01:16                     False  \n",
      "2019-01-01 00:03:06                     False  \n",
      "2019-01-01 00:04:08                     False  \n",
      "2019-01-01 00:04:42                     False  \n",
      "2019-01-01 00:05:08                     False  \n",
      "2019-01-01 00:05:18                     False  \n",
      "2019-01-01 00:06:01                      True  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample DataFrame creation\n",
    "# Assume 'df' is your DataFrame and has a 'TransactionAmount' column\n",
    "# df = pd.read_csv('your_data.csv')  # Load your actual data\n",
    "\n",
    "# Step 1: Log Transaction Amount\n",
    "# Calculate the log of TransactionAmount\n",
    "df['LogTransactionAmount'] = np.log1p(df['TransactionAmount'])  # Use log1p for stability with 0 values\n",
    "\n",
    "# Step 2: Create Transaction Amount Flags\n",
    "# Define a threshold for high-value transactions\n",
    "threshold = 100  # Adjust the threshold based on your data context\n",
    "\n",
    "# Create a flag for high-value transactions\n",
    "df['HighValueTransactionFlag'] = df['TransactionAmount'] > threshold\n",
    "\n",
    "# Verify the new features\n",
    "print(df[['TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag']].head(10))  # Display the first 10 entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217bc43",
   "metadata": {},
   "source": [
    "Behavioral Features:\n",
    "Count of Transactions in Last X Days: Count how many transactions have occurred in the last 7, 14, or 30 days.\n",
    "Average Transaction Amount in Last X Days: Calculate the average transaction amount over the same periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25aeefda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     TransactionCountLast7Days  TransactionCountLast14Days  \\\n",
      "TransactionTime                                                              \n",
      "2019-01-01 12:47:15                        1.0                         1.0   \n",
      "2019-01-02 08:44:57                        2.0                         2.0   \n",
      "2019-01-02 08:47:36                        3.0                         3.0   \n",
      "2019-01-02 12:38:14                        4.0                         4.0   \n",
      "2019-01-02 13:10:46                        5.0                         5.0   \n",
      "2019-01-03 13:56:35                        6.0                         6.0   \n",
      "2019-01-03 17:05:10                        7.0                         7.0   \n",
      "2019-01-04 13:59:55                        8.0                         8.0   \n",
      "2019-01-04 21:17:22                        9.0                         9.0   \n",
      "2019-01-05 00:42:24                       10.0                        10.0   \n",
      "\n",
      "                     TransactionCountLast30Days  \\\n",
      "TransactionTime                                   \n",
      "2019-01-01 12:47:15                         1.0   \n",
      "2019-01-02 08:44:57                         2.0   \n",
      "2019-01-02 08:47:36                         3.0   \n",
      "2019-01-02 12:38:14                         4.0   \n",
      "2019-01-02 13:10:46                         5.0   \n",
      "2019-01-03 13:56:35                         6.0   \n",
      "2019-01-03 17:05:10                         7.0   \n",
      "2019-01-04 13:59:55                         8.0   \n",
      "2019-01-04 21:17:22                         9.0   \n",
      "2019-01-05 00:42:24                        10.0   \n",
      "\n",
      "                     AverageTransactionAmountLast7Days  \\\n",
      "TransactionTime                                          \n",
      "2019-01-01 12:47:15                           7.270000   \n",
      "2019-01-02 08:44:57                          30.105000   \n",
      "2019-01-02 08:47:36                          47.430000   \n",
      "2019-01-02 12:38:14                          44.270000   \n",
      "2019-01-02 13:10:46                          40.852000   \n",
      "2019-01-03 13:56:35                          35.188333   \n",
      "2019-01-03 17:05:10                          31.365714   \n",
      "2019-01-04 13:59:55                          42.083750   \n",
      "2019-01-04 21:17:22                          40.378889   \n",
      "2019-01-05 00:42:24                          46.861000   \n",
      "\n",
      "                     AverageTransactionAmountLast14Days  \\\n",
      "TransactionTime                                           \n",
      "2019-01-01 12:47:15                            7.270000   \n",
      "2019-01-02 08:44:57                           30.105000   \n",
      "2019-01-02 08:47:36                           47.430000   \n",
      "2019-01-02 12:38:14                           44.270000   \n",
      "2019-01-02 13:10:46                           40.852000   \n",
      "2019-01-03 13:56:35                           35.188333   \n",
      "2019-01-03 17:05:10                           31.365714   \n",
      "2019-01-04 13:59:55                           42.083750   \n",
      "2019-01-04 21:17:22                           40.378889   \n",
      "2019-01-05 00:42:24                           46.861000   \n",
      "\n",
      "                     AverageTransactionAmountLast30Days  \n",
      "TransactionTime                                          \n",
      "2019-01-01 12:47:15                            7.270000  \n",
      "2019-01-02 08:44:57                           30.105000  \n",
      "2019-01-02 08:47:36                           47.430000  \n",
      "2019-01-02 12:38:14                           44.270000  \n",
      "2019-01-02 13:10:46                           40.852000  \n",
      "2019-01-03 13:56:35                           35.188333  \n",
      "2019-01-03 17:05:10                           31.365714  \n",
      "2019-01-04 13:59:55                           42.083750  \n",
      "2019-01-04 21:17:22                           40.378889  \n",
      "2019-01-05 00:42:24                           46.861000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'TransactionTime' is already set as the index and in datetime format\n",
    "\n",
    "# Step 1: Count of Transactions in Last X Days\n",
    "for days in [7, 14, 30]:\n",
    "    # Sort data by CreditCardNumber and TransactionTime to ensure rolling works properly\n",
    "    df = df.sort_values(by=['CreditCardNumber', 'TransactionTime'])\n",
    "    \n",
    "    # Apply rolling and count the number of transactions for each card\n",
    "    df[f'TransactionCountLast{days}Days'] = (\n",
    "        df.groupby('CreditCardNumber')['CreditCardNumber']\n",
    "        .rolling(f'{days}D')\n",
    "        .count()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Step 2: Average Transaction Amount in Last X Days\n",
    "for days in [7, 14, 30]:\n",
    "    # Sort data by CreditCardNumber and TransactionTime to ensure rolling works properly\n",
    "    df = df.sort_values(by=['CreditCardNumber', 'TransactionTime'])\n",
    "    \n",
    "    # Calculate the average transaction amount for each credit card in the last X days\n",
    "    df[f'AverageTransactionAmountLast{days}Days'] = (\n",
    "        df.groupby('CreditCardNumber')['TransactionAmount']\n",
    "        .rolling(f'{days}D')\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Verify the new features\n",
    "print(df[['TransactionCountLast7Days', 'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
    "           'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast14Days', 'AverageTransactionAmountLast30Days']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebcd77cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'CreditCardNumber', 'merchant', 'category',\n",
      "       'TransactionAmount', 'first', 'last', 'gender', 'street', 'city',\n",
      "       'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID', 'Hour', 'HighRiskHour', 'DayOfWeek', 'DayName',\n",
      "       'IsWeekend', 'distance', 'HighRiskMerchantCategory', 'Age', 'AgeGroup',\n",
      "       'TransactionFrequency', 'RapidTransactionFlag', 'LogTransactionAmount',\n",
      "       'HighValueTransactionFlag', 'TransactionCountLast7Days',\n",
      "       'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
      "       'AverageTransactionAmountLast7Days',\n",
      "       'AverageTransactionAmountLast14Days',\n",
      "       'AverageTransactionAmountLast30Days'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)  # Display all columns in the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc639c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part6 - TransactionCountLast_X_Days & AverageTrxAmountLast_X_Days completed at Sun Nov  3 18:39:06 2024. Elapsed time: 0 minutes and 14.58 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part6 - TransactionCountLast_X_Days & AverageTrxAmountLast_X_Days\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a24a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b4aaef3",
   "metadata": {},
   "source": [
    "# Graph Construction with NetworkX:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18793de2",
   "metadata": {},
   "source": [
    "Highlight Fraudulent Nodes: Overlay of fraudulent and non-fraudulent credit cards on this degree distribution to see if there’s a difference in their degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e26bd1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique credit card nodes: 983\n",
      "Number of unique merchant nodes: 693\n",
      "Number of credit card nodes with degrees: 983\n",
      "Number of merchant nodes with degrees: 693\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges between credit cards and merchants, including transaction amount as an edge attribute\n",
    "for idx, row in df.iterrows():\n",
    "    credit_card = str(row['CreditCardNumber'])\n",
    "    merchant = str(row['merchant'])\n",
    "    transaction_amount = row['TransactionAmount']  # Ensure TransactionAmount exists in your dataframe\n",
    "    \n",
    "    # Add an edge with the transaction amount as an attribute\n",
    "    G.add_edge(credit_card, merchant, transaction_amount=transaction_amount)\n",
    "\n",
    "\n",
    "# Calculate degrees for all nodes in the graph\n",
    "degrees = dict(G.degree())\n",
    "\n",
    "# Filter degrees for credit cards and merchants\n",
    "credit_card_nodes = df['CreditCardNumber'].astype(str).unique()\n",
    "merchant_nodes = df['merchant'].astype(str).unique()\n",
    "\n",
    "credit_card_degrees = {node: degrees[node] for node in credit_card_nodes if node in degrees}\n",
    "merchant_degrees = {node: degrees[node] for node in merchant_nodes if node in degrees}\n",
    "\n",
    "# Debugging: Print counts to ensure correctness\n",
    "print(f\"Number of unique credit card nodes: {len(credit_card_nodes)}\")\n",
    "print(f\"Number of unique merchant nodes: {len(merchant_nodes)}\")\n",
    "print(f\"Number of credit card nodes with degrees: {len(credit_card_degrees)}\")\n",
    "print(f\"Number of merchant nodes with degrees: {len(merchant_degrees)}\")\n",
    "\n",
    "# Create a new DataFrame for easier plotting\n",
    "degree_df = pd.DataFrame({\n",
    "    'CreditCardDegree': pd.Series(credit_card_degrees),\n",
    "    'MerchantDegree': pd.Series(merchant_degrees)\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a55584fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add degree information back to the original DataFrame\n",
    "df['degree'] = df['CreditCardNumber'].astype(str).map(credit_card_degrees)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a8f6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check edges and their attributes\n",
    "#for edge in G.edges(data=True):\n",
    "#    print(edge)\n",
    "\n",
    "#do NOT print this, huge list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a9c525e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CreditCardNumber'] = df['CreditCardNumber'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5115524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_mapping = df.set_index('CreditCardNumber')['is_fraud'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9e261bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part7 - NetworkX Start Step completed at Sun Nov  3 18:41:16 2024. Elapsed time: 2 minutes and 10.10 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part7 - NetworkX Start Step\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "167571cb",
   "metadata": {},
   "source": [
    "betweenness_centrality"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0093ce9f",
   "metadata": {},
   "source": [
    "# Calculate betweenness centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac75df",
   "metadata": {},
   "source": [
    "# MULTIPROCESSING : betweenness_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50d69fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part8 - Betweenness Centrality Calculation with Multiprocessing completed at Sun Nov  3 18:46:37 2024. Elapsed time: 5 minutes and 21.35 seconds\n",
      "\n",
      "[('60416207185.0', 3.903743365141523e-05), ('fraud_Jones, Sawayn and Romaguera', 0.00016718121427278148), ('fraud_Berge LLC', 0.0003731639453736409), ('fraud_Luettgen PLC', 0.0002880918347168189), ('fraud_Daugherty LLC', 0.00026296051041424566), ('fraud_Beier and Sons', 0.00025675424900787366), ('fraud_Stamm-Witting', 0.00021600459190031658), ('fraud_Conroy-Emard', 0.00021409812196013464), ('fraud_Pollich LLC', 0.00029566515903354184), ('fraud_Monahan-Morar', 0.0002101294467358623)]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import time\n",
    "from networkx_graph_betweeness_centrality import parallel_betweenness_centrality\n",
    "\n",
    "# Assuming G is your graph\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate betweenness centrality using parallel processing\n",
    "    betweenness_centrality = parallel_betweenness_centrality(G, num_partitions=4)  # Adjust number of cores if needed\n",
    "\n",
    "    # Log the time taken for betweenness centrality calculation with multiprocessing\n",
    "    log_time(\"Part8 - Betweenness Centrality Calculation with Multiprocessing\", start_time)\n",
    "\n",
    "    # Check a few centrality values\n",
    "    print(list(betweenness_centrality.items())[:10])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e7b4462",
   "metadata": {},
   "source": [
    "import os\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85bd6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['betweenness_centrality'] = df['CreditCardNumber'].map(betweenness_centrality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb8bf772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1.296675e+06\n",
      "mean     4.163569e-05\n",
      "std      1.178525e-05\n",
      "min      2.728628e-09\n",
      "25%      3.630433e-05\n",
      "50%      4.399435e-05\n",
      "75%      5.147284e-05\n",
      "max      5.736748e-05\n",
      "Name: betweenness_centrality, dtype: float64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df['betweenness_centrality'].describe())\n",
    "print(df['betweenness_centrality'].isna().sum())  # Check for missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e5e4b947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60416207185: None\n",
      "fraud_Kutch-Ferry: 0.00026288184139774875\n"
     ]
    }
   ],
   "source": [
    "# Check betweenness centrality for specific credit card numbers\n",
    "sample_nodes = ['60416207185', 'fraud_Kutch-Ferry']  # Replace with actual nodes\n",
    "for node in sample_nodes:\n",
    "    print(f\"{node}: {betweenness_centrality.get(node)}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a33a0b35",
   "metadata": {},
   "source": [
    "log_time(\"Part8 - Feature Engineering -- NetworkX betweenness_centrality\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439124c6",
   "metadata": {},
   "source": [
    "1. Investigate Nodes with High Betweenness Centrality:\n",
    "\n",
    "Now that you’ve visualized nodes with high betweenness centrality, you can:\n",
    "\n",
    "    Examine if fraudulent nodes tend to have high betweenness centrality. This might indicate that these nodes are acting as \"connectors\" between different parts of the network, which could be a sign of suspicious behavior.\n",
    "    Compare centrality between fraud and non-fraud nodes to see if there's a pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13fdd8e",
   "metadata": {},
   "source": [
    "2. Visualize Communities in the Network:\n",
    "\n",
    "You could apply community detection to uncover fraud rings or clusters of merchants targeted by fraudsters. The Louvain algorithm is great for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5f148a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community.community_louvain as community_louvain\n",
    "\n",
    "\n",
    "# Apply Louvain method for community detection\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee872fb",
   "metadata": {},
   "source": [
    "Fraud Node Highlighting:\n",
    "\n",
    "    Fraudulent nodes (from df['is_fraud'] == 1) are colored red to make them stand out. The rest of the nodes are still colored based on their communities.\n",
    "    This should help you easily spot any fraudulent nodes in the network.\n",
    "\n",
    "Top 10 Most Central Nodes:\n",
    "\n",
    "    We calculate betweenness centrality and extract the top 10 most central nodes.\n",
    "    These nodes are visualized with their connections, which should help declutter the graph and focus on the key players in the transaction network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8fd26394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply Louvain method for community detection\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "# Create positions for nodes using a spring layout\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Add the community information to the DataFrame\n",
    "df['community'] = df['CreditCardNumber'].map(partition)\n",
    "\n",
    "# Highlight fraud nodes separately\n",
    "fraud_nodes = df[df['is_fraud'] == 1]['CreditCardNumber'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95613930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the community information to the DataFrame\n",
    "df['community'] = df['CreditCardNumber'].map(partition)\n",
    "\n",
    "# Calculate the percentage of fraud in each community\n",
    "community_fraud = df.groupby('community')['is_fraud'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d9407a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community\n",
      "0    0.004361\n",
      "1    0.005121\n",
      "2    0.008378\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print fraud rate per community\n",
    "print(community_fraud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17572504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community\n",
      "0    694955\n",
      "1    173807\n",
      "2    427913\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "community_size = df.groupby('community').size()\n",
    "print(community_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43cca311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fraud rates and community sizes into a single DataFrame\n",
    "fraud_vs_size = pd.concat([community_fraud, df.groupby('community').size()], axis=1)\n",
    "fraud_vs_size.columns = ['FraudRate', 'CommunitySize']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "accbc68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community\n",
      "2    0.008378\n",
      "1    0.005121\n",
      "0    0.004361\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "top_fraud_communities = community_fraud.sort_values(ascending=False).head(5)\n",
    "print(top_fraud_communities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4a875aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the community labels of the top fraud communities\n",
    "top_community_labels = top_fraud_communities.index.tolist()\n",
    "\n",
    "# Filter the DataFrame for only the top fraud communities\n",
    "top_communities_df = df[df['community'].isin(top_community_labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "88b0bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merchant\n",
      "fraud_Kozey-Boehm                       0.025723\n",
      "fraud_Herman, Treutel and Dickens       0.025385\n",
      "fraud_Kerluke-Abshire                   0.022307\n",
      "fraud_Brown PLC                         0.022109\n",
      "fraud_Goyette Inc                       0.021616\n",
      "fraud_Terry-Huel                        0.021543\n",
      "fraud_Jast Ltd                          0.021505\n",
      "fraud_Schmeler, Bashirian and Price     0.020833\n",
      "fraud_Boyer-Reichert                    0.019916\n",
      "fraud_Langworth, Boehm and Gulgowski    0.019807\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Show Only the Top Merchants by Fraud Rate:\n",
    "# Instead of displaying all merchants, you can filter the plot to show only the top 10 or 20 merchants with the highest fraud rates.\n",
    "\n",
    "# Calculate fraud rate by merchant in the top fraud communities\n",
    "merchant_fraud_rate = top_communities_df.groupby('merchant')['is_fraud'].mean()\n",
    "\n",
    "# Sort merchants by fraud rate in descending order\n",
    "top_merchants = merchant_fraud_rate.sort_values(ascending=False).head(10)\n",
    "\n",
    "# Print top 10 merchants with highest fraud rate\n",
    "print(top_merchants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bbd1dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'category' is a column representing merchant categories\n",
    "merchantcategory_fraud = top_communities_df.groupby('category')['is_fraud'].mean()\n",
    "\n",
    "# Sort the fraud rate by merchant category in descending order\n",
    "merchantcategory_fraud_sorted = merchantcategory_fraud.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e7c000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part9 - Community & Top Merchants completed at Sun Nov  3 18:48:15 2024. Elapsed time: 6 minutes and 58.80 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log_time(\"Part9 - Community & Top Merchants\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "154aa31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Density: 0.34130445623909095\n"
     ]
    }
   ],
   "source": [
    "# Check the density of the graph (a measure of sparsity)\n",
    "density = nx.density(G)\n",
    "print(f\"Graph Density: {density}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5e360c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Degree of Nodes: 571.6849642004773\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average degree\n",
    "degree_sequence = [degree for node, degree in G.degree()]\n",
    "average_degree = sum(degree_sequence) / len(degree_sequence)\n",
    "print(f\"Average Degree of Nodes: {average_degree}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5fa3b21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part10 - Density completed at Sun Nov  3 18:48:15 2024. Elapsed time: 0 minutes and 0.03 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part10 - Density\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6bcacc",
   "metadata": {},
   "source": [
    "The results you’ve provided show:\n",
    "\n",
    "    Graph Density: 0.2513\n",
    "        This is a moderate density value. A density of 0 would indicate a completely disconnected graph, while a value close to 1 would indicate a very tightly connected graph (like a clique). A density of 0.25 means about 25% of the possible connections between nodes are present, which suggests the graph isn’t overly sparse, but it’s not densely connected either.\n",
    "\n",
    "    Average Degree: 406.18\n",
    "        This is relatively high, meaning that, on average, each node (credit card or merchant) is connected to about 406 other nodes. This high degree could indicate that nodes, especially credit cards, are interacting with many merchants. However, these connections are likely not forming closed loops or triangles, which is why the clustering coefficient is zero for all nodes.\n",
    "\n",
    "What This Means:\n",
    "\n",
    "    Even though the average degree is high, suggesting that credit cards are interacting with many merchants, the interactions are likely not forming triangles (where connected nodes are also connected to each other). This results in zero clustering coefficients across the board.\n",
    "\n",
    "    The moderate graph density indicates that the network is connected to some extent, but not densely enough to produce high clustering coefficients.\n",
    "\n",
    "Why This Happens:\n",
    "\n",
    "In transaction networks, it’s common for credit cards to interact with different merchants, but merchants don’t typically transact with each other, which means closed triangles (required for a non-zero clustering coefficient) are rare. In fraud detection, this is normal, as fraudsters typically transact with many distinct merchants rather than creating highly connected communities.\n",
    "Next Steps:\n",
    "\n",
    "Given the moderate density and high degree of the nodes, the clustering coefficient might not be the most insightful metric. Instead, you could focus on the following:\n",
    "1. Focus on Betweenness Centrality and Degree:\n",
    "\n",
    "These metrics are more likely to highlight key nodes (e.g., credit cards or merchants) that are crucial in the transaction network. You’ve already calculated betweenness centrality, and the average degree indicates that some credit cards or merchants might have a significant number of connections.\n",
    "2. Look for Key Nodes (High Degree or Centrality):\n",
    "\n",
    "You could identify nodes with high degree or betweenness centrality to see if they’re involved in fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "452804a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'CreditCardNumber', 'merchant', 'category',\n",
      "       'TransactionAmount', 'first', 'last', 'gender', 'street', 'city',\n",
      "       'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID', 'Hour', 'HighRiskHour', 'DayOfWeek', 'DayName',\n",
      "       'IsWeekend', 'distance', 'HighRiskMerchantCategory', 'Age', 'AgeGroup',\n",
      "       'TransactionFrequency', 'RapidTransactionFlag', 'LogTransactionAmount',\n",
      "       'HighValueTransactionFlag', 'TransactionCountLast7Days',\n",
      "       'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
      "       'AverageTransactionAmountLast7Days',\n",
      "       'AverageTransactionAmountLast14Days',\n",
      "       'AverageTransactionAmountLast30Days', 'degree',\n",
      "       'betweenness_centrality', 'community'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d5be3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag',\n",
    "    'TransactionCountLast7Days', 'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
    "    'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast14Days', 'AverageTransactionAmountLast30Days',\n",
    "    'Hour', 'HighRiskHour', 'DayOfWeek', 'IsWeekend', 'TransactionFrequency', 'RapidTransactionFlag',\n",
    "    'lat', 'long', 'merch_lat', 'merch_long', 'distance', 'city_pop',\n",
    "    'Age', 'AgeGroup', 'gender', 'state', 'city',\n",
    "    'degree', 'betweenness_centrality', 'community'\n",
    "]\n",
    "\n",
    "df_selected_features = df[selected_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f21f1e",
   "metadata": {},
   "source": [
    "# Page rank as new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6fda32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PageRank for each node in the graph\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "# Map the PageRank values to the 'CreditCardNumber' in the DataFrame\n",
    "df['pagerank'] = df['CreditCardNumber'].map(pagerank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "53596e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in the pagerank column\n",
    "print(df['pagerank'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f5e2620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1.296675e+06\n",
      "mean     5.971670e-04\n",
      "std      7.856303e-05\n",
      "min      9.478567e-05\n",
      "25%      5.812104e-04\n",
      "50%      6.195727e-04\n",
      "75%      6.566711e-04\n",
      "max      6.834763e-04\n",
      "Name: pagerank, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check descriptive statistics of pagerank values\n",
    "print(df['pagerank'].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a525047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with zero PageRank: 0\n"
     ]
    }
   ],
   "source": [
    "# Check how many nodes have a PageRank of zero\n",
    "zero_pagerank_count = (df['pagerank'] == 0).sum()\n",
    "print(f\"Number of nodes with zero PageRank: {zero_pagerank_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c97f80b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PageRank for Fraud: 0.000505843602201983\n",
      "Average PageRank for Non-Fraud: 0.0005976986758972222\n"
     ]
    }
   ],
   "source": [
    "# Compare the average PageRank for fraud and non-fraud transactions\n",
    "fraud_avg_pagerank = df[df['is_fraud'] == 1]['pagerank'].mean()\n",
    "non_fraud_avg_pagerank = df[df['is_fraud'] == 0]['pagerank'].mean()\n",
    "\n",
    "print(f\"Average PageRank for Fraud: {fraud_avg_pagerank}\")\n",
    "print(f\"Average PageRank for Non-Fraud: {non_fraud_avg_pagerank}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "47121274",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features.append('pagerank')\n",
    "df_selected_features = df[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "439c0ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag',\n",
      "       'TransactionCountLast7Days', 'TransactionCountLast14Days',\n",
      "       'TransactionCountLast30Days', 'AverageTransactionAmountLast7Days',\n",
      "       'AverageTransactionAmountLast14Days',\n",
      "       'AverageTransactionAmountLast30Days', 'Hour', 'HighRiskHour',\n",
      "       'DayOfWeek', 'IsWeekend', 'TransactionFrequency',\n",
      "       'RapidTransactionFlag', 'lat', 'long', 'merch_lat', 'merch_long',\n",
      "       'distance', 'city_pop', 'Age', 'AgeGroup', 'gender', 'state', 'city',\n",
      "       'degree', 'betweenness_centrality', 'community', 'pagerank'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_selected_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ae9a67f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1296675, 46)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a89b08a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part11 - PageRank completed at Sun Nov  3 18:48:18 2024. Elapsed time: 0 minutes and 2.82 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part11 - PageRank\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e17cecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "56237852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END - Feature Engineering .....   completed at Sun Nov  3 18:48:18 2024. Elapsed time: 0 minutes and 0.04 seconds\n",
      "\n",
      "--------------------------------------------------- ------------------   completed at Sun Nov  3 18:48:18 2024. Elapsed time: 0 minutes and 0.00 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"END - Feature Engineering .....  \", start_time)\n",
    "start_time = time.time()\n",
    "log_time(\"--------------------------------------------------- ------------------  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac908b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "163f30c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest_Balanced_SMOTE_GridSearch_Train START Model ....   completed at Sun Nov  3 18:48:18 2024. Elapsed time: 0 minutes and 0.01 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(f\"{model_specs}_{dataset_type} START Model ....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14312bfe",
   "metadata": {},
   "source": [
    "Close the dASK Client"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7575e8b",
   "metadata": {},
   "source": [
    "from dask.distributed import Client\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Start Dask client\n",
    "client = Client()\n",
    "\n",
    "# Define directory paths and other configurations\n",
    "reports_output_dir = \"/path/to/reports\"  # Define the directory to save reports\n",
    "dataset_type = \"train\" if not use_test_data else \"test\"  # Set based on evaluation type\n",
    "\n",
    "# Load and preprocess function for both train and test datasets\n",
    "def load_and_preprocess(data_source):\n",
    "    X = df[selected_features]\n",
    "    y = df['is_fraud']\n",
    "    \n",
    "    # Convert to Dask, categorize, one-hot encode, then back to Pandas\n",
    "    X = dd.from_pandas(X, npartitions=5).categorize()\n",
    "    X = dd.get_dummies(X, drop_first=True).compute()  # Convert Dask DataFrame to Pandas\n",
    "    y = dd.from_pandas(y, npartitions=5).compute()  # Convert Dask Series to Pandas\n",
    "    return X, y\n",
    "\n",
    "# Model training and evaluation\n",
    "if use_test_data:\n",
    "    # Load and preprocess test data\n",
    "    X_test, y_test = load_and_preprocess(f\"{input_src_dir}/fraudTest.csv\")\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(\"Accuracy on test data:\", test_accuracy)\n",
    "    \n",
    "    # Generate classification report for test data\n",
    "    clf_report = classification_report(y_test, y_test_pred)\n",
    "    print(\"Classification Report: \" , clf_report)\n",
    "    \n",
    "else:\n",
    "    # Load and preprocess train data\n",
    "    X_train, y_train = load_and_preprocess(df)\n",
    "\n",
    "    # Train Random Forest model on train data\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on train data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    print(\"Accuracy on train data:\", train_accuracy)\n",
    "    \n",
    "    # Generate classification report for train data\n",
    "    clf_report = classification_report(y_train, y_train_pred)\n",
    "    print(\"Classification Report: \", clf_report)\n",
    "    accuracy = train_accuracy if not use_test_data else test_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a86f8",
   "metadata": {},
   "source": [
    "# The following worked wonderfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae088d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36e6f6e4",
   "metadata": {},
   "source": [
    "Optimize Feature Set: Consider focusing more on these top features to streamline the model, which could reduce noise and potentially improve generalization.\n",
    "\n",
    "Interaction Features: You might explore interaction terms (e.g., combining distance with high transaction amounts) if you suspect relationships between features could capture more nuanced patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5027114",
   "metadata": {},
   "source": [
    "# the below code with optimized features also worked great"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ef169",
   "metadata": {},
   "source": [
    "def load_and_preprocess(df, selected_features, sample_fraction=1.0):\n",
    "    # Filter `selected_features` to include only columns available in `df`\n",
    "    selected_features = [col for col in selected_features if col in df.columns]\n",
    "    \n",
    "    # Sample a fraction of the data for debugging if needed\n",
    "    if sample_fraction < 1.0:\n",
    "        df = df.sample(frac=sample_fraction, random_state=42)\n",
    "    \n",
    "    X = df[selected_features].copy()\n",
    "    y = df['is_fraud'].copy()\n",
    "\n",
    "    # Define numeric and categorical features explicitly\n",
    "    numeric_features = [\n",
    "        'TransactionAmount', 'LogTransactionAmount', 'TransactionCountLast7Days', \n",
    "        'TransactionCountLast14Days', 'TransactionCountLast30Days', \n",
    "        'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast14Days', \n",
    "        'AverageTransactionAmountLast30Days', 'lat', 'long', 'merch_lat', \n",
    "        'merch_long', 'distance', 'city_pop', 'degree', 'betweenness_centrality'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = [\n",
    "        'HighValueTransactionFlag', 'HighRiskHour', 'IsWeekend', 'AgeGroup', \n",
    "        'gender', 'state', 'city', 'community', 'TransactionFrequency'\n",
    "    ]\n",
    "    \n",
    "    # Use only the numeric and categorical columns available in `X`\n",
    "    numeric_features = [col for col in numeric_features if col in X.columns]\n",
    "    categorical_features = [col for col in categorical_features if col in X.columns]\n",
    "    \n",
    "    # Convert each numeric column to numeric type and fill NaNs with mean\n",
    "    for col in numeric_features:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')  # Convert to numeric, coerce errors to NaN\n",
    "        X[col] = X[col].fillna(X[col].mean())  # Fill NaNs with mean for each column\n",
    "\n",
    "    # Convert categorical columns to 'category' dtype and fill NaNs with \"Unknown\"\n",
    "    for col in categorical_features:\n",
    "        X[col] = X[col].astype(\"category\")\n",
    "        X[col] = X[col].cat.add_categories(\"Unknown\").fillna(\"Unknown\")\n",
    "\n",
    "    # Convert to Dask DataFrame\n",
    "    X = dd.from_pandas(X, npartitions=5)\n",
    "\n",
    "    # Explicitly categorize all categorical columns\n",
    "    X = X.categorize(columns=categorical_features)\n",
    "\n",
    "    # Apply one-hot encoding to categorized columns without immediately computing\n",
    "    X = dd.get_dummies(X, drop_first=True)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Define optimized features\n",
    "optimized_features = [\n",
    "    'LogTransactionAmount', 'merch_lat', 'AverageTransactionAmountLast14Days',\n",
    "    'TransactionAmount', 'distance', 'merch_long', \n",
    "    'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast30Days',\n",
    "    'TransactionCountLast30Days', 'TransactionCountLast7Days'\n",
    "]\n",
    "\n",
    "# Load and preprocess with optimized features and handle missing columns automatically\n",
    "X_train, y_train = load_and_preprocess(df, optimized_features, sample_fraction=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d0994",
   "metadata": {},
   "source": [
    "# now with interaction features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6453aa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(use_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2820d1f",
   "metadata": {},
   "source": [
    "#Load_and_preprocess with OPTIMIZED features:\n",
    "The following is done successfully,howvere test set generated accuracy of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630958cb",
   "metadata": {},
   "source": [
    "both downsampling and upsampling (using SMOTE) are applied to balance the classes. Here’s a breakdown of the approach and its purpose:\n",
    "Steps in the Code\n",
    "\n",
    "    Downsampling the Majority Class (non-fraud):\n",
    "        The non-fraud samples (majority class) are downsampled to match the number of fraud samples. This reduces the dataset size and helps balance it, ensuring the model doesn’t become overly biased toward the non-fraud majority.\n",
    "\n",
    "    SMOTE Upsampling:\n",
    "        After downsampling, SMOTE (Synthetic Minority Over-sampling Technique) is applied to further balance the data by synthetically creating additional samples of the minority class (fraud). This approach helps the model generalize by exposing it to varied patterns in the minority class.\n",
    "\n",
    "Why This Combination is Used\n",
    "\n",
    "Using both downsampling and SMOTE is beneficial in scenarios with extreme class imbalance, as it:\n",
    "\n",
    "    Reduces the dataset size, making training more efficient without overwhelming the model with non-fraud samples.\n",
    "    Adds synthetic diversity to the fraud class, improving the model’s ability to generalize to different fraud patterns.\n",
    "\n",
    "Why the Model May Still Struggle on fraudTest.csv\n",
    "\n",
    "Despite these efforts, testing on an imbalanced fraudTest.csv with no additional balancing measures means the model faces a very different class distribution compared to the training data. Here’s why the performance may not translate well to the test set:\n",
    "\n",
    "    Synthetic Patterns vs. Real Data: SMOTE creates synthetic samples that may not fully reflect real-world fraud cases. As a result, the model may be less accurate on genuine fraud cases in the test data.\n",
    "    Threshold Sensitivity: The default threshold may still be biased toward predicting non-fraud, as fraud cases remain rare in fraudTest.csv. Lowering this threshold can improve recall on fraud cases.\n",
    "\n",
    "Suggested Adjustments\n",
    "\n",
    "To improve test performance on fraudTest.csv:\n",
    "\n",
    "    Adjust the Classification Threshold: As previously recommended, lowering the threshold (e.g., to 0.3) will make the model more sensitive to fraud cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0b0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d2e715b",
   "metadata": {},
   "source": [
    "below code is now comented out for above reason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e0c282",
   "metadata": {},
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.experimental import enable_halving_search_cv  # Required for ParallelPostFit\n",
    "\n",
    "# Define optimized features at the top to make it accessible in both blocks\n",
    "optimized_features = [\n",
    "    'LogTransactionAmount', 'merch_lat', 'AverageTransactionAmountLast14Days',\n",
    "    'TransactionAmount', 'distance', 'merch_long', \n",
    "    'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast30Days',\n",
    "    'TransactionCountLast30Days', 'TransactionCountLast7Days'\n",
    "]\n",
    "\n",
    "def load_and_preprocess(df, selected_features, sample_fraction=1.0):\n",
    "    # Sample and select features\n",
    "    X_train = df[selected_features].sample(frac=sample_fraction)\n",
    "    y_train = df['is_fraud']  # Assuming 'is_fraud' is the target column\n",
    "    \n",
    "    # Convert to Pandas if using Dask DataFrame\n",
    "    if hasattr(X_train, \"compute\"):\n",
    "        X_train = X_train.compute().reset_index(drop=True)\n",
    "    else:\n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "    \n",
    "    # Reset index for y_train if needed\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "    # Generate pairwise interactions only for features that exist in X_train\n",
    "    existing_features = [feature for feature in selected_features if feature in X_train.columns]\n",
    "    for feature1, feature2 in combinations(existing_features, 2):\n",
    "        interaction_name = f\"{feature1}_x_{feature2}\"\n",
    "        X_train[interaction_name] = X_train[feature1] * X_train[feature2]\n",
    "\n",
    "    print(X_train.head())  # Preview the final dataframe with interaction terms\n",
    "    return X_train, y_train\n",
    "\n",
    "# Model training and evaluation\n",
    "if use_test_data:\n",
    "    # Load and preprocess test data\n",
    "    X_test, y_test = load_and_preprocess(df, optimized_features)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(\"Accuracy on test data:\", test_accuracy)\n",
    "    \n",
    "    # Classification report\n",
    "    clf_report = classification_report(y_test, y_test_pred)\n",
    "    print(\"Classification Report:\", clf_report)\n",
    "\n",
    "    # Calculate AUC-ROC\n",
    "    roc_auc = roc_auc_score(y_test, y_test_pred)\n",
    "    print(\"ROC AUC Score on test data:\", roc_auc)\n",
    "\n",
    "    # Calculate Precision-Recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_test_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print(\"Precision-Recall AUC on test data:\", pr_auc)\n",
    "\n",
    "    # Plot Precision-Recall curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(recall, precision, marker='.', label='Precision-Recall Curve (Test)')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    roc_title = 'Random Forest with DASK Balanced & SMOTE - Precision-Recall Curve for Test Set'\n",
    "    plt.title(roc_title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    # Training phase with sampling and SMOTE\n",
    "    X_train, y_train = load_and_preprocess(df, optimized_features, sample_fraction=1.0)\n",
    "    X_train = X_train.reset_index(drop=True)  # Convert X_train to Pandas if needed\n",
    "    y_train = y_train.reset_index(drop=True)  # Reset y_train index for alignment\n",
    "    \n",
    "\n",
    "    # Initialize model\n",
    "    # model = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, class_weight='balanced')\n",
    "    # this lead to 50% accuracy on test data\n",
    "    \n",
    "    # so Set max_depth to a lower value, such as 10, to reduce the complexity and make the model more generalizable.\n",
    "    # Experiment with fewer estimators (e.g., n_estimators=50), which can also help prevent the model \n",
    "    #.       from learning overly specific patterns in the training data.\n",
    "    # this lead to 88% accuracy with sample factor = 0.4\n",
    "    \n",
    "    # now same model with sample factor = 0.8\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, class_weight='balanced')\n",
    "\n",
    "    \n",
    "\n",
    "    # Downsample and SMOTE Upsample\n",
    "    fraud = X_train[y_train == 1]\n",
    "    non_fraud = X_train[y_train == 0]\n",
    "\n",
    "    # Downsample non-fraud class to match fraud class size\n",
    "    non_fraud_downsampled = resample(non_fraud, replace=False, n_samples=len(fraud), random_state=42)\n",
    "\n",
    "    # Concatenate downsampled non-fraud and fraud samples\n",
    "    X_resampled = pd.concat([fraud, non_fraud_downsampled]).reset_index(drop=True)\n",
    "    y_resampled = pd.concat([\n",
    "        y_train.loc[fraud.index],\n",
    "        y_train.loc[non_fraud_downsampled.index]\n",
    "    ]).reset_index(drop=True)\n",
    "\n",
    "    # Verify that X_resampled and y_resampled have the same length\n",
    "    assert len(X_resampled) == len(y_resampled), \"Lengths of X_resampled and y_resampled do not match.\"\n",
    "\n",
    "    # Apply SMOTE on the resampled data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = smote.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_smote, y_smote)\n",
    "\n",
    "    # Evaluate on train data\n",
    "    y_train_pred = model.predict(X_smote)\n",
    "    train_accuracy = accuracy_score(y_smote, y_train_pred)\n",
    "    print(\"Accuracy on train data:\", train_accuracy)\n",
    "    \n",
    "    # Classification report\n",
    "    clf_report = classification_report(y_smote, y_train_pred)\n",
    "    print(\"Classification Report:\", clf_report)\n",
    "\n",
    "    # Calculate AUC-ROC\n",
    "    roc_auc = roc_auc_score(y_smote, y_train_pred)\n",
    "    print(\"ROC AUC Score on train data:\", roc_auc)\n",
    "\n",
    "    # Calculate Precision-Recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_smote, y_train_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print(\"Precision-Recall AUC on train data:\", pr_auc)\n",
    "\n",
    "    # Plot Precision-Recall curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(recall, precision, marker='.', label='Precision-Recall Curve (Train)')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    roc_title = 'sample_fraction=1.0 Random Forest with DASK Balanced & SMOTE - Precision-Recall Curve for Train Set'\n",
    "    plt.title(roc_title)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd0e3a7",
   "metadata": {},
   "source": [
    "Threshold with 0.3 didnt lead to good results\n",
    "Train ROC AUC: 0.5029976019184652\n",
    "Train Precision-Recall AUC: 0.7507516536380037\n",
    "    \n",
    "The results show that the model has overfit to the minority class (fraud), leading to a low recall and accuracy for non-fraud cases. Here are a few strategies to improve model performance:\n",
    "\n",
    "    Increase Number of Estimators and Depth: Try increasing n_estimators to 100 or more and max_depth to allow the model to better capture patterns. You may need to balance this with computational limits.\n",
    "\n",
    "    Refine Class Weights: The class weights {0: 1, 1: 10} might be too aggressive, leading the model to overemphasize fraud cases. You could reduce the weight for fraud, such as {0: 1, 1: 5}, or even use class_weight='balanced'.\n",
    "\n",
    "    Threshold Calibration: The current threshold of 0.3 might need further adjustment. Consider evaluating several thresholds (e.g., from 0.1 to 0.5) based on validation data to find the best trade-off between precision and recall.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1a02d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df33fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3bd4e70",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "# Define the features to be used in the model\n",
    "optimized_features = [\n",
    "    'LogTransactionAmount', 'merch_lat', 'AverageTransactionAmountLast14Days',\n",
    "    'TransactionAmount', 'distance', 'merch_long', \n",
    "    'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast30Days',\n",
    "    'TransactionCountLast30Days', 'TransactionCountLast7Days'\n",
    "]\n",
    "\n",
    "# Function to load and preprocess the data\n",
    "def load_and_preprocess(df, selected_features, sample_fraction=1.0):\n",
    "    X = df[selected_features].sample(frac=sample_fraction)\n",
    "    y = df['is_fraud']\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    \n",
    "    # Add interaction terms\n",
    "    for feature1, feature2 in combinations(selected_features, 2):\n",
    "        interaction_name = f\"{feature1}_x_{feature2}\"\n",
    "        X[interaction_name] = X[feature1] * X[feature2]\n",
    "    \n",
    "    print(\"Sample of X with interactions:\", X.head())  # Preview the dataframe with interactions\n",
    "    return X, y\n",
    "\n",
    "# Function for model training with GridSearchCV\n",
    "def train_model_with_grid_search(X, y):\n",
    "    # Balance the data\n",
    "    fraud = X[y == 1]\n",
    "    non_fraud = X[y == 0]\n",
    "    non_fraud_downsampled = resample(non_fraud, replace=False, n_samples=len(fraud), random_state=42)\n",
    "    \n",
    "    # Combine the downsampled non-fraud and fraud samples\n",
    "    X_resampled = pd.concat([fraud, non_fraud_downsampled]).reset_index(drop=True)\n",
    "    y_resampled = pd.concat([y.loc[fraud.index], y.loc[non_fraud_downsampled.index]]).reset_index(drop=True)\n",
    "    \n",
    "    # Apply SMOTE to generate synthetic samples of the minority class\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = smote.fit_resample(X_resampled, y_resampled)\n",
    "    \n",
    "    # Define the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'class_weight': [{0: 1, 1: 3}, {0: 1, 1: 5}, {0: 1, 1: 10}]\n",
    "    }\n",
    "    \n",
    "    # Initialize the RandomForestClassifier\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # Set up GridSearchCV with 3-fold cross-validation\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
    "    grid_search.fit(X_smote, y_smote)\n",
    "    \n",
    "    # Best model after grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best Parameters from GridSearch:\", grid_search.best_params_)\n",
    "    \n",
    "    # Evaluate on training data\n",
    "    y_train_pred = best_model.predict(X_smote)\n",
    "    train_accuracy = accuracy_score(y_smote, y_train_pred))\n",
    "    print(\"Training Accuracy:\", accuracy)\n",
    "    \n",
    "    clf = classification_report(y_smote, y_train_pred)\n",
    "    print(\"Classification Report (Train):\\n\", classification_report(y_smote, y_train_pred))\n",
    "    \n",
    "    # Calculate and display AUC and Precision-Recall curves\n",
    "    print(\"Train ROC AUC:\", roc_auc_score(y_smote, y_train_pred))\n",
    "    precision, recall, _ = precision_recall_curve(y_smote, y_train_pred)\n",
    "    print(\"Train Precision-Recall AUC:\", auc(recall, precision))\n",
    "    plt.plot(recall, precision, label=\"Train Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Train Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model, accuracy, clf, precision, recall\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "    # Predict probabilities and adjust threshold\n",
    "    y_test_probs = model.predict_proba(X_test)[:, 1]\n",
    "    y_test_pred_adjusted = (y_test_probs >= threshold).astype(int)\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred_adjusted)\n",
    "    print(\"Adjusted Threshold Test Accuracy:\", test_accuracy)\n",
    "    \n",
    "    clf = classification_report(y_test, y_test_pred_adjusted)\n",
    "    print(\"Classification Report (Test, Adjusted Threshold):\\n\", classification_report(y_test, y_test_pred_adjusted))\n",
    "    print(\"Test ROC AUC:\", roc_auc_score(y_test, y_test_pred_adjusted))\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_test_pred_adjusted)\n",
    "    print(\"Test Precision-Recall AUC:\", auc(recall, precision))\n",
    "    plt.plot(recall, precision, label=\"Test Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Test Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model, test_accuracy, clf, precision, recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ab80428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "# Define features\n",
    "optimized_features = [\n",
    "    'LogTransactionAmount', 'merch_lat', 'AverageTransactionAmountLast14Days',\n",
    "    'TransactionAmount', 'distance', 'merch_long', \n",
    "    'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast30Days',\n",
    "    'TransactionCountLast30Days', 'TransactionCountLast7Days'\n",
    "]\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess(df, selected_features, sample_fraction=1.0):\n",
    "    X = df[selected_features].sample(frac=sample_fraction)\n",
    "    y = df['is_fraud']\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    # Add interaction terms\n",
    "    for feature1, feature2 in combinations(selected_features, 2):\n",
    "        interaction_name = f\"{feature1}_x_{feature2}\"\n",
    "        X[interaction_name] = X[feature1] * X[feature2]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e8b47591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_improved_sampling(X, y):\n",
    "    # Resample with less downsampling\n",
    "    fraud, non_fraud = X[y == 1], X[y == 0]\n",
    "    non_fraud_downsampled = resample(non_fraud, replace=False, n_samples=int(len(fraud) * 3), random_state=42)\n",
    "    X_resampled = pd.concat([fraud, non_fraud_downsampled]).reset_index(drop=True)\n",
    "    y_resampled = pd.concat([y[fraud.index], y[non_fraud_downsampled.index]]).reset_index(drop=True)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = smote.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "    # Grid search with heavier class weights\n",
    "    param_grid = {'n_estimators': [100, 200], 'max_depth': [15, 20], 'class_weight': [{0: 1, 1: 10}]}\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
    "    grid_search.fit(X_smote, y_smote)\n",
    "\n",
    "    # Evaluate best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_train_pred = best_model.predict(X_smote)\n",
    "    roc_auc = roc_auc_score(y_smote, y_train_pred)\n",
    "    print(\"Train ROC AUC:\", roc_auc)\n",
    "    precision, recall, _ = precision_recall_curve(y_smote, y_train_pred)\n",
    "    auc_pr = auc(recall, precision)\n",
    "    print(\"Train Precision-Recall AUC:\", auc_pr)\n",
    "\n",
    "    # Plot Precision-Recall curve\n",
    "    plt.plot(recall, precision, label=\"Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return best_model, X_smote, y_smote, roc_auc, auc_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "54d1ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Evaluation on test set\n",
    "def evaluate_model(model, X_test, y_test, threshold=0.5):\n",
    "    y_test_probs = model.predict_proba(X_test)[:, 1]\n",
    "    y_test_pred = (y_test_probs >= threshold).astype(int)\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    \n",
    "    test_clf = classification_report(y_test, y_test_pred)\n",
    "    print(\"Classification Report (Test):\\n\", test_clf)\n",
    "    \n",
    "    \n",
    "    # Evaluation metrics\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_pred_adjusted)\n",
    "    print(\"Test ROC AUC:\", test_roc_auc)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_test_pred_adjusted)\n",
    "    test_auc_pr = auc(recall, precision)  # Renamed to avoid conflict\n",
    "    \n",
    "    print(\"Test Precision-Recall AUC:\", auc(recall, precision))\n",
    "    plt.plot(recall, precision, label=\"Test Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Test Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return best_model, X_test, y_test, test_accuracy, test_clf,  test_roc_auc, test_auc_pr # return balanced dataset for report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6c45080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report generation and saving\n",
    "def generate_and_save_report(model, x_input, y_output, accuracy, clf, roc_auc, auc_pr, report_dir=\"reports\", file_name=\"classification_report.txt\"):\n",
    "    report_path = os.path.join(report_dir, file_name)\n",
    "\n",
    "    # Create the report content\n",
    "    full_report = (\n",
    "        f\"{dataset_type}_Accuracy: {accuracy:.4f}\\n\\n\"\n",
    "        f\"Classification Report:\\n{clf}\\n\\n\"\n",
    "        f\"ROC AUC: {roc_auc:.4f}\\n\\n\"\n",
    "        f\"Precision-Recall AUC: {auc_pr:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Write the report to a file\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(full_report)\n",
    "    print(f\"Report saved to {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e6f48b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC AUC: 0.5403455013766765\n",
      "Train Precision-Recall AUC: 0.7605104236562623\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     X_train, y_train \u001b[38;5;241m=\u001b[39m load_and_preprocess(df, optimized_features)\n\u001b[0;32m---> 14\u001b[0m     best_rf_model, X_smote, y_smote, accuracy, clf, roc_auc, auc_pr \u001b[38;5;241m=\u001b[39m train_model_with_improved_sampling(X_train, y_train)\n\u001b[1;32m     15\u001b[0m     generate_and_save_report(best_rf_model, X_smote, y_smote, accuracy, clf, roc_auc, auc_pr, report_dir\u001b[38;5;241m=\u001b[39mreports_output_dir, file_name\u001b[38;5;241m=\u001b[39mclassification_report_filename)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 5)"
     ]
    }
   ],
   "source": [
    "\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "# Define the report title and output filename\n",
    "classification_report_title = f\" Report \"\n",
    "classification_report_filename = f\"{model_specs.replace(',', '')}_{dataset_type}_{classification_report_title}_{timestamp}.txt\"\n",
    "\n",
    "# Execute training and evaluation\n",
    "if use_test_data:\n",
    "    X_test, y_test = load_and_preprocess(df, optimized_features)\n",
    "    best_rf_model, X_test, y_test, accuracy, clf, roc_auc, auc_pr = evaluate_model(best_rf_model, X_test, y_test)\n",
    "    generate_and_save_report(best_rf_model, X_test, y_test, accuracy, clf, roc_auc, auc_pr, report_dir=reports_output_dir, file_name=classification_report_filename)\n",
    "\n",
    "else:\n",
    "    X_train, y_train = load_and_preprocess(df, optimized_features)\n",
    "    best_rf_model, X_smote, y_smote, accuracy, clf, roc_auc, auc_pr = train_model_with_improved_sampling(X_train, y_train)\n",
    "    generate_and_save_report(best_rf_model, X_smote, y_smote, accuracy, clf, roc_auc, auc_pr, report_dir=reports_output_dir, file_name=classification_report_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e91db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time(f\"{model_specs}_{dataset_type} END Model ....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d9134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Close Dask client when done\n",
    "client.close()\n",
    "log_time(\"Closed the DASK Client\", start_time)\n",
    "log_time(\".................................................  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4713ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Specify the output directory\n",
    "output_dir_model = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/models'\n",
    "if not os.path.exists(output_dir_model):\n",
    "    os.makedirs(output_dir_model)  # Ensure the directory exists\n",
    "\n",
    "# Define model filename\n",
    "model_outputfilename = f\"{model_specs.replace(' ', '_').replace(',', '').lower()}_{dataset_type}.pkl\"\n",
    "\n",
    "if use_tset_data:\n",
    "    print(\"no need to save model while testing phase..\")\n",
    "else:\n",
    "\n",
    "    # Assuming best_rf_model is your final or best model\n",
    "    try:\n",
    "        # Save the model\n",
    "        with open(os.path.join(output_dir_model, model_outputfilename), 'wb') as model_file:\n",
    "            pickle.dump(best_rf_model, model_file)\n",
    "\n",
    "        print(f\"Model saved to {os.path.join(output_dir_model, model_outputfilename)}\")\n",
    "\n",
    "    except NameError:\n",
    "        print(\"Error: 'best_rf_model' is not defined. Please ensure the model is assigned before saving.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d57811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Assuming start_time is defined earlier in the notebook\n",
    "end_time_notebook = time.time()\n",
    "elapsed_time = end_time_notebook - start_time_notebook\n",
    "\n",
    "# Print and format the notebook end time and total execution time\n",
    "print(f\"Notebook ended at: {time.ctime(end_time_notebook)}\")\n",
    "print(f\"Total execution time: {elapsed_time // 60:.0f} minutes and {elapsed_time % 60:.2f} seconds\")\n",
    "\n",
    "\n",
    "log_time(f\"{model_specs}_{dataset_type} Notebook Ended at... \", start_time_notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b3bb83",
   "metadata": {},
   "source": [
    "# Observation for this run on SMOTE Gridsearch on Train:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768f671",
   "metadata": {},
   "source": [
    "Best Params: {'class_weight': {0: 1, 1: 3}, 'max_depth': 10, 'n_estimators': 50}\n",
    "Train Accuracy: 0.5091926458832934\n",
    "Classification Report (Train):\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "         0.0       1.00      0.02      0.04      7506\n",
    "         1.0       0.50      1.00      0.67      7506\n",
    "\n",
    "    accuracy                           0.51     15012\n",
    "   macro avg       0.75      0.51      0.35     15012\n",
    "weighted avg       0.75      0.51      0.35     15012\n",
    "\n",
    "Train ROC AUC: 0.5091926458832934\n",
    "Train Precision-Recall AUC: 0.7523194836627672"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c359fe",
   "metadata": {},
   "source": [
    "# moving now to xgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72358366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ZHAW_Project)",
   "language": "python",
   "name": "zhaw_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
