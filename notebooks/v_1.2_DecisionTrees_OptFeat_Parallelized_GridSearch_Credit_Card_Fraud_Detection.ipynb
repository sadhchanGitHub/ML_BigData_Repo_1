{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b2470767",
   "metadata": {},
   "source": [
    "# v2 - Decision Trees Parallelized GridSearch\n",
    "\n",
    "\n",
    "1. MultiProcessing for FeatureEngineering - distance, count_transactions_within_last_hour,betweeness centrality\n",
    "2. Code is modularised in \"references\" folder \n",
    "    decisiontree_gridsearch_modelutils.py\n",
    "      -- parallel_one_hot_encode  -- This Module will parallelize OneHot Encoding.\n",
    "      -- preprocessdata_parallel_onehot_impute   -- This Module will parallelize preprocessing data\n",
    "      -- perform_grid_search --uses n_jobs = -1 to parallelize GridSearch\n",
    "      -- generate_classification_report_and_roc\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8029af83",
   "metadata": {},
   "source": [
    "NOTE:\n",
    "This notebook is tseted to call two functions perform_grid_search and perform_grid_search_balanced\n",
    "with top 10 or features:importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e22053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import geopy\n",
    "from geopy.distance import geodesic\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ff9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../references')  # Add the references folder to the system path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d635934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_specs = 'DecisionTrees_Parallelized_GridSearch_by_Top10Features'\n",
    "\n",
    "# now with perform_grid_search_balanced\n",
    "model_specs = 'DecisionTrees_Parallelized_GridSearch_Balanced_by_Top10Features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab52f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_notebook = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08142377",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c56c046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/DecisionTrees\n"
     ]
    }
   ],
   "source": [
    "# Directory to save the figures \n",
    "\n",
    "input_src_dir = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/data/raw'\n",
    "output_dir_figures_train = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/figures/train_figures'\n",
    "output_dir_figures_test = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/figures/test_figures'\n",
    "\n",
    "\n",
    "reports_output_dir_base = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports'\n",
    "# reports_output_dir for DecisionTrees\n",
    "reports_output_dir = f\"{reports_output_dir_base}/DecisionTrees\"\n",
    "print(reports_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e37d27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which dataset to use\n",
    "use_test_data = False  # Set to True when using fraudtest.csv\n",
    "\n",
    "# Determine dataset type based on the variable\n",
    "dataset_type = 'Test' if use_test_data else 'Train'\n",
    "\n",
    "# Load the appropriate dataset\n",
    "\n",
    "if use_test_data:\n",
    "    output_dir_figures = output_dir_figures_test\n",
    "else:\n",
    "    output_dir_figures = output_dir_figures_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd0bc5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the preprocess file name dynamically\n",
    "# Get the current timestamp\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")  # Format: YYYYMMDD_HHMMSS\n",
    "\n",
    "logfile_title = 'LogFile'\n",
    "logfile_name = f\"{model_specs}_{dataset_type}_{logfile_title.replace(',', '').lower().split('.')[0]}_{timestamp}.txt\"\n",
    "\n",
    "logfile_path = os.path.join(reports_output_dir, logfile_name)\n",
    "\n",
    "# Function to log times to a file\n",
    "def log_time(step_name, start_time):\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    log_message = (f\"{step_name} completed at {time.ctime(end_time)}. \"\n",
    "                   f\"Elapsed time: {elapsed_time // 60:.0f} minutes and {elapsed_time % 60:.2f} seconds\\n\")\n",
    "    \n",
    "    # Append log to file\n",
    "    with open(logfile_path, 'a') as f:\n",
    "        f.write(log_message)\n",
    "    \n",
    "    # Print the message to the console as well\n",
    "    print(log_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "383fd5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTrees_Parallelized_GridSearch_Balanced_by_Top10Features_Train Notebook  started at...  completed at Tue Nov  5 09:37:49 2024. Elapsed time: 0 minutes and 0.04 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log_time(f\"{model_specs}_{dataset_type} Notebook  started at... \", start_time_notebook)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47704d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the appropriate dataset\n",
    "\n",
    "if use_test_data:\n",
    "    df = pd.read_csv(f\"{input_src_dir}/fraudTest.csv\")  # Concatenate the directory with the filename\n",
    "else:\n",
    "    df = pd.read_csv(f\"{input_src_dir}/fraudTrain.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ff38959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category',\n",
      "       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n",
      "       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n",
      "       'merch_lat', 'merch_long', 'is_fraud'],\n",
      "      dtype='object')\n",
      "(1296675, 23)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66bca2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "df = df.rename(columns={'amt': 'TransactionAmount', 'cc_num': 'CreditCardNumber', 'dob': 'DateOfBirth', 'trans_date_trans_time': 'TransactionTime'})\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba8e77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique TransactionID for each row\n",
    "df['TransactionID'] = range(1, len(df) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccf4f812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID'],\n",
      "      dtype='object')\n",
      "(1296675, 24)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b6036bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " Unnamed: 0           0\n",
      "TransactionTime      0\n",
      "CreditCardNumber     0\n",
      "merchant             0\n",
      "category             0\n",
      "TransactionAmount    0\n",
      "first                0\n",
      "last                 0\n",
      "gender               0\n",
      "street               0\n",
      "city                 0\n",
      "state                0\n",
      "zip                  0\n",
      "lat                  0\n",
      "long                 0\n",
      "city_pop             0\n",
      "job                  0\n",
      "DateOfBirth          0\n",
      "trans_num            0\n",
      "unix_time            0\n",
      "merch_lat            0\n",
      "merch_long           0\n",
      "is_fraud             0\n",
      "TransactionID        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values)\n",
    "\n",
    "# no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d454f706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_fraud\n",
      "0    1289169\n",
      "1       7506\n",
      "Name: count, dtype: int64\n",
      "is_fraud\n",
      "0    99.421135\n",
      "1     0.578865\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Count of fraud and non-fraud transactions\n",
    "fraud_counts = df['is_fraud'].value_counts()\n",
    "print(fraud_counts)\n",
    "\n",
    "# Optionally, you can get it in percentage terms\n",
    "fraud_percentage = df['is_fraud'].value_counts(normalize=True) * 100\n",
    "print(fraud_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c60dd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many unique credit cards in the data set ??\n",
    "df['CreditCardNumber'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "391577c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75b3c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TransactionTime to datetime\n",
    "df['TransactionTime'] = pd.to_datetime(df['TransactionTime'])\n",
    "\n",
    "# Optional: Convert DateOfBirth to datetime, if needed\n",
    "df['DateOfBirth'] = pd.to_datetime(df['DateOfBirth'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52941f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-01 00:00:18', '2019-01-01 00:00:44',\n",
      "               '2019-01-01 00:00:51', '2019-01-01 00:01:16',\n",
      "               '2019-01-01 00:03:06', '2019-01-01 00:04:08',\n",
      "               '2019-01-01 00:04:42', '2019-01-01 00:05:08',\n",
      "               '2019-01-01 00:05:18', '2019-01-01 00:06:01',\n",
      "               ...\n",
      "               '2020-06-21 12:08:42', '2020-06-21 12:09:22',\n",
      "               '2020-06-21 12:10:56', '2020-06-21 12:11:23',\n",
      "               '2020-06-21 12:11:36', '2020-06-21 12:12:08',\n",
      "               '2020-06-21 12:12:19', '2020-06-21 12:12:32',\n",
      "               '2020-06-21 12:13:36', '2020-06-21 12:13:37'],\n",
      "              dtype='datetime64[ns]', name='TransactionTime', length=1296675, freq=None)\n"
     ]
    }
   ],
   "source": [
    "# Set 'TransactionTime' as the index permanently\n",
    "df.set_index('TransactionTime', inplace=True)\n",
    "\n",
    "# Verify the index\n",
    "print(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55397673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Transaction Time: 2019-01-01 00:00:18\n",
      "Maximum Transaction Time: 2020-06-21 12:13:37\n"
     ]
    }
   ],
   "source": [
    "# Get the minimum and maximum transaction times from the index\n",
    "min_time = df.index.min()\n",
    "max_time = df.index.max()\n",
    "\n",
    "print(f\"Minimum Transaction Time: {min_time}\")\n",
    "print(f\"Maximum Transaction Time: {max_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1fc7ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CreditCardNumber</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>TransactionAmount</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>DateOfBirth</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>TransactionID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:18</th>\n",
       "      <td>0</td>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>Moravian Falls</td>\n",
       "      <td>...</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:44</th>\n",
       "      <td>1</td>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>Orient</td>\n",
       "      <td>...</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:51</th>\n",
       "      <td>2</td>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>Malad City</td>\n",
       "      <td>...</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:01:16</th>\n",
       "      <td>3</td>\n",
       "      <td>3534093764340240</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>9443 Cynthia Court Apt. 038</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>...</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>1939</td>\n",
       "      <td>Patent attorney</td>\n",
       "      <td>1967-01-12</td>\n",
       "      <td>6b849c168bdad6f867558c3793159a81</td>\n",
       "      <td>1325376076</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:03:06</th>\n",
       "      <td>4</td>\n",
       "      <td>375534208663984</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>Garcia</td>\n",
       "      <td>M</td>\n",
       "      <td>408 Bradley Rest</td>\n",
       "      <td>Doe Hill</td>\n",
       "      <td>...</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>99</td>\n",
       "      <td>Dance movement psychotherapist</td>\n",
       "      <td>1986-03-28</td>\n",
       "      <td>a41d7549acf90789359a9aa5346dcb46</td>\n",
       "      <td>1325376186</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Unnamed: 0  CreditCardNumber  \\\n",
       "TransactionTime                                     \n",
       "2019-01-01 00:00:18           0  2703186189652095   \n",
       "2019-01-01 00:00:44           1      630423337322   \n",
       "2019-01-01 00:00:51           2    38859492057661   \n",
       "2019-01-01 00:01:16           3  3534093764340240   \n",
       "2019-01-01 00:03:06           4   375534208663984   \n",
       "\n",
       "                                               merchant       category  \\\n",
       "TransactionTime                                                          \n",
       "2019-01-01 00:00:18          fraud_Rippin, Kub and Mann       misc_net   \n",
       "2019-01-01 00:00:44     fraud_Heller, Gutmann and Zieme    grocery_pos   \n",
       "2019-01-01 00:00:51                fraud_Lind-Buckridge  entertainment   \n",
       "2019-01-01 00:01:16  fraud_Kutch, Hermiston and Farrell  gas_transport   \n",
       "2019-01-01 00:03:06                 fraud_Keeling-Crist       misc_pos   \n",
       "\n",
       "                     TransactionAmount      first     last gender  \\\n",
       "TransactionTime                                                     \n",
       "2019-01-01 00:00:18               4.97   Jennifer    Banks      F   \n",
       "2019-01-01 00:00:44             107.23  Stephanie     Gill      F   \n",
       "2019-01-01 00:00:51             220.11     Edward  Sanchez      M   \n",
       "2019-01-01 00:01:16              45.00     Jeremy    White      M   \n",
       "2019-01-01 00:03:06              41.96      Tyler   Garcia      M   \n",
       "\n",
       "                                           street            city  ...  \\\n",
       "TransactionTime                                                    ...   \n",
       "2019-01-01 00:00:18                561 Perry Cove  Moravian Falls  ...   \n",
       "2019-01-01 00:00:44  43039 Riley Greens Suite 393          Orient  ...   \n",
       "2019-01-01 00:00:51      594 White Dale Suite 530      Malad City  ...   \n",
       "2019-01-01 00:01:16   9443 Cynthia Court Apt. 038         Boulder  ...   \n",
       "2019-01-01 00:03:06              408 Bradley Rest        Doe Hill  ...   \n",
       "\n",
       "                         long  city_pop                                job  \\\n",
       "TransactionTime                                                              \n",
       "2019-01-01 00:00:18  -81.1781      3495          Psychologist, counselling   \n",
       "2019-01-01 00:00:44 -118.2105       149  Special educational needs teacher   \n",
       "2019-01-01 00:00:51 -112.2620      4154        Nature conservation officer   \n",
       "2019-01-01 00:01:16 -112.1138      1939                    Patent attorney   \n",
       "2019-01-01 00:03:06  -79.4629        99     Dance movement psychotherapist   \n",
       "\n",
       "                     DateOfBirth                         trans_num  \\\n",
       "TransactionTime                                                      \n",
       "2019-01-01 00:00:18   1988-03-09  0b242abb623afc578575680df30655b9   \n",
       "2019-01-01 00:00:44   1978-06-21  1f76529f8574734946361c461b024d99   \n",
       "2019-01-01 00:00:51   1962-01-19  a1a22d70485983eac12b5b88dad1cf95   \n",
       "2019-01-01 00:01:16   1967-01-12  6b849c168bdad6f867558c3793159a81   \n",
       "2019-01-01 00:03:06   1986-03-28  a41d7549acf90789359a9aa5346dcb46   \n",
       "\n",
       "                      unix_time  merch_lat  merch_long  is_fraud  \\\n",
       "TransactionTime                                                    \n",
       "2019-01-01 00:00:18  1325376018  36.011293  -82.048315         0   \n",
       "2019-01-01 00:00:44  1325376044  49.159047 -118.186462         0   \n",
       "2019-01-01 00:00:51  1325376051  43.150704 -112.154481         0   \n",
       "2019-01-01 00:01:16  1325376076  47.034331 -112.561071         0   \n",
       "2019-01-01 00:03:06  1325376186  38.674999  -78.632459         0   \n",
       "\n",
       "                     TransactionID  \n",
       "TransactionTime                     \n",
       "2019-01-01 00:00:18              1  \n",
       "2019-01-01 00:00:44              2  \n",
       "2019-01-01 00:00:51              3  \n",
       "2019-01-01 00:01:16              4  \n",
       "2019-01-01 00:03:06              5  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10777762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Steps Completed File Loading, Describe, Date Conversions etc..   completed at Tue Nov  5 09:37:57 2024. Elapsed time: 0 minutes and 7.55 seconds\n",
      "\n",
      "--------------------------------------------------- ------------------   completed at Tue Nov  5 09:37:57 2024. Elapsed time: 0 minutes and 7.55 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Initial Steps Completed File Loading, Describe, Date Conversions etc..  \", start_time)\n",
    "log_time(\"--------------------------------------------------- ------------------  \", start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b32d0d",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca771ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log pre-process time at various steps\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2a6ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START - Feature Engineering .....   completed at Tue Nov  5 09:37:57 2024. Elapsed time: 0 minutes and 0.01 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"START - Feature Engineering .....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31252f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clip outliers if necessary\n",
    "df['TransactionAmount'] = df['TransactionAmount'].clip(upper=df['TransactionAmount'].quantile(0.99))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf9e3c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_2934/1075701793.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['TransactionAmount'].replace([np.inf, -np.inf], np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace inf values with NaN (in case they exist in the 'TransactionAmount' column)\n",
    "df['TransactionAmount'].replace([np.inf, -np.inf], np.nan, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b21308e",
   "metadata": {},
   "source": [
    "# next type of VIZ via transaction id vs transaction count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70594f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour from TransactionTime\n",
    "df['Hour'] = df.index.hour  # Since TransactionTime is already set as the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea729e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Risk Hours: [22, 23, 1, 0, 2, 3]\n",
      "                     Hour  HighRiskHour\n",
      "TransactionTime                        \n",
      "2019-01-01 00:00:18     0             1\n",
      "2019-01-01 00:00:44     0             1\n",
      "2019-01-01 00:00:51     0             1\n",
      "2019-01-01 00:01:16     0             1\n",
      "2019-01-01 00:03:06     0             1\n",
      "...                   ...           ...\n",
      "2020-06-21 12:12:08    12             0\n",
      "2020-06-21 12:12:19    12             0\n",
      "2020-06-21 12:12:32    12             0\n",
      "2020-06-21 12:13:36    12             0\n",
      "2020-06-21 12:13:37    12             0\n",
      "\n",
      "[1296675 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate fraud rate by hour\n",
    "fraud_rate_by_hour = df.groupby('Hour')['is_fraud'].mean()\n",
    "\n",
    "# Sort by fraud rate in descending order\n",
    "fraud_rate_by_hour = fraud_rate_by_hour.sort_values(ascending=False)\n",
    "\n",
    "# Define a threshold for high-risk hours (adjust as needed)\n",
    "threshold = fraud_rate_by_hour.mean()  # Mean fraud rate across all hours\n",
    "\n",
    "# Dynamically identify high-risk hours based on the threshold\n",
    "high_risk_hours = fraud_rate_by_hour[fraud_rate_by_hour > threshold].index.tolist()\n",
    "\n",
    "# Print high-risk hours for reference\n",
    "print(\"High-Risk Hours:\", high_risk_hours)\n",
    "\n",
    "# Create the HighRiskHour flag based on dynamically identified high-risk hours\n",
    "df['HighRiskHour'] = df['Hour'].apply(lambda x: 1 if x in high_risk_hours else 0)\n",
    "\n",
    "# Print a sample of the DataFrame to verify the new column\n",
    "print(df[['Hour', 'HighRiskHour']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75736fa",
   "metadata": {},
   "source": [
    "1. Time-Based Analysis:\n",
    "Already explored daily and hourly trends in transaction volumes, but now dive deeper into fraud patterns based on time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a61c0eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Weekday vs. Weekend: Is fraud more common on weekdays or weekends?\n",
    "df['DayOfWeek'] = df.index.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "fraud_by_day = df[df['is_fraud'] == 1]['DayOfWeek'].value_counts().sort_index()\n",
    "non_fraud_by_day = df[df['is_fraud'] == 0]['DayOfWeek'].value_counts().sort_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0d77fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the correct day order\n",
    "day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "\n",
    "\n",
    "df['DayName'] = df.index.day_name()\n",
    "# Convert the 'DayName' column to a categorical type with the correct order\n",
    "df['DayName'] = pd.Categorical(df['DayName'], categories=day_order, ordered=True)\n",
    "\n",
    "fraud_by_day = df[df['is_fraud'] == 1]['DayName'].value_counts().sort_index()\n",
    "non_fraud_by_day = df[df['is_fraud'] == 0]['DayName'].value_counts().sort_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3edc7f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of fraud on weekends: 32.55%\n",
      "Percentage of non-fraud on weekends: 34.84%\n"
     ]
    }
   ],
   "source": [
    "df['IsWeekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "weekend_fraud = df[df['is_fraud'] == 1]['IsWeekend'].mean()\n",
    "weekend_non_fraud = df[df['is_fraud'] == 0]['IsWeekend'].mean()\n",
    "\n",
    "print(f\"Percentage of fraud on weekends: {weekend_fraud * 100:.2f}%\")\n",
    "print(f\"Percentage of non-fraud on weekends: {weekend_non_fraud * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67a62065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part1 - TrxAmount, Hour, DayOfWeeek etc.. completed at Tue Nov  5 09:37:59 2024. Elapsed time: 0 minutes and 2.69 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part1 - TrxAmount, Hour, DayOfWeeek etc..\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b51d0f2e",
   "metadata": {},
   "source": [
    "# Calculate distance between cardholder and merchant\n",
    "df['distance'] = df.apply(lambda row: geodesic((row['lat'], row['long']), (row['merch_lat'], row['merch_long'])).km, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f04c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'v_1.2_DecisionTrees_OptFeat_Parallelized_GridSearch_Credit_Card_Fraud_Detection.ipynb', 'v_2.1_RandomForest_Balanced_SMOTE_Credit_Card_Fraud_Detection.ipynb', 'v_2.2_RandomForest_Balanced_SMOTE_GridSearch_Credit_Card_Fraud_Detection.ipynb', 'v_1.0.1_DecisionTrees_Parallelized_Credit_Card_Fraud_Detection.ipynb', 'bkp', 'v_0.0_LogisticRegression_Credit_Card_Fraud_Detection.ipynb', 'v_3.2_xgBoost_Credit_Card_Fraud_Detection.ipynb', '.gitkeep', 'v_3.1_xgBoost_SMOTE_Credit_Card_Fraud_Detection.ipynb', 'v_2.0.1_RandomForest_use_DASK_to_train_Model_Credit_Card_Fraud_Detection.ipynb', '__pycache__', 'v_0.1_LogisticRegression_Balanced_Credit_Card_Fraud_Detection.ipynb', 'v_1.1_DecisionTrees_Parallelized_GridSearch_Credit_Card_Fraud_Detection.ipynb', 'v_2.0.0_RandomForest_Credit_Card_Fraud_Detection.ipynb', '.ipynb_checkpoints', 'Project_Presentation_BigData.ipynb', 'Project_Presentation_ML.ipynb', 'v_1.0_DecisionTrees_Credit_Card_Fraud_Detection.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())  # List all files in the current directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a9514",
   "metadata": {},
   "source": [
    "# MULTIPROCESSING : distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40c0bbf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part2 - Distance Calculation with Multiprocessing (4 cores) completed at Tue Nov  5 09:39:12 2024. Elapsed time: 1 minutes and 12.41 seconds\n",
      "\n",
      "                         lat      long  merch_lat  merch_long    distance\n",
      "TransactionTime                                                          \n",
      "2019-01-01 00:00:18  36.0788  -81.1781  36.011293  -82.048315   78.773821\n",
      "2019-01-01 00:00:44  48.8878 -118.2105  49.159047 -118.186462   30.216618\n",
      "2019-01-01 00:00:51  42.1808 -112.2620  43.150704 -112.154481  108.102912\n",
      "2019-01-01 00:01:16  46.2306 -112.1138  47.034331 -112.561071   95.685115\n",
      "2019-01-01 00:03:06  38.4207  -79.4629  38.674999  -78.632459   77.702395\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from distance_calculation import calculate_distance_chunk\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Add the current working directory to the system path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Multiprocessing function to split the dataframe and apply the distance calculation\n",
    "def parallel_distance_calculation(df, num_partitions=None):\n",
    "    if num_partitions is None:\n",
    "        num_partitions = mp.cpu_count()  # Use all available CPU cores\n",
    "    \n",
    "    # Split the dataframe into chunks\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    \n",
    "    # Create a multiprocessing Pool\n",
    "    with mp.Pool(num_partitions) as pool:\n",
    "        # Apply the calculate_distance_chunk function to each chunk in parallel\n",
    "        result = pool.map(calculate_distance_chunk, df_split)\n",
    "    \n",
    "    # Concatenate the results back into a single dataframe\n",
    "    return pd.concat(result)\n",
    "\n",
    "# Main block to ensure multiprocessing works correctly\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Assuming df has the columns ['lat', 'long', 'merch_lat', 'merch_long']\n",
    "    \n",
    "    # Run with limited number of cores (e.g., 4 cores)\n",
    "    df = parallel_distance_calculation(df, num_partitions=4)  # Use 4 cores instead of all available cores\n",
    "\n",
    "    # Log the time taken for distance calculation with multiprocessing\n",
    "    log_time(\"Part2 - Distance Calculation with Multiprocessing (4 cores)\", start_time)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Check the first few rows to verify the result\n",
    "    print(df[['lat', 'long', 'merch_lat', 'merch_long', 'distance']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c72a4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/Desktop/coding/ZHAW_Project/ML_BigData_Repo_1/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())  # This will print the current working directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f985dcd",
   "metadata": {},
   "source": [
    "log_time(\"Part2 -  Distance Calculation\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50a155e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check unique values in the 'is_fraud' column\n",
    "df['is_fraud'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09d464f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud vs Non-Fraud by Merchant Category\n",
    "fraud_by_category = df[df['is_fraud'] == 1]['category'].value_counts().head(10)\n",
    "non_fraud_by_category = df[df['is_fraud'] == 0]['category'].value_counts().head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07ec224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Fraudulent Merchant Categories: ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport']\n"
     ]
    }
   ],
   "source": [
    "# Top 5 categories with the highest fraud counts\n",
    "top_fraud_merchant_categories = df[df['is_fraud'] == 1]['category'].value_counts().head(5).index.tolist()\n",
    "\n",
    "# Print top fraudulent categories\n",
    "print(\"Top Fraudulent Merchant Categories:\", top_fraud_merchant_categories)\n",
    "\n",
    "# Create HighRiskMerchantCategory flag\n",
    "df['HighRiskMerchantCategory'] = df['category'].apply(lambda x: 1 if x in top_fraud_merchant_categories else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7780b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HighRiskMerchantCategory\n",
      "0    763876\n",
      "1    532799\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the count of 1s and 0s in HighRiskMerchantCategory\n",
    "print(df['HighRiskMerchantCategory'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df499da3",
   "metadata": {},
   "source": [
    "# Potential Additional Features:\n",
    "Transaction Frequency:\n",
    "    Feature: How often a credit card has been used within a specific time frame (e.g., last hour or day).\n",
    "    Why: Fraudsters often make rapid successive transactions within short periods. You could create a rolling window to calculate transaction frequency.\n",
    "    How: You could calculate the number of transactions within the past X hours/days using a rolling window on the TransactionTime feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828d3a0e",
   "metadata": {},
   "source": [
    "#age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "221b8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure 'DateOfBirth' is in datetime format\n",
    "df['DateOfBirth'] = pd.to_datetime(df['DateOfBirth'], errors='coerce')  # Handle errors during conversion\n",
    "\n",
    "# Step 1: Calculate Age\n",
    "# Calculate age in years\n",
    "df['Age'] = (pd.Timestamp.now() - df['DateOfBirth']).dt.days // 365  # Age in years\n",
    "\n",
    "# Step 2: Create Age Groups\n",
    "# Define age bins and labels\n",
    "bins = [0, 18, 25, 35, 45, 55, 65, 100]  # Define your age bins, ensuring to cover all possible ages\n",
    "labels = ['0-18', '19-25', '26-35', '36-45', '46-55', '56-65', '66+']  # Corresponding labels\n",
    "\n",
    "# Create age group feature, include NaN values handling\n",
    "df['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "\n",
    "# Verify the new features without truncating DataFrame\n",
    "#print(df[['DateOfBirth', 'Age', 'AgeGroup']].head(10))  # Display the first 10 entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "296a6aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part3 - Merchant Categories & Age group completed at Tue Nov  5 09:39:13 2024. Elapsed time: 0 minutes and 1.33 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part3 - Merchant Categories & Age group\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc72de",
   "metadata": {},
   "source": [
    "# MULTIPROCESSING : count_transactions_within_last_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "620555a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part4 - TransactionFrequency Multiprocessing completed at Tue Nov  5 09:41:35 2024. Elapsed time: 2 minutes and 21.97 seconds\n",
      "\n",
      "                    TransactionFrequency\n",
      "TransactionTime                         \n",
      "2019-01-01 00:00:18                  NaN\n",
      "2019-01-01 00:00:44                  NaN\n",
      "2019-01-01 00:00:51                  NaN\n",
      "2019-01-01 00:01:16                  NaN\n",
      "2019-01-01 00:03:06                  NaN\n",
      "2019-01-01 00:04:08                  NaN\n",
      "2019-01-01 00:04:42                  NaN\n",
      "2019-01-01 00:05:08                  NaN\n",
      "2019-01-01 00:05:18                  NaN\n",
      "2019-01-01 00:06:01                  NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import time\n",
    "from transaction_frequency import process_chunk  # Import from the .py file\n",
    "\n",
    "# Multiprocessing function to parallelize the transaction counting\n",
    "def parallel_count_transactions(df, num_partitions=None):\n",
    "    if num_partitions is None:\n",
    "        num_partitions = mp.cpu_count()  # Use all available CPU cores\n",
    "    \n",
    "    # Ensure the index is a datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Split the dataframe into chunks based on the number of partitions (CPU cores)\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    \n",
    "    # Create a multiprocessing Pool\n",
    "    with mp.Pool(num_partitions) as pool:\n",
    "        # Apply the processing function to each chunk in parallel\n",
    "        result = pool.map(process_chunk, df_split)\n",
    "    \n",
    "    # Combine the results from each chunk into a single series, reset index for consistency\n",
    "    return pd.concat(result).reset_index(drop=True)\n",
    "\n",
    "# Assuming df has 'CreditCardNumber' as a column and transaction times are indexed\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Apply the parallel processing for transaction frequency counting\n",
    "    df['TransactionFrequency'] = parallel_count_transactions(df, num_partitions=4)  # Adjust num_partitions as needed\n",
    "\n",
    "    # Log the time taken for transaction frequency calculation with multiprocessing\n",
    "    log_time(\"Part4 - TransactionFrequency Multiprocessing\", start_time)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Check the first 10 rows\n",
    "    print(df[['TransactionFrequency']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3e571f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-01 00:00:18', '2019-01-01 00:00:44',\n",
      "               '2019-01-01 00:00:51', '2019-01-01 00:01:16',\n",
      "               '2019-01-01 00:03:06', '2019-01-01 00:04:08',\n",
      "               '2019-01-01 00:04:42', '2019-01-01 00:05:08',\n",
      "               '2019-01-01 00:05:18', '2019-01-01 00:06:01',\n",
      "               ...\n",
      "               '2020-06-21 12:08:42', '2020-06-21 12:09:22',\n",
      "               '2020-06-21 12:10:56', '2020-06-21 12:11:23',\n",
      "               '2020-06-21 12:11:36', '2020-06-21 12:12:08',\n",
      "               '2020-06-21 12:12:19', '2020-06-21 12:12:32',\n",
      "               '2020-06-21 12:13:36', '2020-06-21 12:13:37'],\n",
      "              dtype='datetime64[ns]', name='TransactionTime', length=1296675, freq=None)\n"
     ]
    }
   ],
   "source": [
    "df.index = pd.to_datetime(df.index)\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96249ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dadb160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_2934/1490275857.py:2: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  transaction_counts_hourly = df.resample('H').size()\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_2934/1490275857.py:6: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  transaction_counts = df.groupby('CreditCardNumber').resample('H').size().reset_index(name='TransactionCount')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CreditCardNumber     TransactionTime  TransactionCount\n",
      "0       60416207185 2019-01-01 12:00:00                 1\n",
      "1       60416207185 2019-01-01 13:00:00                 0\n",
      "2       60416207185 2019-01-01 14:00:00                 0\n",
      "3       60416207185 2019-01-01 15:00:00                 0\n",
      "4       60416207185 2019-01-01 16:00:00                 0\n",
      "5       60416207185 2019-01-01 17:00:00                 0\n",
      "6       60416207185 2019-01-01 18:00:00                 0\n",
      "7       60416207185 2019-01-01 19:00:00                 0\n",
      "8       60416207185 2019-01-01 20:00:00                 0\n",
      "9       60416207185 2019-01-01 21:00:00                 0\n"
     ]
    }
   ],
   "source": [
    "# Resample the data to count transactions every hour\n",
    "transaction_counts_hourly = df.resample('H').size()\n",
    "transaction_counts_daily = df.resample('D').size()\n",
    "\n",
    "# Combine with CreditCardNumber if necessary\n",
    "transaction_counts = df.groupby('CreditCardNumber').resample('H').size().reset_index(name='TransactionCount')\n",
    "print(transaction_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97c19713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CreditCardNumber  TotalTransactionCount\n",
      "0       60416207185                   1518\n",
      "1       60422928733                   1531\n",
      "2       60423098130                    510\n",
      "3       60427851591                    528\n",
      "4       60487002085                    496\n",
      "5       60490596305                   1010\n",
      "6       60495593109                    518\n",
      "7      501802953619                   1559\n",
      "8      501818133297                      8\n",
      "9      501828204849                    515\n"
     ]
    }
   ],
   "source": [
    "total_transactions = df.groupby('CreditCardNumber').size().reset_index(name='TotalTransactionCount')\n",
    "print(total_transactions.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aef71d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01    1960\n",
      "2019-01-02     606\n",
      "2019-01-03     701\n",
      "2019-01-04     952\n",
      "2019-01-05     879\n",
      "              ... \n",
      "2020-06-17    1327\n",
      "2020-06-18    1547\n",
      "2020-06-19    1966\n",
      "2020-06-20    1910\n",
      "2020-06-21    1142\n",
      "Length: 537, dtype: int64\n",
      "         Unnamed: 0  CreditCardNumber  TransactionAmount           zip  \\\n",
      "count  1.061083e+06      1.061083e+06       1.061083e+06  1.061083e+06   \n",
      "mean   6.541777e+05      4.168739e+17       6.463842e+01  4.881377e+04   \n",
      "min    1.000000e+00      6.041621e+10       1.000000e+00  1.257000e+03   \n",
      "25%    3.399115e+05      1.800429e+14       9.500000e+00  2.623700e+04   \n",
      "50%    6.543810e+05      3.521417e+15       4.635000e+01  4.817400e+04   \n",
      "75%    9.639775e+05      4.642255e+15       8.223000e+01  7.204200e+04   \n",
      "max    1.296674e+06      4.992346e+18       5.459926e+02  9.978300e+04   \n",
      "std    3.688933e+05      1.308438e+18       8.254229e+01  2.689496e+04   \n",
      "\n",
      "                lat          long      city_pop  \\\n",
      "count  1.061083e+06  1.061083e+06  1.061083e+06   \n",
      "mean   3.853009e+01 -9.022821e+01  8.979136e+04   \n",
      "min    2.002710e+01 -1.656723e+02  2.300000e+01   \n",
      "25%    3.459060e+01 -9.679800e+01  7.430000e+02   \n",
      "50%    3.935430e+01 -8.747690e+01  2.470000e+03   \n",
      "75%    4.194040e+01 -8.015800e+01  2.047800e+04   \n",
      "max    6.669330e+01 -6.795030e+01  2.906700e+06   \n",
      "std    5.078796e+00  1.374857e+01  3.041541e+05   \n",
      "\n",
      "                         DateOfBirth     unix_time     merch_lat  \\\n",
      "count                        1061083  1.061083e+06  1.061083e+06   \n",
      "mean   1974-01-07 01:53:43.193284640  1.349452e+09  3.853026e+01   \n",
      "min              1924-10-30 00:00:00  1.325376e+09  1.902779e+01   \n",
      "25%              1962-12-06 00:00:00  1.339269e+09  3.471879e+01   \n",
      "50%              1975-12-28 00:00:00  1.349484e+09  3.935917e+01   \n",
      "75%              1987-05-05 00:00:00  1.359010e+09  4.195454e+01   \n",
      "max              2005-01-29 00:00:00  1.371817e+09  6.751027e+01   \n",
      "std                              NaN  1.264150e+07  5.112969e+00   \n",
      "\n",
      "         merch_long      is_fraud  TransactionID          Hour  HighRiskHour  \\\n",
      "count  1.061083e+06  1.061083e+06   1.061083e+06  1.061083e+06  1.061083e+06   \n",
      "mean  -9.022814e+01  5.652715e-03   6.541787e+05  1.323944e+01  2.301988e-01   \n",
      "min   -1.666701e+02  0.000000e+00   2.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%   -9.689874e+01  0.000000e+00   3.399125e+05  8.000000e+00  0.000000e+00   \n",
      "50%   -8.744732e+01  0.000000e+00   6.543820e+05  1.400000e+01  0.000000e+00   \n",
      "75%   -8.024346e+01  0.000000e+00   9.639785e+05  1.900000e+01  0.000000e+00   \n",
      "max   -6.695090e+01  1.000000e+00   1.296675e+06  2.300000e+01  1.000000e+00   \n",
      "std    1.376070e+01  7.497178e-02   3.688933e+05  6.727723e+00  4.209602e-01   \n",
      "\n",
      "          DayOfWeek     IsWeekend      distance  HighRiskMerchantCategory  \\\n",
      "count  1.061083e+06  1.061083e+06  1.061083e+06              1.061083e+06   \n",
      "mean   3.092014e+00  3.698118e-01  7.611910e+01              3.897716e-01   \n",
      "min    0.000000e+00  0.000000e+00  2.227351e-02              0.000000e+00   \n",
      "25%    1.000000e+00  0.000000e+00  5.535721e+01              0.000000e+00   \n",
      "50%    3.000000e+00  0.000000e+00  7.827691e+01              0.000000e+00   \n",
      "75%    5.000000e+00  1.000000e+00  9.849232e+01              1.000000e+00   \n",
      "max    6.000000e+00  1.000000e+00  1.518682e+02              1.000000e+00   \n",
      "std    2.263057e+00  4.827538e-01  2.909755e+01              4.876986e-01   \n",
      "\n",
      "                Age  \n",
      "count  1.061083e+06  \n",
      "mean   5.036776e+01  \n",
      "min    1.900000e+01  \n",
      "25%    3.700000e+01  \n",
      "50%    4.800000e+01  \n",
      "75%    6.100000e+01  \n",
      "max    1.000000e+02  \n",
      "std    1.738241e+01  \n"
     ]
    }
   ],
   "source": [
    "# Calculate the time difference between consecutive transactions\n",
    "time_diff = df.index.to_series().diff().dt.total_seconds()\n",
    "# Flag rapid transactions (within 5 minutes)\n",
    "df['RapidTransactionFlag'] = time_diff < 60  # For a 1-minute threshold\n",
    "\n",
    "# Create a temporary DataFrame for rapid transactions\n",
    "rapid_transactions = df[df['RapidTransactionFlag']]\n",
    "\n",
    "# Group by date and count the number of rapid transactions\n",
    "rapid_transaction_counts = rapid_transactions.groupby(rapid_transactions.index.date).size()\n",
    "print(rapid_transaction_counts)\n",
    "\n",
    "# Get a summary of the rapid transactions\n",
    "rapid_transactions_summary = rapid_transactions.describe()\n",
    "print(rapid_transactions_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64e05375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'CreditCardNumber', 'merchant', 'category',\n",
      "       'TransactionAmount', 'first', 'last', 'gender', 'street', 'city',\n",
      "       'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID', 'Hour', 'HighRiskHour', 'DayOfWeek', 'DayName',\n",
      "       'IsWeekend', 'distance', 'HighRiskMerchantCategory', 'Age', 'AgeGroup',\n",
      "       'TransactionFrequency', 'RapidTransactionFlag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bca5e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part5 - RapidTransactionFlag completed at Tue Nov  5 09:41:45 2024. Elapsed time: 0 minutes and 10.24 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part5 - RapidTransactionFlag\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b200c",
   "metadata": {},
   "source": [
    "Transaction Amount Features:\n",
    "Log Transaction Amount: Normalize the TransactionAmount by taking its logarithm to reduce skewness.\n",
    "Transaction Amount Flags: Create binary flags for high-value transactions (e.g., if TransactionAmount exceeds a certain threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da75a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     TransactionAmount  LogTransactionAmount  \\\n",
      "TransactionTime                                                \n",
      "2019-01-01 00:00:18               4.97              1.786747   \n",
      "2019-01-01 00:00:44             107.23              4.684259   \n",
      "2019-01-01 00:00:51             220.11              5.398660   \n",
      "2019-01-01 00:01:16              45.00              3.828641   \n",
      "2019-01-01 00:03:06              41.96              3.760269   \n",
      "2019-01-01 00:04:08              94.63              4.560487   \n",
      "2019-01-01 00:04:42              44.54              3.818591   \n",
      "2019-01-01 00:05:08              71.65              4.285653   \n",
      "2019-01-01 00:05:18               4.27              1.662030   \n",
      "2019-01-01 00:06:01             198.39              5.295263   \n",
      "\n",
      "                     HighValueTransactionFlag  \n",
      "TransactionTime                                \n",
      "2019-01-01 00:00:18                     False  \n",
      "2019-01-01 00:00:44                      True  \n",
      "2019-01-01 00:00:51                      True  \n",
      "2019-01-01 00:01:16                     False  \n",
      "2019-01-01 00:03:06                     False  \n",
      "2019-01-01 00:04:08                     False  \n",
      "2019-01-01 00:04:42                     False  \n",
      "2019-01-01 00:05:08                     False  \n",
      "2019-01-01 00:05:18                     False  \n",
      "2019-01-01 00:06:01                      True  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample DataFrame creation\n",
    "# Assume 'df' is your DataFrame and has a 'TransactionAmount' column\n",
    "# df = pd.read_csv('your_data.csv')  # Load your actual data\n",
    "\n",
    "# Step 1: Log Transaction Amount\n",
    "# Calculate the log of TransactionAmount\n",
    "df['LogTransactionAmount'] = np.log1p(df['TransactionAmount'])  # Use log1p for stability with 0 values\n",
    "\n",
    "# Step 2: Create Transaction Amount Flags\n",
    "# Define a threshold for high-value transactions\n",
    "threshold = 100  # Adjust the threshold based on your data context\n",
    "\n",
    "# Create a flag for high-value transactions\n",
    "df['HighValueTransactionFlag'] = df['TransactionAmount'] > threshold\n",
    "\n",
    "# Verify the new features\n",
    "print(df[['TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag']].head(10))  # Display the first 10 entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217bc43",
   "metadata": {},
   "source": [
    "Behavioral Features:\n",
    "Count of Transactions in Last X Days: Count how many transactions have occurred in the last 7, 14, or 30 days.\n",
    "Average Transaction Amount in Last X Days: Calculate the average transaction amount over the same periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25aeefda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     TransactionCountLast7Days  TransactionCountLast14Days  \\\n",
      "TransactionTime                                                              \n",
      "2019-01-01 12:47:15                        1.0                         1.0   \n",
      "2019-01-02 08:44:57                        2.0                         2.0   \n",
      "2019-01-02 08:47:36                        3.0                         3.0   \n",
      "2019-01-02 12:38:14                        4.0                         4.0   \n",
      "2019-01-02 13:10:46                        5.0                         5.0   \n",
      "2019-01-03 13:56:35                        6.0                         6.0   \n",
      "2019-01-03 17:05:10                        7.0                         7.0   \n",
      "2019-01-04 13:59:55                        8.0                         8.0   \n",
      "2019-01-04 21:17:22                        9.0                         9.0   \n",
      "2019-01-05 00:42:24                       10.0                        10.0   \n",
      "\n",
      "                     TransactionCountLast30Days  \\\n",
      "TransactionTime                                   \n",
      "2019-01-01 12:47:15                         1.0   \n",
      "2019-01-02 08:44:57                         2.0   \n",
      "2019-01-02 08:47:36                         3.0   \n",
      "2019-01-02 12:38:14                         4.0   \n",
      "2019-01-02 13:10:46                         5.0   \n",
      "2019-01-03 13:56:35                         6.0   \n",
      "2019-01-03 17:05:10                         7.0   \n",
      "2019-01-04 13:59:55                         8.0   \n",
      "2019-01-04 21:17:22                         9.0   \n",
      "2019-01-05 00:42:24                        10.0   \n",
      "\n",
      "                     AverageTransactionAmountLast7Days  \\\n",
      "TransactionTime                                          \n",
      "2019-01-01 12:47:15                           7.270000   \n",
      "2019-01-02 08:44:57                          30.105000   \n",
      "2019-01-02 08:47:36                          47.430000   \n",
      "2019-01-02 12:38:14                          44.270000   \n",
      "2019-01-02 13:10:46                          40.852000   \n",
      "2019-01-03 13:56:35                          35.188333   \n",
      "2019-01-03 17:05:10                          31.365714   \n",
      "2019-01-04 13:59:55                          42.083750   \n",
      "2019-01-04 21:17:22                          40.378889   \n",
      "2019-01-05 00:42:24                          46.861000   \n",
      "\n",
      "                     AverageTransactionAmountLast14Days  \\\n",
      "TransactionTime                                           \n",
      "2019-01-01 12:47:15                            7.270000   \n",
      "2019-01-02 08:44:57                           30.105000   \n",
      "2019-01-02 08:47:36                           47.430000   \n",
      "2019-01-02 12:38:14                           44.270000   \n",
      "2019-01-02 13:10:46                           40.852000   \n",
      "2019-01-03 13:56:35                           35.188333   \n",
      "2019-01-03 17:05:10                           31.365714   \n",
      "2019-01-04 13:59:55                           42.083750   \n",
      "2019-01-04 21:17:22                           40.378889   \n",
      "2019-01-05 00:42:24                           46.861000   \n",
      "\n",
      "                     AverageTransactionAmountLast30Days  \n",
      "TransactionTime                                          \n",
      "2019-01-01 12:47:15                            7.270000  \n",
      "2019-01-02 08:44:57                           30.105000  \n",
      "2019-01-02 08:47:36                           47.430000  \n",
      "2019-01-02 12:38:14                           44.270000  \n",
      "2019-01-02 13:10:46                           40.852000  \n",
      "2019-01-03 13:56:35                           35.188333  \n",
      "2019-01-03 17:05:10                           31.365714  \n",
      "2019-01-04 13:59:55                           42.083750  \n",
      "2019-01-04 21:17:22                           40.378889  \n",
      "2019-01-05 00:42:24                           46.861000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'TransactionTime' is already set as the index and in datetime format\n",
    "\n",
    "# Step 1: Count of Transactions in Last X Days\n",
    "for days in [7, 14, 30]:\n",
    "    # Sort data by CreditCardNumber and TransactionTime to ensure rolling works properly\n",
    "    df = df.sort_values(by=['CreditCardNumber', 'TransactionTime'])\n",
    "    \n",
    "    # Apply rolling and count the number of transactions for each card\n",
    "    df[f'TransactionCountLast{days}Days'] = (\n",
    "        df.groupby('CreditCardNumber')['CreditCardNumber']\n",
    "        .rolling(f'{days}D')\n",
    "        .count()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Step 2: Average Transaction Amount in Last X Days\n",
    "for days in [7, 14, 30]:\n",
    "    # Sort data by CreditCardNumber and TransactionTime to ensure rolling works properly\n",
    "    df = df.sort_values(by=['CreditCardNumber', 'TransactionTime'])\n",
    "    \n",
    "    # Calculate the average transaction amount for each credit card in the last X days\n",
    "    df[f'AverageTransactionAmountLast{days}Days'] = (\n",
    "        df.groupby('CreditCardNumber')['TransactionAmount']\n",
    "        .rolling(f'{days}D')\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Verify the new features\n",
    "print(df[['TransactionCountLast7Days', 'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
    "           'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast14Days', 'AverageTransactionAmountLast30Days']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ebcd77cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'CreditCardNumber', 'merchant', 'category',\n",
      "       'TransactionAmount', 'first', 'last', 'gender', 'street', 'city',\n",
      "       'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID', 'Hour', 'HighRiskHour', 'DayOfWeek', 'DayName',\n",
      "       'IsWeekend', 'distance', 'HighRiskMerchantCategory', 'Age', 'AgeGroup',\n",
      "       'TransactionFrequency', 'RapidTransactionFlag', 'LogTransactionAmount',\n",
      "       'HighValueTransactionFlag', 'TransactionCountLast7Days',\n",
      "       'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
      "       'AverageTransactionAmountLast7Days',\n",
      "       'AverageTransactionAmountLast14Days',\n",
      "       'AverageTransactionAmountLast30Days'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)  # Display all columns in the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc639c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part6 - TransactionCountLast_X_Days & AverageTrxAmountLast_X_Days completed at Tue Nov  5 09:41:58 2024. Elapsed time: 0 minutes and 12.39 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part6 - TransactionCountLast_X_Days & AverageTrxAmountLast_X_Days\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a24a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b4aaef3",
   "metadata": {},
   "source": [
    "# Graph Construction with NetworkX:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18793de2",
   "metadata": {},
   "source": [
    "Highlight Fraudulent Nodes: Overlay of fraudulent and non-fraudulent credit cards on this degree distribution to see if thereâ€™s a difference in their degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e26bd1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique credit card nodes: 983\n",
      "Number of unique merchant nodes: 693\n",
      "Number of credit card nodes with degrees: 983\n",
      "Number of merchant nodes with degrees: 693\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges between credit cards and merchants, including transaction amount as an edge attribute\n",
    "for idx, row in df.iterrows():\n",
    "    credit_card = str(row['CreditCardNumber'])\n",
    "    merchant = str(row['merchant'])\n",
    "    transaction_amount = row['TransactionAmount']  # Ensure TransactionAmount exists in your dataframe\n",
    "    \n",
    "    # Add an edge with the transaction amount as an attribute\n",
    "    G.add_edge(credit_card, merchant, transaction_amount=transaction_amount)\n",
    "\n",
    "\n",
    "# Calculate degrees for all nodes in the graph\n",
    "degrees = dict(G.degree())\n",
    "\n",
    "# Filter degrees for credit cards and merchants\n",
    "credit_card_nodes = df['CreditCardNumber'].astype(str).unique()\n",
    "merchant_nodes = df['merchant'].astype(str).unique()\n",
    "\n",
    "credit_card_degrees = {node: degrees[node] for node in credit_card_nodes if node in degrees}\n",
    "merchant_degrees = {node: degrees[node] for node in merchant_nodes if node in degrees}\n",
    "\n",
    "# Debugging: Print counts to ensure correctness\n",
    "print(f\"Number of unique credit card nodes: {len(credit_card_nodes)}\")\n",
    "print(f\"Number of unique merchant nodes: {len(merchant_nodes)}\")\n",
    "print(f\"Number of credit card nodes with degrees: {len(credit_card_degrees)}\")\n",
    "print(f\"Number of merchant nodes with degrees: {len(merchant_degrees)}\")\n",
    "\n",
    "# Create a new DataFrame for easier plotting\n",
    "degree_df = pd.DataFrame({\n",
    "    'CreditCardDegree': pd.Series(credit_card_degrees),\n",
    "    'MerchantDegree': pd.Series(merchant_degrees)\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a55584fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add degree information back to the original DataFrame\n",
    "df['degree'] = df['CreditCardNumber'].astype(str).map(credit_card_degrees)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a8f6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check edges and their attributes\n",
    "#for edge in G.edges(data=True):\n",
    "#    print(edge)\n",
    "\n",
    "#do NOT print this, huge list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9c525e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CreditCardNumber'] = df['CreditCardNumber'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5115524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_mapping = df.set_index('CreditCardNumber')['is_fraud'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9e261bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part7 - NetworkX Start Step completed at Tue Nov  5 09:43:17 2024. Elapsed time: 1 minutes and 19.41 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part7 - NetworkX Start Step\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa04d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e509da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(fraud_mapping.head(5))\n",
    "#only testing purposes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "167571cb",
   "metadata": {},
   "source": [
    "betweenness_centrality"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0093ce9f",
   "metadata": {},
   "source": [
    "# Calculate betweenness centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac75df",
   "metadata": {},
   "source": [
    "# MULTIPROCESSING : betweenness_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50d69fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part8 - Betweenness Centrality Calculation with Multiprocessing completed at Tue Nov  5 09:45:59 2024. Elapsed time: 2 minutes and 41.34 seconds\n",
      "\n",
      "[('60416207185', 3.903743365141523e-05), ('fraud_Jones, Sawayn and Romaguera', 0.00016718121427278148), ('fraud_Berge LLC', 0.0003731639453736409), ('fraud_Luettgen PLC', 0.0002880918347168189), ('fraud_Daugherty LLC', 0.00026296051041424566), ('fraud_Beier and Sons', 0.00025675424900787366), ('fraud_Stamm-Witting', 0.00021600459190031658), ('fraud_Conroy-Emard', 0.00021409812196013464), ('fraud_Pollich LLC', 0.00029566515903354184), ('fraud_Monahan-Morar', 0.0002101294467358623)]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import time\n",
    "from networkx_graph_betweeness_centrality import parallel_betweenness_centrality\n",
    "\n",
    "# Assuming G is your graph\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate betweenness centrality using parallel processing\n",
    "    betweenness_centrality = parallel_betweenness_centrality(G, num_partitions=4)  \n",
    "\n",
    "    # Log the time taken for betweenness centrality calculation with multiprocessing\n",
    "    log_time(\"Part8 - Betweenness Centrality Calculation with Multiprocessing\", start_time)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Check a few centrality values\n",
    "    print(list(betweenness_centrality.items())[:10])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e7b4462",
   "metadata": {},
   "source": [
    "import os\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "85bd6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['betweenness_centrality'] = df['CreditCardNumber'].map(betweenness_centrality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb8bf772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1.296675e+06\n",
      "mean     4.163569e-05\n",
      "std      1.178525e-05\n",
      "min      2.728628e-09\n",
      "25%      3.630433e-05\n",
      "50%      4.399435e-05\n",
      "75%      5.147284e-05\n",
      "max      5.736748e-05\n",
      "Name: betweenness_centrality, dtype: float64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df['betweenness_centrality'].describe())\n",
    "print(df['betweenness_centrality'].isna().sum())  # Check for missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5e4b947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60416207185: 3.903743365141523e-05\n",
      "fraud_Kutch-Ferry: 0.00026288184139774875\n"
     ]
    }
   ],
   "source": [
    "# Check betweenness centrality for specific credit card numbers\n",
    "sample_nodes = ['60416207185', 'fraud_Kutch-Ferry']  # Replace with actual nodes\n",
    "for node in sample_nodes:\n",
    "    print(f\"{node}: {betweenness_centrality.get(node)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439124c6",
   "metadata": {},
   "source": [
    "1. Investigate Nodes with High Betweenness Centrality:\n",
    "\n",
    "Now that youâ€™ve visualized nodes with high betweenness centrality, you can:\n",
    "\n",
    "    Examine if fraudulent nodes tend to have high betweenness centrality. This might indicate that these nodes are acting as \"connectors\" between different parts of the network, which could be a sign of suspicious behavior.\n",
    "    Compare centrality between fraud and non-fraud nodes to see if there's a pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13fdd8e",
   "metadata": {},
   "source": [
    "2. Visualize Communities in the Network:\n",
    "\n",
    "You could apply community detection to uncover fraud rings or clusters of merchants targeted by fraudsters. The Louvain algorithm is great for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b5f148a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community.community_louvain as community_louvain\n",
    "\n",
    "\n",
    "# Apply Louvain method for community detection\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee872fb",
   "metadata": {},
   "source": [
    "Fraud Node Highlighting:\n",
    "\n",
    "    Fraudulent nodes (from df['is_fraud'] == 1) are colored red to make them stand out. The rest of the nodes are still colored based on their communities.\n",
    "    This should help you easily spot any fraudulent nodes in the network.\n",
    "\n",
    "Top 10 Most Central Nodes:\n",
    "\n",
    "    We calculate betweenness centrality and extract the top 10 most central nodes.\n",
    "    These nodes are visualized with their connections, which should help declutter the graph and focus on the key players in the transaction network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8fd26394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply Louvain method for community detection\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "# Create positions for nodes using a spring layout\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Add the community information to the DataFrame\n",
    "df['community'] = df['CreditCardNumber'].map(partition)\n",
    "\n",
    "# Highlight fraud nodes separately\n",
    "fraud_nodes = df[df['is_fraud'] == 1]['CreditCardNumber'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95613930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the community information to the DataFrame\n",
    "df['community'] = df['CreditCardNumber'].map(partition)\n",
    "\n",
    "# Calculate the percentage of fraud in each community\n",
    "community_fraud = df.groupby('community')['is_fraud'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d9407a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community\n",
      "0    0.003945\n",
      "1    0.010391\n",
      "2    0.006325\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print fraud rate per community\n",
    "print(community_fraud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "17572504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community\n",
      "0    668641\n",
      "1    220292\n",
      "2    407742\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "community_size = df.groupby('community').size()\n",
    "print(community_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "43cca311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fraud rates and community sizes into a single DataFrame\n",
    "fraud_vs_size = pd.concat([community_fraud, df.groupby('community').size()], axis=1)\n",
    "fraud_vs_size.columns = ['FraudRate', 'CommunitySize']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "accbc68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community\n",
      "1    0.010391\n",
      "2    0.006325\n",
      "0    0.003945\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "top_fraud_communities = community_fraud.sort_values(ascending=False).head(5)\n",
    "print(top_fraud_communities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4a875aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the community labels of the top fraud communities\n",
    "top_community_labels = top_fraud_communities.index.tolist()\n",
    "\n",
    "# Filter the DataFrame for only the top fraud communities\n",
    "top_communities_df = df[df['community'].isin(top_community_labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88b0bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merchant\n",
      "fraud_Kozey-Boehm                       0.025723\n",
      "fraud_Herman, Treutel and Dickens       0.025385\n",
      "fraud_Kerluke-Abshire                   0.022307\n",
      "fraud_Brown PLC                         0.022109\n",
      "fraud_Goyette Inc                       0.021616\n",
      "fraud_Terry-Huel                        0.021543\n",
      "fraud_Jast Ltd                          0.021505\n",
      "fraud_Schmeler, Bashirian and Price     0.020833\n",
      "fraud_Boyer-Reichert                    0.019916\n",
      "fraud_Langworth, Boehm and Gulgowski    0.019807\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Show Only the Top Merchants by Fraud Rate:\n",
    "# Instead of displaying all merchants, you can filter the plot to show only the top 10 or 20 merchants with the highest fraud rates.\n",
    "\n",
    "# Calculate fraud rate by merchant in the top fraud communities\n",
    "merchant_fraud_rate = top_communities_df.groupby('merchant')['is_fraud'].mean()\n",
    "\n",
    "# Sort merchants by fraud rate in descending order\n",
    "top_merchants = merchant_fraud_rate.sort_values(ascending=False).head(10)\n",
    "\n",
    "# Print top 10 merchants with highest fraud rate\n",
    "print(top_merchants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bbd1dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'category' is a column representing merchant categories\n",
    "merchantcategory_fraud = top_communities_df.groupby('category')['is_fraud'].mean()\n",
    "\n",
    "# Sort the fraud rate by merchant category in descending order\n",
    "merchantcategory_fraud_sorted = merchantcategory_fraud.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2e7c000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part9 - Community & Top Merchants completed at Tue Nov  5 09:47:04 2024. Elapsed time: 1 minutes and 4.93 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "log_time(\"Part9 - Community & Top Merchants\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "154aa31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Density: 0.34130445623909095\n"
     ]
    }
   ],
   "source": [
    "# Check the density of the graph (a measure of sparsity)\n",
    "density = nx.density(G)\n",
    "print(f\"Graph Density: {density}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5e360c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Degree of Nodes: 571.6849642004773\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average degree\n",
    "degree_sequence = [degree for node, degree in G.degree()]\n",
    "average_degree = sum(degree_sequence) / len(degree_sequence)\n",
    "print(f\"Average Degree of Nodes: {average_degree}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5fa3b21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part10 - Density completed at Tue Nov  5 09:47:04 2024. Elapsed time: 0 minutes and 0.02 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part10 - Density\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "452804a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'CreditCardNumber', 'merchant', 'category',\n",
      "       'TransactionAmount', 'first', 'last', 'gender', 'street', 'city',\n",
      "       'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID', 'Hour', 'HighRiskHour', 'DayOfWeek', 'DayName',\n",
      "       'IsWeekend', 'distance', 'HighRiskMerchantCategory', 'Age', 'AgeGroup',\n",
      "       'TransactionFrequency', 'RapidTransactionFlag', 'LogTransactionAmount',\n",
      "       'HighValueTransactionFlag', 'TransactionCountLast7Days',\n",
      "       'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
      "       'AverageTransactionAmountLast7Days',\n",
      "       'AverageTransactionAmountLast14Days',\n",
      "       'AverageTransactionAmountLast30Days', 'degree',\n",
      "       'betweenness_centrality', 'community'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d5be3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag',\n",
    "    'TransactionCountLast7Days', 'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
    "    'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast14Days', 'AverageTransactionAmountLast30Days',\n",
    "    'Hour', 'HighRiskHour', 'DayOfWeek', 'IsWeekend', 'TransactionFrequency', 'RapidTransactionFlag',\n",
    "    'lat', 'long', 'merch_lat', 'merch_long', 'distance', 'city_pop',\n",
    "    'Age', 'AgeGroup', 'gender', 'state', 'city',\n",
    "    'degree', 'betweenness_centrality', 'community'\n",
    "]\n",
    "\n",
    "df_selected_features = df[selected_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f21f1e",
   "metadata": {},
   "source": [
    "# Page rank as new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6fda32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PageRank for each node in the graph\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "# Map the PageRank values to the 'CreditCardNumber' in the DataFrame\n",
    "df['pagerank'] = df['CreditCardNumber'].map(pagerank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "53596e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in the pagerank column\n",
    "print(df['pagerank'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f5e2620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1.296675e+06\n",
      "mean     5.971670e-04\n",
      "std      7.856303e-05\n",
      "min      9.478567e-05\n",
      "25%      5.812104e-04\n",
      "50%      6.195727e-04\n",
      "75%      6.566711e-04\n",
      "max      6.834763e-04\n",
      "Name: pagerank, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check descriptive statistics of pagerank values\n",
    "print(df['pagerank'].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a525047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with zero PageRank: 0\n"
     ]
    }
   ],
   "source": [
    "# Check how many nodes have a PageRank of zero\n",
    "zero_pagerank_count = (df['pagerank'] == 0).sum()\n",
    "print(f\"Number of nodes with zero PageRank: {zero_pagerank_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c97f80b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PageRank for Fraud: 0.000505843602201983\n",
      "Average PageRank for Non-Fraud: 0.0005976986758972222\n"
     ]
    }
   ],
   "source": [
    "# Compare the average PageRank for fraud and non-fraud transactions\n",
    "fraud_avg_pagerank = df[df['is_fraud'] == 1]['pagerank'].mean()\n",
    "non_fraud_avg_pagerank = df[df['is_fraud'] == 0]['pagerank'].mean()\n",
    "\n",
    "print(f\"Average PageRank for Fraud: {fraud_avg_pagerank}\")\n",
    "print(f\"Average PageRank for Non-Fraud: {non_fraud_avg_pagerank}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "47121274",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features.append('pagerank')\n",
    "df_selected_features = df[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "439c0ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag',\n",
      "       'TransactionCountLast7Days', 'TransactionCountLast14Days',\n",
      "       'TransactionCountLast30Days', 'AverageTransactionAmountLast7Days',\n",
      "       'AverageTransactionAmountLast14Days',\n",
      "       'AverageTransactionAmountLast30Days', 'Hour', 'HighRiskHour',\n",
      "       'DayOfWeek', 'IsWeekend', 'TransactionFrequency',\n",
      "       'RapidTransactionFlag', 'lat', 'long', 'merch_lat', 'merch_long',\n",
      "       'distance', 'city_pop', 'Age', 'AgeGroup', 'gender', 'state', 'city',\n",
      "       'degree', 'betweenness_centrality', 'community', 'pagerank'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_selected_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ae9a67f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1296675, 46)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a89b08a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part11 - PageRank completed at Tue Nov  5 09:47:06 2024. Elapsed time: 0 minutes and 2.28 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Part11 - PageRank\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e17cecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "56237852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END - Feature Engineering .....   completed at Tue Nov  5 09:47:06 2024. Elapsed time: 0 minutes and 0.01 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"END - Feature Engineering .....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "163f30c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTrees_Parallelized_GridSearch_Balanced_by_Top10Features_Train START Model ....   completed at Tue Nov  5 09:47:06 2024. Elapsed time: 0 minutes and 0.00 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(f\"{model_specs}_{dataset_type} START Model ....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aa0b4689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'decisiontree_gridsearch_modelutils' from '/Users/sadhvichandragiri/Desktop/coding/ZHAW_Project/ML_BigData_Repo_1/notebooks/../references/decisiontree_gridsearch_modelutils.py'>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import decisiontree_gridsearch_modelutils\n",
    "importlib.reload(decisiontree_gridsearch_modelutils)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "460bc85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTrees_Parallelized_GridSearch_Balanced_by_Top10Features\n"
     ]
    }
   ],
   "source": [
    "print(model_specs)\n",
    "#decisiontrees_parallelized_gridsearch_Train.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c0892344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, precision_recall_curve, auc\n",
    "from decisiontree_gridsearch_modelutils import parallel_one_hot_encode, preprocessdata_parallel_onehot_impute\n",
    "from decisiontree_gridsearch_modelutils import perform_grid_search_balanced, perform_grid_search, generate_classification_report_and_roc, save_decision_tree_viz\n",
    "\n",
    "sample_fraction = 1.0  # Fraction to sample for memory efficiency\n",
    "\n",
    "\n",
    "# Parameters\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20], \n",
    "    'min_samples_split': [20, 50], \n",
    "    'min_samples_leaf': [2, 5], \n",
    "    'criterion': ['gini']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Train Model with Grid Search\n",
    "def train_model(X, y, param_grid):\n",
    "    #calling  perform_grid_search from decisiontree_gridsearch_modelutils.py\n",
    "    # best_model = perform_grid_search(X, y, param_grid)\n",
    "    best_model = perform_grid_search_balanced(X, y, param_grid) \n",
    "    return best_model\n",
    "\n",
    "def save_trained_model(model_name):\n",
    "    output_dir_model = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/models'\n",
    "    # Define model filename\n",
    "    model_outputfilename = f\"{model_specs.replace(' ', '_').replace(',', '').lower()}.pkl\"\n",
    "\n",
    "    # Assuming best_rf_model is your final or best model\n",
    "    try:\n",
    "        # Save the model\n",
    "        with open(os.path.join(output_dir_model, model_outputfilename), 'wb') as model_file:\n",
    "            pickle.dump(model_name, model_file)\n",
    "\n",
    "        print(f\"Model saved to {os.path.join(output_dir_model, model_outputfilename)}\")\n",
    "\n",
    "    except NameError:\n",
    "        print(f\"Error: {model_specs} is not defined. Please ensure the model is assigned before saving.\")\n",
    "\n",
    "# Load the trained model\n",
    "def load_trained_model(model_specs, dataset_type, output_dir_model='/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/models'):\n",
    "    \"\"\"\n",
    "    Loads a trained model from the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        model_specs (str): Specifications or name for the model.\n",
    "        dataset_type (str): Indicates if itâ€™s for 'Train' or 'Test'.\n",
    "        output_dir_model (str): Directory where the model is saved.\n",
    "        \n",
    "    Returns:\n",
    "        model: The loaded model.\n",
    "    \"\"\"\n",
    "    model_outputfilename = f\"{model_specs.replace(' ', '_').replace(',', '').lower()}.pkl\"\n",
    "    model_path = os.path.join(output_dir_model, model_outputfilename)\n",
    "\n",
    "    # Load the model if it exists\n",
    "    if os.path.exists(model_path):\n",
    "        with open(model_path, 'rb') as model_file:\n",
    "            model = pickle.load(model_file)\n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "        return model\n",
    "    else:\n",
    "        print(f\"Model file not found at {model_path}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Step 4: Main Execution\n",
    "def main(df, features_by_importance, param_grid, model_specs, reports_output_dir, use_test_data=False, sample_fraction=1.0):\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X, y, imp_feature_names = preprocessdata_parallel_onehot_impute(df, features_by_importance)\n",
    "    dataset_type = 'Test' if use_test_data else 'Train'\n",
    "    \n",
    "    if use_test_data:\n",
    "        # Load pre-trained model for test predictions\n",
    "        loaded_model = load_trained_model(model_specs, dataset_type)\n",
    "        if loaded_model:\n",
    "            # Generate report and ROC for loaded model on test data\n",
    "            generate_classification_report_and_roc(loaded_model, X, y, dataset_type, model_specs, reports_output_dir)\n",
    "            return None, imp_feature_names\n",
    "        else:\n",
    "            print(\"No pre-trained model found.\")\n",
    "            return None, imp_feature_names\n",
    "    else:\n",
    "        # Perform training with grid search on train data\n",
    "        best_model = train_model(X, y, param_grid)\n",
    "        # Generate report and ROC for best model\n",
    "        generate_classification_report_and_roc(best_model, X, y, dataset_type, model_specs, reports_output_dir)\n",
    "        \n",
    "        # Feature importances\n",
    "        feature_importances = best_model.feature_importances_\n",
    "        importance_df = pd.DataFrame({'Feature': imp_feature_names, 'Importance': feature_importances})\n",
    "        print(importance_df.sort_values(by='Importance', ascending=False))\n",
    "\n",
    "        # Save the trained model\n",
    "        save_trained_model(best_model)\n",
    "\n",
    "        return best_model, imp_feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "15fa200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/DecisionTrees\n"
     ]
    }
   ],
   "source": [
    "print(reports_output_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9f23726",
   "metadata": {},
   "source": [
    "features_by_importance = [\n",
    "    'AverageTransactionAmountLast7Days','TransactionAmount', 'HighRiskHour', \n",
    "    'LogTransactionAmount','betweenness_centrality','Hour', \n",
    "    'AverageTransactionAmountLast30Days', 'TransactionCountLast7Days',\n",
    "    'city_pop','AverageTransactionAmountLast14Days'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cc422657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=50; total time=   3.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=   3.2s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=50; total time=   3.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=50; total time=   3.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=50; total time=   3.3s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=50; total time=   3.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=50; total time=   3.7s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=   3.4s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=   3.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=   3.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=50; total time=   3.6s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=   3.6s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=   3.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=50; total time=   3.2s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=20; total time=   3.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=   3.5s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=50; total time=   3.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=5, min_samples_split=20; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=50; total time=   4.0s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=50; total time=   3.2s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=20; total time=   3.7s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=2, min_samples_split=20; total time=   3.5s\n",
      "[CV] END criterion=gini, max_depth=20, min_samples_leaf=5, min_samples_split=50; total time=   3.7s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     _, imp_feature_names \u001b[38;5;241m=\u001b[39m main(df, features_by_importance, param_grid, model_specs, reports_output_dir, use_test_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     best_model, imp_feature_names \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_by_importance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_specs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreports_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_test_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[95], line 92\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(df, features_by_importance, param_grid, model_specs, reports_output_dir, use_test_data, sample_fraction)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, imp_feature_names\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Perform training with grid search on train data\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# Generate report and ROC for best model\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     generate_classification_report_and_roc(best_model, X, y, dataset_type, model_specs, reports_output_dir)\n",
      "Cell \u001b[0;32mIn[95], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(X, y, param_grid)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(X, y, param_grid):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m#calling  perform_grid_search from decisiontree_gridsearch_modelutils.py\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# best_model = perform_grid_search(X, y, param_grid)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     best_model \u001b[38;5;241m=\u001b[39m \u001b[43mperform_grid_search_balanced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_model\n",
      "File \u001b[0;32m~/Desktop/coding/ZHAW_Project/ML_BigData_Repo_1/notebooks/../references/decisiontree_gridsearch_modelutils.py:84\u001b[0m, in \u001b[0;36mperform_grid_search_balanced\u001b[0;34m(X, y, param_grid)\u001b[0m\n\u001b[1;32m     82\u001b[0m decision_tree_model \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     83\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mdecision_tree_model, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "\n",
    "# Assuming `df` is your data DataFrame and includes the 'is_fraud' target column\n",
    "\n",
    "features_by_importance = [\n",
    "    'TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag',\n",
    "    'TransactionCountLast7Days', 'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
    "    'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast14Days', 'AverageTransactionAmountLast30Days',\n",
    "    'Hour', 'HighRiskHour', 'DayOfWeek', 'IsWeekend', 'TransactionFrequency', 'RapidTransactionFlag',\n",
    "    'lat', 'long', 'merch_lat', 'merch_long', 'distance', 'city_pop',\n",
    "    'Age', 'AgeGroup', 'gender', 'state', 'city',\n",
    "    'degree', 'betweenness_centrality', 'community'\n",
    "]\n",
    "\n",
    "if use_test_data:\n",
    "    _, imp_feature_names = main(df, features_by_importance, param_grid, model_specs, reports_output_dir, use_test_data=True)\n",
    "else:\n",
    "    best_model, imp_feature_names = main(df, features_by_importance, param_grid, model_specs, reports_output_dir, use_test_data=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "30c613fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTrees_Parallelized_GridSearch_Balanced_by_Top10Features_Train END Model ....   completed at Tue Nov  5 09:47:27 2024. Elapsed time: 0 minutes and 20.72 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(f\"{model_specs}_{dataset_type} END Model ....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c81139df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree visualization saved to /Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/DecisionTrees/DecisionTrees_Parallelized_GridSearch_Balanced_by_Top10Features_Train_decision_tree_balanced_gridsearch_viz_20241105_094727.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate a unique timestamp for the filename\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "decisiontree_gridsearch_viz_filename = f\"{model_specs}_{dataset_type}_decision_tree_balanced_gridsearch_viz_{timestamp}.png\"\n",
    "decisiontree_gridsearch_viz_path = os.path.join(reports_output_dir, decisiontree_gridsearch_viz_filename)\n",
    "\n",
    "class_names = [\"Non-Fraud\", \"Fraud\"]\n",
    "\n",
    "if not use_test_data:\n",
    "    # Use the feature names returned from `main` to ensure consistency with the model\n",
    "    save_decision_tree_viz(model=best_model, feature_names=imp_feature_names, class_names=class_names, output_path=decisiontree_gridsearch_viz_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea9ad7",
   "metadata": {},
   "source": [
    "1. Load and Preprocess the Data for the Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7ced879f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTrees_Parallelized_GridSearch_Balanced_by_Top10Features_Train saved the descision tree ....   completed at Tue Nov  5 09:47:50 2024. Elapsed time: 0 minutes and 23.48 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(f\"{model_specs}_{dataset_type} saved the descision tree ....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac2aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0229d5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook ended at: Tue Nov  5 09:47:50 2024\n",
      "Total execution time: 10 minutes and 0.93 seconds\n",
      "DecisionTrees_Parallelized_GridSearch_Balanced_by_Top10Features_Train Notebook Ended at...  completed at Tue Nov  5 09:47:50 2024. Elapsed time: 10 minutes and 0.93 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Assuming start_time is defined earlier in the notebook\n",
    "end_time_notebook = time.time()\n",
    "elapsed_time = end_time_notebook - start_time_notebook\n",
    "\n",
    "# Print and format the notebook end time and total execution time\n",
    "print(f\"Notebook ended at: {time.ctime(end_time_notebook)}\")\n",
    "print(f\"Total execution time: {elapsed_time // 60:.0f} minutes and {elapsed_time % 60:.2f} seconds\")\n",
    "\n",
    "\n",
    "log_time(f\"{model_specs}_{dataset_type} Notebook Ended at... \", start_time_notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a8f73b",
   "metadata": {},
   "source": [
    "# Results of Top 10 features using Grid Search"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f06b82f0",
   "metadata": {},
   "source": [
    "Here are the notes summarizing your findings from the Decision Tree model with grid search using the top 10 important features:\n",
    "Decision Tree Model with Grid Search - Top 10 Features\n",
    "Training Set Performance\n",
    "\n",
    "    Accuracy: 0.9974\n",
    "    Classification Report:\n",
    "        Precision for Fraud (Class 1): 0.87\n",
    "        Recall for Fraud (Class 1): 0.66\n",
    "        F1-score for Fraud (Class 1): 0.75\n",
    "    Overall AUC Metrics:\n",
    "        ROC AUC: 0.9915\n",
    "        Precision-Recall AUC: 0.8295\n",
    "\n",
    "Test Set Performance\n",
    "\n",
    "    Accuracy: 0.9970\n",
    "    Classification Report:\n",
    "        Precision for Fraud (Class 1): 0.76\n",
    "        Recall for Fraud (Class 1): 0.31\n",
    "        F1-score for Fraud (Class 1): 0.44\n",
    "    Overall AUC Metrics:\n",
    "        ROC AUC: 0.8925\n",
    "        Precision-Recall AUC: 0.5714\n",
    "\n",
    "Observations\n",
    "\n",
    "    Strong Precision, Lower Recall:\n",
    "        The model achieves high precision for fraud cases on both the training and test sets, meaning that most flagged fraud cases are correctly identified.\n",
    "        Recall for fraud on the test set is relatively low (0.31), indicating that while the model is precise, it misses a significant portion of actual fraud cases in unseen data.\n",
    "\n",
    "    High AUC Scores in Training, Lower in Test:\n",
    "        The training set shows high ROC AUC and Precision-Recall AUC values, reflecting good separability between fraud and non-fraud classes.\n",
    "        The test set exhibits a drop in both AUC scores, especially in the Precision-Recall AUC, highlighting potential challenges in generalizing to new data.\n",
    "\n",
    "Next Steps to Improve Recall for Fraud Detection\n",
    "\n",
    "    Class Weight Adjustment:\n",
    "        Apply class_weight='balanced' in the DecisionTreeClassifier to give more weight to the minority class (fraud), which could help improve recall by encouraging the model to focus more on fraud cases.\n",
    "\n",
    "    Threshold Tuning:\n",
    "        Consider tuning the decision threshold for classifying fraud cases. Lowering the threshold may help increase recall, though it may slightly impact precision.\n",
    "\n",
    "    Use of Oversampling Techniques (SMOTE):\n",
    "        Applying SMOTE on the training data to balance the fraud and non-fraud cases can provide the model with more representative fraud patterns, potentially improving its ability to detect fraud.\n",
    "\n",
    "These adjustments should help create a more balanced model that maintains precision while boosting its ability to identify a greater proportion of fraud cases in both training and test scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f476254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d88080b",
   "metadata": {},
   "source": [
    "# Results of Top 10 features using Grid Search BALANCED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1460e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "9f60ed67",
   "metadata": {},
   "source": [
    "Balanced Decision Tree Model with Grid Search - Top 10 Features\n",
    "Training Set Performance\n",
    "\n",
    "    Accuracy: 0.9919\n",
    "    Classification Report:\n",
    "        Precision for Fraud (Class 1): 0.42\n",
    "        Recall for Fraud (Class 1): 1.00\n",
    "        F1-score for Fraud (Class 1): 0.59\n",
    "    Overall AUC Metrics:\n",
    "        ROC AUC: 0.9992\n",
    "        Precision-Recall AUC: 0.9038\n",
    "\n",
    "Test Set Performance\n",
    "\n",
    "    Accuracy: 0.9919\n",
    "    Classification Report:\n",
    "        Precision for Fraud (Class 1): 0.25\n",
    "        Recall for Fraud (Class 1): 0.67\n",
    "        F1-score for Fraud (Class 1): 0.37\n",
    "    Overall AUC Metrics:\n",
    "        Macro Avg (Precision-Recall): 0.63 (Precision) / 0.83 (Recall) / 0.68 (F1)\n",
    "\n",
    "Observations\n",
    "\n",
    "    High Recall for Fraud:\n",
    "        The model achieves 100% recall for fraud cases in the training set, demonstrating the effectiveness of the class_weight='balanced' adjustment in capturing all fraud cases.\n",
    "        In the test set, recall for fraud is also significantly improved (0.67), meaning the model is capturing a much higher percentage of actual fraud cases compared to the unbalanced model.\n",
    "\n",
    "    Precision Trade-off:\n",
    "        Precision for fraud is lower in both training (0.42) and test (0.25) sets. This is expected with high recall, as the model identifies more fraud cases but also includes more false positives.\n",
    "        The F1-score on the test set for fraud (0.37) reflects this trade-off between precision and recall.\n",
    "\n",
    "    Strong AUC Metrics:\n",
    "        The high ROC AUC and Precision-Recall AUC values in the training set indicate that the model has learned to distinguish between classes effectively, though this generalizes slightly less well on the test set due to more varied data.\n",
    "\n",
    "Next Steps\n",
    "\n",
    "    Fine-tune Threshold for Fraud Detection:\n",
    "        Adjust the decision threshold for classifying fraud cases. Lowering the threshold further may improve recall, while a slightly higher threshold may balance out precision.\n",
    "\n",
    "    Evaluate on Larger Test Sets:\n",
    "        Test the model on additional or larger test sets to confirm if the current recall improvements are consistent. This could provide a more comprehensive evaluation of model performance under different conditions.\n",
    "\n",
    "    Consider Using Ensemble Methods:\n",
    "        If maintaining both high precision and recall is crucial, consider trying ensemble models like Random Forest or Gradient Boosting. These methods can sometimes better balance precision and recall in imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f78e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ZHAW_Project)",
   "language": "python",
   "name": "zhaw_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
