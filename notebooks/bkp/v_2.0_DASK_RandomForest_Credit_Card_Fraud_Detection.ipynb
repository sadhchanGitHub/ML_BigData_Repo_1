{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd5c6a5",
   "metadata": {},
   "source": [
    "# v2.0 - Random Forest with DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3e22053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn import preprocessing\n",
    "import geopy\n",
    "from geopy.distance import geodesic\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import dask.dataframe as dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3ff9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../references')  # Add the references folder to the system path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7177dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_specs = 'Random Forest using DASK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab52f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_notebook = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c56c046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/RandomForest\n"
     ]
    }
   ],
   "source": [
    "# Directory to save the figures \n",
    "\n",
    "input_src_dir = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/data/raw'\n",
    "output_dir_figures_train = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/figures/train_figures'\n",
    "output_dir_figures_test = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/figures/test_figures'\n",
    "#reports_output_dir = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports'\n",
    "\n",
    "reports_output_dir_base = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports'\n",
    "# reports_output_dir for DecisionTrees\n",
    "reports_output_dir = f\"{reports_output_dir_base}/RandomForest\"\n",
    "print(reports_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e37d27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which dataset to use\n",
    "use_test_data = False  # Set to True when using fraudtest.csv\n",
    "\n",
    "# Determine dataset type based on the variable\n",
    "dataset_type = 'Test' if use_test_data else 'Train'\n",
    "\n",
    "# Load the appropriate dataset\n",
    "\n",
    "if use_test_data:\n",
    "    output_dir_figures = output_dir_figures_test\n",
    "else:\n",
    "    output_dir_figures = output_dir_figures_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd0bc5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the preprocess file name dynamically\n",
    "# Get the current timestamp\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")  # Format: YYYYMMDD_HHMMSS\n",
    "\n",
    "logfile_title = 'LogFile'\n",
    "logfile_name = f\"{model_specs}_{dataset_type}_{logfile_title.replace(',', '').lower().split('.')[0]}_{timestamp}.txt\"\n",
    "\n",
    "logfile_path = os.path.join(reports_output_dir, logfile_name)\n",
    "\n",
    "# Function to log times to a file\n",
    "def log_time(step_name, start_time):\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    log_message = (f\"{step_name} completed at {time.ctime(end_time)}. \"\n",
    "                   f\"Elapsed time: {elapsed_time // 60:.0f} minutes and {elapsed_time % 60:.2f} seconds\\n\")\n",
    "    \n",
    "    # Append log to file\n",
    "    with open(logfile_path, 'a') as f:\n",
    "        f.write(log_message)\n",
    "    \n",
    "    # Print the message to the console as well\n",
    "    print(log_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "383fd5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest using DASK_Train Notebook started at...  completed at Sun Nov  3 10:52:52 2024. Elapsed time: 0 minutes and 0.03 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(f\"{model_specs}_{dataset_type} Notebook started at... \", start_time_notebook)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0253dfc7",
   "metadata": {},
   "source": [
    "# Use Dask already from the beginning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0b58eee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Starting the DASK Client----- completed at Sun Nov  3 11:50:04 2024. Elapsed time: 57 minutes and 11.35 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 12:14:22,322 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle d8ab6c393e9fcceb2f101b68cf9967bd initialized by task ('shuffle-transfer-d8ab6c393e9fcceb2f101b68cf9967bd', 9) executed on worker tcp://127.0.0.1:62741\n",
      "2024-11-03 12:14:22,623 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle d8ab6c393e9fcceb2f101b68cf9967bd deactivated due to stimulus 'task-finished-1730632462.619124'\n",
      "2024-11-03 12:14:22,812 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle d05affed247452e8db1a96ea9c36092c initialized by task ('shuffle-transfer-d05affed247452e8db1a96ea9c36092c', 18) executed on worker tcp://127.0.0.1:62742\n",
      "2024-11-03 12:14:23,072 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle d05affed247452e8db1a96ea9c36092c deactivated due to stimulus 'task-finished-1730632463.0686378'\n",
      "2024-11-03 12:14:23,227 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 1d2d4a4d97a78d0f5d154c60586404ad initialized by task ('shuffle-transfer-1d2d4a4d97a78d0f5d154c60586404ad', 10) executed on worker tcp://127.0.0.1:62742\n",
      "2024-11-03 12:14:23,547 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 1d2d4a4d97a78d0f5d154c60586404ad deactivated due to stimulus 'task-finished-1730632463.5450392'\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import dask.dataframe as dd\n",
    "\n",
    "log_time(\"------Starting the DASK Client-----\", start_time_notebook)\n",
    "\n",
    "# Start Dask client\n",
    "#client = Client()\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client(dashboard_address=':0')  # Dask will pick an available port\n",
    "#to avoid warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "80470a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Load the dataset directly into Dask\n",
    "if use_test_data:\n",
    "    df_pre = dd.read_csv(f\"{input_src_dir}/fraudTest.csv\", assume_missing=True, blocksize=\"16MB\")\n",
    "else:\n",
    "    df_pre = dd.read_csv(f\"{input_src_dir}/fraudTrain.csv\", assume_missing=True, blocksize=\"16MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "76ac6a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0     TransactionTime  CreditCardNumber  \\\n",
      "0         0.0 2019-01-01 00:00:18      2.703186e+15   \n",
      "1         1.0 2019-01-01 00:00:44      6.304233e+11   \n",
      "2         2.0 2019-01-01 00:00:51      3.885949e+13   \n",
      "3         3.0 2019-01-01 00:01:16      3.534094e+15   \n",
      "4         4.0 2019-01-01 00:03:06      3.755342e+14   \n",
      "\n",
      "                             merchant       category  TransactionAmount  \\\n",
      "0          fraud_Rippin, Kub and Mann       misc_net               4.97   \n",
      "1     fraud_Heller, Gutmann and Zieme    grocery_pos             107.23   \n",
      "2                fraud_Lind-Buckridge  entertainment             220.11   \n",
      "3  fraud_Kutch, Hermiston and Farrell  gas_transport              45.00   \n",
      "4                 fraud_Keeling-Crist       misc_pos              41.96   \n",
      "\n",
      "       first     last gender                        street  ...      long  \\\n",
      "0   Jennifer    Banks      F                561 Perry Cove  ...  -81.1781   \n",
      "1  Stephanie     Gill      F  43039 Riley Greens Suite 393  ... -118.2105   \n",
      "2     Edward  Sanchez      M      594 White Dale Suite 530  ... -112.2620   \n",
      "3     Jeremy    White      M   9443 Cynthia Court Apt. 038  ... -112.1138   \n",
      "4      Tyler   Garcia      M              408 Bradley Rest  ...  -79.4629   \n",
      "\n",
      "  city_pop                                job  DateOfBirth  \\\n",
      "0   3495.0          Psychologist, counselling   1988-03-09   \n",
      "1    149.0  Special educational needs teacher   1978-06-21   \n",
      "2   4154.0        Nature conservation officer   1962-01-19   \n",
      "3   1939.0                    Patent attorney   1967-01-12   \n",
      "4     99.0     Dance movement psychotherapist   1986-03-28   \n",
      "\n",
      "                          trans_num     unix_time  merch_lat  merch_long  \\\n",
      "0  0b242abb623afc578575680df30655b9  1.325376e+09  36.011293  -82.048315   \n",
      "1  1f76529f8574734946361c461b024d99  1.325376e+09  49.159047 -118.186462   \n",
      "2  a1a22d70485983eac12b5b88dad1cf95  1.325376e+09  43.150704 -112.154481   \n",
      "3  6b849c168bdad6f867558c3793159a81  1.325376e+09  47.034331 -112.561071   \n",
      "4  a41d7549acf90789359a9aa5346dcb46  1.325376e+09  38.674999  -78.632459   \n",
      "\n",
      "  is_fraud  TransactionID  \n",
      "0      0.0              1  \n",
      "1      0.0              2  \n",
      "2      0.0              3  \n",
      "3      0.0              4  \n",
      "4      0.0              5  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Rename columns\n",
    "df_pre = df_pre.rename(columns={\n",
    "    'amt': 'TransactionAmount', \n",
    "    'cc_num': 'CreditCardNumber', \n",
    "    'dob': 'DateOfBirth', \n",
    "    'trans_date_trans_time': 'TransactionTime'\n",
    "})\n",
    "\n",
    "# Data Cleaning and Feature Engineering\n",
    "df_pre['TransactionTime'] = dd.to_datetime(df_pre['TransactionTime'])\n",
    "df_pre['DateOfBirth'] = dd.to_datetime(df_pre['DateOfBirth'], errors='coerce')\n",
    "\n",
    "# Define metadata with the new 'TransactionID' column\n",
    "meta = df_pre.head(0)  # Capture existing schema\n",
    "meta['TransactionID'] = int()  # Add the new column to the metadata\n",
    "\n",
    "# Define function to assign unique TransactionIDs within each partition\n",
    "def assign_transaction_id(df, start_id=1):\n",
    "    df = df.copy()\n",
    "    df['TransactionID'] = range(start_id, start_id + len(df))\n",
    "    return df\n",
    "\n",
    "# Apply function with updated metadata\n",
    "df_pre = df_pre.map_partitions(assign_transaction_id, meta=meta)\n",
    "\n",
    "# Persist the DataFrame after transformations\n",
    "df_pre = df_pre.persist()\n",
    "\n",
    "# Optional: Display the head of the DataFrame to verify\n",
    "print(df_pre.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "258745ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 21\n",
      "Columns: Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID'],\n",
      "      dtype='object')\n",
      "Head of Data:    Unnamed: 0     TransactionTime  CreditCardNumber  \\\n",
      "0         0.0 2019-01-01 00:00:18      2.703186e+15   \n",
      "1         1.0 2019-01-01 00:00:44      6.304233e+11   \n",
      "2         2.0 2019-01-01 00:00:51      3.885949e+13   \n",
      "3         3.0 2019-01-01 00:01:16      3.534094e+15   \n",
      "4         4.0 2019-01-01 00:03:06      3.755342e+14   \n",
      "\n",
      "                             merchant       category  TransactionAmount  \\\n",
      "0          fraud_Rippin, Kub and Mann       misc_net               4.97   \n",
      "1     fraud_Heller, Gutmann and Zieme    grocery_pos             107.23   \n",
      "2                fraud_Lind-Buckridge  entertainment             220.11   \n",
      "3  fraud_Kutch, Hermiston and Farrell  gas_transport              45.00   \n",
      "4                 fraud_Keeling-Crist       misc_pos              41.96   \n",
      "\n",
      "       first     last gender                        street  ...      long  \\\n",
      "0   Jennifer    Banks      F                561 Perry Cove  ...  -81.1781   \n",
      "1  Stephanie     Gill      F  43039 Riley Greens Suite 393  ... -118.2105   \n",
      "2     Edward  Sanchez      M      594 White Dale Suite 530  ... -112.2620   \n",
      "3     Jeremy    White      M   9443 Cynthia Court Apt. 038  ... -112.1138   \n",
      "4      Tyler   Garcia      M              408 Bradley Rest  ...  -79.4629   \n",
      "\n",
      "  city_pop                                job  DateOfBirth  \\\n",
      "0   3495.0          Psychologist, counselling   1988-03-09   \n",
      "1    149.0  Special educational needs teacher   1978-06-21   \n",
      "2   4154.0        Nature conservation officer   1962-01-19   \n",
      "3   1939.0                    Patent attorney   1967-01-12   \n",
      "4     99.0     Dance movement psychotherapist   1986-03-28   \n",
      "\n",
      "                          trans_num     unix_time  merch_lat  merch_long  \\\n",
      "0  0b242abb623afc578575680df30655b9  1.325376e+09  36.011293  -82.048315   \n",
      "1  1f76529f8574734946361c461b024d99  1.325376e+09  49.159047 -118.186462   \n",
      "2  a1a22d70485983eac12b5b88dad1cf95  1.325376e+09  43.150704 -112.154481   \n",
      "3  6b849c168bdad6f867558c3793159a81  1.325376e+09  47.034331 -112.561071   \n",
      "4  a41d7549acf90789359a9aa5346dcb46  1.325376e+09  38.674999  -78.632459   \n",
      "\n",
      "  is_fraud  TransactionID  \n",
      "0      0.0              1  \n",
      "1      0.0              2  \n",
      "2      0.0              3  \n",
      "3      0.0              4  \n",
      "4      0.0              5  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Summary statistics without .compute()\n",
    "print(\"Number of partitions:\", df_pre.npartitions)\n",
    "print(\"Columns:\", df_pre.columns)\n",
    "print(\"Head of Data:\", df_pre.head())  # Displays head without full .compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ba18d2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud counts: is_fraud\n",
      "1.0       7506\n",
      "0.0    1289169\n",
      "Name: count, dtype: int64\n",
      "Fraud percentage: is_fraud\n",
      "1.0     0.578865\n",
      "0.0    99.421135\n",
      "Name: proportion, dtype: float64\n",
      "Unique Credit Cards: 983\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specific aggregations or unique counts, with selective .compute()\n",
    "fraud_counts = df_pre['is_fraud'].value_counts().compute()\n",
    "fraud_percentage = df_pre['is_fraud'].value_counts(normalize=True).compute() * 100\n",
    "unique_credit_cards = df_pre['CreditCardNumber'].nunique().compute()\n",
    "\n",
    "print(\"Fraud counts:\", fraud_counts)\n",
    "print(\"Fraud percentage:\", fraud_percentage)\n",
    "print(\"Unique Credit Cards:\", unique_credit_cards)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96d16888",
   "metadata": {},
   "source": [
    "#debug\n",
    "df = dd.read_csv(f\"{input_src_dir}/fraudTrain.csv\", assume_missing=True)\n",
    "row_count = df.size.compute() // len(df.columns)  # Approximate row count\n",
    "print(\"Row count after loading:\", row_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d3babc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " Unnamed: 0           0\n",
      "TransactionTime      0\n",
      "CreditCardNumber     0\n",
      "merchant             0\n",
      "category             0\n",
      "TransactionAmount    0\n",
      "first                0\n",
      "last                 0\n",
      "gender               0\n",
      "street               0\n",
      "city                 0\n",
      "state                0\n",
      "zip                  0\n",
      "lat                  0\n",
      "long                 0\n",
      "city_pop             0\n",
      "job                  0\n",
      "DateOfBirth          0\n",
      "trans_num            0\n",
      "unix_time            0\n",
      "merch_lat            0\n",
      "merch_long           0\n",
      "is_fraud             0\n",
      "TransactionID        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in Dask DataFrame\n",
    "missing_values = df_pre.isnull().sum().compute()\n",
    "print(\"Missing values per column:\\n\", missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "391577c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "52941f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dask_expr.expr.Index: expr=Index(frame=SetIndex(frame=FromGraph(2897191), _other='TransactionTime', options={}))>\n"
     ]
    }
   ],
   "source": [
    "# Set 'TransactionTime' as the index permanently\n",
    "df_pre = df_pre.set_index('TransactionTime')\n",
    "\n",
    "# Verify the index\n",
    "print(df_pre.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "55397673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Transaction Time: 2019-01-01 00:00:18\n",
      "Maximum Transaction Time: 2020-06-21 12:13:37\n"
     ]
    }
   ],
   "source": [
    "# Get the minimum and maximum transaction times, computing the results\n",
    "min_time = df_pre.index.min().compute()\n",
    "max_time = df_pre.index.max().compute()\n",
    "\n",
    "print(f\"Minimum Transaction Time: {min_time}\")\n",
    "print(f\"Maximum Transaction Time: {max_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a1fc7ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CreditCardNumber</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>TransactionAmount</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>DateOfBirth</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>TransactionID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.703186e+15</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>Moravian Falls</td>\n",
       "      <td>...</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495.0</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1.325376e+09</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.304233e+11</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>Orient</td>\n",
       "      <td>...</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149.0</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1.325376e+09</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:51</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.885949e+13</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>Malad City</td>\n",
       "      <td>...</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154.0</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1.325376e+09</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:01:16</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.534094e+15</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>9443 Cynthia Court Apt. 038</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>...</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>Patent attorney</td>\n",
       "      <td>1967-01-12</td>\n",
       "      <td>6b849c168bdad6f867558c3793159a81</td>\n",
       "      <td>1.325376e+09</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:03:06</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.755342e+14</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>Garcia</td>\n",
       "      <td>M</td>\n",
       "      <td>408 Bradley Rest</td>\n",
       "      <td>Doe Hill</td>\n",
       "      <td>...</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>99.0</td>\n",
       "      <td>Dance movement psychotherapist</td>\n",
       "      <td>1986-03-28</td>\n",
       "      <td>a41d7549acf90789359a9aa5346dcb46</td>\n",
       "      <td>1.325376e+09</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Unnamed: 0  CreditCardNumber  \\\n",
       "TransactionTime                                     \n",
       "2019-01-01 00:00:18         0.0      2.703186e+15   \n",
       "2019-01-01 00:00:44         1.0      6.304233e+11   \n",
       "2019-01-01 00:00:51         2.0      3.885949e+13   \n",
       "2019-01-01 00:01:16         3.0      3.534094e+15   \n",
       "2019-01-01 00:03:06         4.0      3.755342e+14   \n",
       "\n",
       "                                               merchant       category  \\\n",
       "TransactionTime                                                          \n",
       "2019-01-01 00:00:18          fraud_Rippin, Kub and Mann       misc_net   \n",
       "2019-01-01 00:00:44     fraud_Heller, Gutmann and Zieme    grocery_pos   \n",
       "2019-01-01 00:00:51                fraud_Lind-Buckridge  entertainment   \n",
       "2019-01-01 00:01:16  fraud_Kutch, Hermiston and Farrell  gas_transport   \n",
       "2019-01-01 00:03:06                 fraud_Keeling-Crist       misc_pos   \n",
       "\n",
       "                     TransactionAmount      first     last gender  \\\n",
       "TransactionTime                                                     \n",
       "2019-01-01 00:00:18               4.97   Jennifer    Banks      F   \n",
       "2019-01-01 00:00:44             107.23  Stephanie     Gill      F   \n",
       "2019-01-01 00:00:51             220.11     Edward  Sanchez      M   \n",
       "2019-01-01 00:01:16              45.00     Jeremy    White      M   \n",
       "2019-01-01 00:03:06              41.96      Tyler   Garcia      M   \n",
       "\n",
       "                                           street            city  ...  \\\n",
       "TransactionTime                                                    ...   \n",
       "2019-01-01 00:00:18                561 Perry Cove  Moravian Falls  ...   \n",
       "2019-01-01 00:00:44  43039 Riley Greens Suite 393          Orient  ...   \n",
       "2019-01-01 00:00:51      594 White Dale Suite 530      Malad City  ...   \n",
       "2019-01-01 00:01:16   9443 Cynthia Court Apt. 038         Boulder  ...   \n",
       "2019-01-01 00:03:06              408 Bradley Rest        Doe Hill  ...   \n",
       "\n",
       "                         long  city_pop                                job  \\\n",
       "TransactionTime                                                              \n",
       "2019-01-01 00:00:18  -81.1781    3495.0          Psychologist, counselling   \n",
       "2019-01-01 00:00:44 -118.2105     149.0  Special educational needs teacher   \n",
       "2019-01-01 00:00:51 -112.2620    4154.0        Nature conservation officer   \n",
       "2019-01-01 00:01:16 -112.1138    1939.0                    Patent attorney   \n",
       "2019-01-01 00:03:06  -79.4629      99.0     Dance movement psychotherapist   \n",
       "\n",
       "                     DateOfBirth                         trans_num  \\\n",
       "TransactionTime                                                      \n",
       "2019-01-01 00:00:18   1988-03-09  0b242abb623afc578575680df30655b9   \n",
       "2019-01-01 00:00:44   1978-06-21  1f76529f8574734946361c461b024d99   \n",
       "2019-01-01 00:00:51   1962-01-19  a1a22d70485983eac12b5b88dad1cf95   \n",
       "2019-01-01 00:01:16   1967-01-12  6b849c168bdad6f867558c3793159a81   \n",
       "2019-01-01 00:03:06   1986-03-28  a41d7549acf90789359a9aa5346dcb46   \n",
       "\n",
       "                        unix_time  merch_lat  merch_long  is_fraud  \\\n",
       "TransactionTime                                                      \n",
       "2019-01-01 00:00:18  1.325376e+09  36.011293  -82.048315       0.0   \n",
       "2019-01-01 00:00:44  1.325376e+09  49.159047 -118.186462       0.0   \n",
       "2019-01-01 00:00:51  1.325376e+09  43.150704 -112.154481       0.0   \n",
       "2019-01-01 00:01:16  1.325376e+09  47.034331 -112.561071       0.0   \n",
       "2019-01-01 00:03:06  1.325376e+09  38.674999  -78.632459       0.0   \n",
       "\n",
       "                     TransactionID  \n",
       "TransactionTime                     \n",
       "2019-01-01 00:00:18              1  \n",
       "2019-01-01 00:00:44              2  \n",
       "2019-01-01 00:00:51              3  \n",
       "2019-01-01 00:01:16              4  \n",
       "2019-01-01 00:03:06              5  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8760e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7ce3b058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally assigning to df as all follwoing code is in df\n",
    "df = df_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49c791",
   "metadata": {},
   "source": [
    "##JUST for iNFO not needed\n",
    "#df = df_pre.compute()  # Converts Dask DataFrame to Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "10777762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Steps Completed File Loading, Describe, Date Conversions etc..   completed at Sun Nov  3 10:53:15 2024. Elapsed time: 0 minutes and 22.84 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"Initial Steps Completed File Loading, Describe, Date Conversions etc..  \", start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b32d0d",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ca771ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log pre-process time at various steps\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a2a6ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START - Feature Engineering .....   completed at Sun Nov  3 12:19:39 2024. Elapsed time: 0 minutes and 0.01 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_time(\"START - Feature Engineering .....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "31252f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clip outliers if necessary\n",
    "df['TransactionAmount'] = df['TransactionAmount'].clip(upper=df['TransactionAmount'].quantile(0.99))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bf9e3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Replace infinite values with NaN in the 'TransactionAmount' column\n",
    "df['TransactionAmount'] = df['TransactionAmount'].replace([np.inf, -np.inf], np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f558d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To persist the change and ensure no computations are delayed, you can persist the DataFrame:\n",
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b21308e",
   "metadata": {},
   "source": [
    "# next type of VIZ via transaction id vs transaction count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "70594f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour from TransactionTime\n",
    "df['Hour'] = df.index.hour  # Since TransactionTime is already set as the index\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efdf4e65",
   "metadata": {},
   "source": [
    "##following is in PANDAS, hence commented\n",
    "\n",
    "# Calculate fraud rate by hour\n",
    "fraud_rate_by_hour = df.groupby('Hour')['is_fraud'].mean()\n",
    "\n",
    "# Sort by fraud rate in descending order\n",
    "fraud_rate_by_hour = fraud_rate_by_hour.sort_values(ascending=False)\n",
    "\n",
    "# Define a threshold for high-risk hours (adjust as needed)\n",
    "threshold = fraud_rate_by_hour.mean()  # Mean fraud rate across all hours\n",
    "\n",
    "# Dynamically identify high-risk hours based on the threshold\n",
    "high_risk_hours = fraud_rate_by_hour[fraud_rate_by_hour > threshold].index.tolist()\n",
    "\n",
    "# Print high-risk hours for reference\n",
    "print(\"High-Risk Hours:\", high_risk_hours)\n",
    "\n",
    "# Create the HighRiskHour flag based on dynamically identified high-risk hours\n",
    "df['HighRiskHour'] = df['Hour'].apply(lambda x: 1 if x in high_risk_hours else 0)\n",
    "\n",
    "# Print a sample of the DataFrame to verify the new column\n",
    "print(df[['Hour', 'HighRiskHour']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1eff53b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[169], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_fraud\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Step 1: Calculate fraud rate by hour and retain it as a Dask DataFrame\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m fraud_rate_by_hour \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHour\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_fraud\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Step 2: Define a threshold for high-risk hours based on the mean fraud rate\u001b[39;00m\n\u001b[1;32m     13\u001b[0m threshold \u001b[38;5;241m=\u001b[39m fraud_rate_by_hour\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mcompute()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_collection.py:447\u001b[0m, in \u001b[0;36mFrameBase.persist\u001b[0;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpersist\u001b[39m(\u001b[38;5;28mself\u001b[39m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 447\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DaskMethodsMixin\u001b[38;5;241m.\u001b[39mpersist(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_collection.py:595\u001b[0m, in \u001b[0;36mFrameBase.optimize\u001b[0;34m(self, fuse)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, fuse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    578\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimizes the DataFrame.\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \n\u001b[1;32m    580\u001b[0m \u001b[38;5;124;03m    Runs the optimizer with all steps over the DataFrame and wraps the result in a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m        The optimized Dask Dataframe\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_expr.py:94\u001b[0m, in \u001b[0;36mExpr.optimize\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_expr.py:3083\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(expr, fuse)\u001b[0m\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"High level query optimization\u001b[39;00m\n\u001b[1;32m   3063\u001b[0m \n\u001b[1;32m   3064\u001b[0m \u001b[38;5;124;03mThis leverages three optimization passes:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3079\u001b[0m \u001b[38;5;124;03moptimize_blockwise_fusion\u001b[39;00m\n\u001b[1;32m   3080\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3081\u001b[0m stage: core\u001b[38;5;241m.\u001b[39mOptimizerStage \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fuse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimplified-physical\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimize_until\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_expr.py:3044\u001b[0m, in \u001b[0;36moptimize_until\u001b[0;34m(expr, stage)\u001b[0m\n\u001b[1;32m   3041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m expr\n\u001b[1;32m   3043\u001b[0m \u001b[38;5;66;03m# Lower\u001b[39;00m\n\u001b[0;32m-> 3044\u001b[0m expr \u001b[38;5;241m=\u001b[39m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_completely\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphysical\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m expr\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_core.py:447\u001b[0m, in \u001b[0;36mExpr.lower_completely\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    445\u001b[0m lowered \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 447\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlowered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m==\u001b[39m expr\u001b[38;5;241m.\u001b[39m_name:\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_core.py:413\u001b[0m, in \u001b[0;36mExpr.lower_once\u001b[0;34m(self, lowered)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m operand \u001b[38;5;129;01min\u001b[39;00m out\u001b[38;5;241m.\u001b[39moperands:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, Expr):\n\u001b[0;32m--> 413\u001b[0m         new \u001b[38;5;241m=\u001b[39m \u001b[43moperand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlowered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m!=\u001b[39m operand\u001b[38;5;241m.\u001b[39m_name:\n\u001b[1;32m    415\u001b[0m             changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_core.py:402\u001b[0m, in \u001b[0;36mExpr.lower_once\u001b[0;34m(self, lowered)\u001b[0m\n\u001b[1;32m    399\u001b[0m expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# Lower this node\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m     out \u001b[38;5;241m=\u001b[39m expr\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_reductions.py:515\u001b[0m, in \u001b[0;36mApplyConcatApply._lower\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    511\u001b[0m split_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_every\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    512\u001b[0m chunked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_cls(\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), chunk, chunk_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_cls_args\n\u001b[1;32m    514\u001b[0m )\n\u001b[0;32m--> 515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshould_shuffle\u001b[49m:\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;66;03m# Lower into TreeReduce(Chunk)\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeReduce(\n\u001b[1;32m    518\u001b[0m         chunked,\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    525\u001b[0m         split_every\u001b[38;5;241m=\u001b[39msplit_every,\n\u001b[1;32m    526\u001b[0m     )\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneed_to_shuffle:\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;66;03m# Repartition and return\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_reductions.py:467\u001b[0m, in \u001b[0;36mApplyConcatApply.should_shuffle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshould_shuffle\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    465\u001b[0m     sort \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msort\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m--> 467\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_out\u001b[49m, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_out \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m sort\n\u001b[1;32m    468\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_groupby.py:692\u001b[0m, in \u001b[0;36mGroupByReduction.split_out\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperand(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_out\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_out\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_reductions.py:421\u001b[0m, in \u001b[0;36mApplyConcatApply.split_out\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    419\u001b[0m split_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperand(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(split_out, Callable):\n\u001b[0;32m--> 421\u001b[0m     split_out \u001b[38;5;241m=\u001b[39m split_out(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnpartitions\u001b[49m)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_out can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_expr.py:398\u001b[0m, in \u001b[0;36mExpr.npartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands[idx]\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisions\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/functools.py:981\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    979\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 981\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    983\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_expr.py:382\u001b[0m, in \u001b[0;36mExpr.divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdivisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_divisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_expr.py:529\u001b[0m, in \u001b[0;36mBlockwise._divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    527\u001b[0m dependencies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdependencies()\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m dependencies:\n\u001b[0;32m--> 529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_broadcast_dep\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    530\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mdivisions \u001b[38;5;241m==\u001b[39m dependencies[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdivisions\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dependencies[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdivisions\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_expr.py:520\u001b[0m, in \u001b[0;36mBlockwise._broadcast_dep\u001b[0;34m(self, dep)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_dep\u001b[39m(\u001b[38;5;28mself\u001b[39m, dep: Expr):\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# Checks if a dependency should be broadcasted to\u001b[39;00m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# all partitions of this `Blockwise` operation\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnpartitions\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dep\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_expr.py:398\u001b[0m, in \u001b[0;36mExpr.npartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands[idx]\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisions\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/functools.py:981\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    979\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 981\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    983\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_expr.py:382\u001b[0m, in \u001b[0;36mExpr.divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdivisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_divisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_expr.py:2267\u001b[0m, in \u001b[0;36mResetIndex._divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_divisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnpartitions\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_expr.py:398\u001b[0m, in \u001b[0;36mExpr.npartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperands[idx]\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisions\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/functools.py:981\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    979\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 981\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    983\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_expr.py:382\u001b[0m, in \u001b[0;36mExpr.divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdivisions\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_divisions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_expr.py:530\u001b[0m, in \u001b[0;36mBlockwise._divisions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m dependencies:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_broadcast_dep(arg):\n\u001b[0;32m--> 530\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mdivisions \u001b[38;5;241m==\u001b[39m dependencies[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdivisions\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dependencies[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdivisions\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Reset the index to avoid any indexing issues (optional based on your data structure)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Drop NaN values in 'Hour' or 'is_fraud' columns if they exist\n",
    "df = df.dropna(subset=['Hour', 'is_fraud'])\n",
    "\n",
    "# Step 1: Calculate fraud rate by hour and retain it as a Dask DataFrame\n",
    "fraud_rate_by_hour = df.groupby('Hour').is_fraud.mean().persist()\n",
    "\n",
    "# Step 2: Define a threshold for high-risk hours based on the mean fraud rate\n",
    "threshold = fraud_rate_by_hour.mean().compute()\n",
    "\n",
    "# Step 3: Identify high-risk hours based on the threshold\n",
    "high_risk_hours = fraud_rate_by_hour[fraud_rate_by_hour > threshold].index.compute().tolist()\n",
    "\n",
    "# Print high-risk hours for reference\n",
    "print(\"High-Risk Hours:\", high_risk_hours)\n",
    "\n",
    "# Step 4: Create the HighRiskHour flag in the main Dask DataFrame\n",
    "df['HighRiskHour'] = df['Hour'].map(lambda x: 1 if x in high_risk_hours else 0, meta=('HighRiskHour', 'int64'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75736fa",
   "metadata": {},
   "source": [
    "1. Time-Based Analysis:\n",
    "Already explored daily and hourly trends in transaction volumes, but now dive deeper into fraud patterns based on time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c0eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Weekday vs. Weekend: Is fraud more common on weekdays or weekends?\n",
    "df['DayOfWeek'] = df.index.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "fraud_by_day = df[df['is_fraud'] == 1]['DayOfWeek'].value_counts().sort_index()\n",
    "non_fraud_by_day = df[df['is_fraud'] == 0]['DayOfWeek'].value_counts().sort_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d77fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the correct day order\n",
    "day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "\n",
    "\n",
    "df['DayName'] = df.index.day_name()\n",
    "# Convert the 'DayName' column to a categorical type with the correct order\n",
    "df['DayName'] = pd.Categorical(df['DayName'], categories=day_order, ordered=True)\n",
    "\n",
    "fraud_by_day = df[df['is_fraud'] == 1]['DayName'].value_counts().sort_index()\n",
    "non_fraud_by_day = df[df['is_fraud'] == 0]['DayName'].value_counts().sort_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IsWeekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "weekend_fraud = df[df['is_fraud'] == 1]['IsWeekend'].mean()\n",
    "weekend_non_fraud = df[df['is_fraud'] == 0]['IsWeekend'].mean()\n",
    "\n",
    "print(f\"Percentage of fraud on weekends: {weekend_fraud * 100:.2f}%\")\n",
    "print(f\"Percentage of non-fraud on weekends: {weekend_non_fraud * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a62065",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time(\"Part1 - TrxAmount, Hour, DayOfWeeek etc..\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b51d0f2e",
   "metadata": {},
   "source": [
    "# Calculate distance between cardholder and merchant\n",
    "df['distance'] = df.apply(lambda row: geodesic((row['lat'], row['long']), (row['merch_lat'], row['merch_long'])).km, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f04c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir())  # List all files in the current directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a9514",
   "metadata": {},
   "source": [
    "# MULTIPROCESSING : distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0bbf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from distance_calculation import calculate_distance_chunk\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Add the current working directory to the system path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Multiprocessing function to split the dataframe and apply the distance calculation\n",
    "def parallel_distance_calculation(df, num_partitions=None):\n",
    "    if num_partitions is None:\n",
    "        num_partitions = mp.cpu_count()  # Use all available CPU cores\n",
    "    \n",
    "    # Split the dataframe into chunks\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    \n",
    "    # Create a multiprocessing Pool\n",
    "    with mp.Pool(num_partitions) as pool:\n",
    "        # Apply the calculate_distance_chunk function to each chunk in parallel\n",
    "        result = pool.map(calculate_distance_chunk, df_split)\n",
    "    \n",
    "    # Concatenate the results back into a single dataframe\n",
    "    return pd.concat(result)\n",
    "\n",
    "# Main block to ensure multiprocessing works correctly\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Assuming df has the columns ['lat', 'long', 'merch_lat', 'merch_long']\n",
    "    \n",
    "    # Run with limited number of cores (e.g., 4 cores)\n",
    "    df = parallel_distance_calculation(df, num_partitions=4)  # Use 4 cores instead of all available cores\n",
    "\n",
    "    # Log the time taken for distance calculation with multiprocessing\n",
    "    log_time(\"Part2 - Distance Calculation with Multiprocessing (4 cores)\", start_time)\n",
    "\n",
    "    # Check the first few rows to verify the result\n",
    "    print(df[['lat', 'long', 'merch_lat', 'merch_long', 'distance']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c72a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())  # This will print the current working directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f985dcd",
   "metadata": {},
   "source": [
    "log_time(\"Part2 -  Distance Calculation\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a155e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in the 'is_fraud' column\n",
    "df['is_fraud'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d464f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud vs Non-Fraud by Merchant Category\n",
    "fraud_by_category = df[df['is_fraud'] == 1]['category'].value_counts().head(10)\n",
    "non_fraud_by_category = df[df['is_fraud'] == 0]['category'].value_counts().head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 categories with the highest fraud counts\n",
    "top_fraud_merchant_categories = df[df['is_fraud'] == 1]['category'].value_counts().head(5).index.tolist()\n",
    "\n",
    "# Print top fraudulent categories\n",
    "print(\"Top Fraudulent Merchant Categories:\", top_fraud_merchant_categories)\n",
    "\n",
    "# Create HighRiskMerchantCategory flag\n",
    "df['HighRiskMerchantCategory'] = df['category'].apply(lambda x: 1 if x in top_fraud_merchant_categories else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the count of 1s and 0s in HighRiskMerchantCategory\n",
    "print(df['HighRiskMerchantCategory'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df499da3",
   "metadata": {},
   "source": [
    "# Potential Additional Features:\n",
    "Transaction Frequency:\n",
    "    Feature: How often a credit card has been used within a specific time frame (e.g., last hour or day).\n",
    "    Why: Fraudsters often make rapid successive transactions within short periods. You could create a rolling window to calculate transaction frequency.\n",
    "    How: You could calculate the number of transactions within the past X hours/days using a rolling window on the TransactionTime feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828d3a0e",
   "metadata": {},
   "source": [
    "#age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure 'DateOfBirth' is in datetime format\n",
    "df['DateOfBirth'] = pd.to_datetime(df['DateOfBirth'], errors='coerce')  # Handle errors during conversion\n",
    "\n",
    "# Step 1: Calculate Age\n",
    "# Calculate age in years\n",
    "df['Age'] = (pd.Timestamp.now() - df['DateOfBirth']).dt.days // 365  # Age in years\n",
    "\n",
    "# Step 2: Create Age Groups\n",
    "# Define age bins and labels\n",
    "bins = [0, 18, 25, 35, 45, 55, 65, 100]  # Define your age bins, ensuring to cover all possible ages\n",
    "labels = ['0-18', '19-25', '26-35', '36-45', '46-55', '56-65', '66+']  # Corresponding labels\n",
    "\n",
    "# Create age group feature, include NaN values handling\n",
    "df['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "\n",
    "# Verify the new features without truncating DataFrame\n",
    "#print(df[['DateOfBirth', 'Age', 'AgeGroup']].head(10))  # Display the first 10 entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time(\"Part3 - Merchant Categories & Age group\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19dc4c6f",
   "metadata": {},
   "source": [
    "RapidTransactionFlag"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e07efaf",
   "metadata": {},
   "source": [
    "def count_transactions_within_last_hour(group):\n",
    "    # Create an empty list to hold the frequencies\n",
    "    frequencies = []\n",
    "    \n",
    "    # Loop over each transaction time in the group\n",
    "    for time in group.index:\n",
    "        # Count the number of transactions within the last hour\n",
    "        count = group[(group.index >= (time - pd.Timedelta(hours=1))) & (group.index <= time)].shape[0]\n",
    "        frequencies.append(count)\n",
    "    \n",
    "    return frequencies\n",
    "\n",
    "# Apply the function to each group\n",
    "df['TransactionFrequency'] = df.groupby('CreditCardNumber').apply(count_transactions_within_last_hour).reset_index(drop=True)\n",
    "print(df[['TransactionFrequency']].head(10))  # Display the first 10 entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc72de",
   "metadata": {},
   "source": [
    "# MULTIPROCESSING : count_transactions_within_last_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "620555a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatetimeIndex' object has no attribute 'expr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Apply the parallel processing for transaction frequency counting\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransactionFrequency\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mparallel_count_transactions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_partitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust num_partitions as needed\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Log the time taken for transaction frequency calculation with multiprocessing\u001b[39;00m\n\u001b[1;32m     34\u001b[0m log_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPart4 - TransactionFrequency Multiprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m, start_time)\n",
      "Cell \u001b[0;32mIn[161], line 13\u001b[0m, in \u001b[0;36mparallel_count_transactions\u001b[0;34m(df, num_partitions)\u001b[0m\n\u001b[1;32m     10\u001b[0m     num_partitions \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mcpu_count()  \u001b[38;5;66;03m# Use all available CPU cores\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Ensure the index is a datetime\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Split the dataframe into chunks based on the number of partitions (CPU cores)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m df_split \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray_split(df, num_partitions)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_collection.py:3065\u001b[0m, in \u001b[0;36mDataFrame.__setattr__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3063\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   3064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3065\u001b[0m     \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/dask_expr/_collection.py:663\u001b[0m, in \u001b[0;36mFrameBase.index\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;129m@index\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindex\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m expr\u001b[38;5;241m.\u001b[39mare_co_aligned(\n\u001b[0;32m--> 663\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr, \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\n\u001b[1;32m    664\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue needs to be aligned with the index\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    665\u001b[0m     _expr \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39mAssignIndex(\u001b[38;5;28mself\u001b[39m, value)\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expr \u001b[38;5;241m=\u001b[39m _expr\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DatetimeIndex' object has no attribute 'expr'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import time\n",
    "from transaction_frequency import process_chunk  # Import from the .py file\n",
    "\n",
    "# Multiprocessing function to parallelize the transaction counting\n",
    "def parallel_count_transactions(df, num_partitions=None):\n",
    "    if num_partitions is None:\n",
    "        num_partitions = mp.cpu_count()  # Use all available CPU cores\n",
    "    \n",
    "    # Ensure the index is a datetime\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Split the dataframe into chunks based on the number of partitions (CPU cores)\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    \n",
    "    # Create a multiprocessing Pool\n",
    "    with mp.Pool(num_partitions) as pool:\n",
    "        # Apply the processing function to each chunk in parallel\n",
    "        result = pool.map(process_chunk, df_split)\n",
    "    \n",
    "    # Combine the results from each chunk into a single series, reset index for consistency\n",
    "    return pd.concat(result).reset_index(drop=True)\n",
    "\n",
    "# Assuming df has 'CreditCardNumber' as a column and transaction times are indexed\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Apply the parallel processing for transaction frequency counting\n",
    "    df['TransactionFrequency'] = parallel_count_transactions(df, num_partitions=4)  # Adjust num_partitions as needed\n",
    "\n",
    "    # Log the time taken for transaction frequency calculation with multiprocessing\n",
    "    log_time(\"Part4 - TransactionFrequency Multiprocessing\", start_time)\n",
    "\n",
    "    # Check the first 10 rows\n",
    "    print(df[['TransactionFrequency']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e571f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = pd.to_datetime(df.index)\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96249ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data to count transactions every hour\n",
    "transaction_counts_hourly = df.resample('H').size()\n",
    "transaction_counts_daily = df.resample('D').size()\n",
    "\n",
    "# Combine with CreditCardNumber if necessary\n",
    "transaction_counts = df.groupby('CreditCardNumber').resample('H').size().reset_index(name='TransactionCount')\n",
    "print(transaction_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c19713",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_transactions = df.groupby('CreditCardNumber').size().reset_index(name='TotalTransactionCount')\n",
    "print(total_transactions.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef71d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the time difference between consecutive transactions\n",
    "time_diff = df.index.to_series().diff().dt.total_seconds()\n",
    "# Flag rapid transactions (within 5 minutes)\n",
    "df['RapidTransactionFlag'] = time_diff < 60  # For a 1-minute threshold\n",
    "\n",
    "# Create a temporary DataFrame for rapid transactions\n",
    "rapid_transactions = df[df['RapidTransactionFlag']]\n",
    "\n",
    "# Group by date and count the number of rapid transactions\n",
    "rapid_transaction_counts = rapid_transactions.groupby(rapid_transactions.index.date).size()\n",
    "print(rapid_transaction_counts)\n",
    "\n",
    "# Get a summary of the rapid transactions\n",
    "rapid_transactions_summary = rapid_transactions.describe()\n",
    "print(rapid_transactions_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e05375",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time(\"Part5 - RapidTransactionFlag\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b200c",
   "metadata": {},
   "source": [
    "Transaction Amount Features:\n",
    "Log Transaction Amount: Normalize the TransactionAmount by taking its logarithm to reduce skewness.\n",
    "Transaction Amount Flags: Create binary flags for high-value transactions (e.g., if TransactionAmount exceeds a certain threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample DataFrame creation\n",
    "# Assume 'df' is your DataFrame and has a 'TransactionAmount' column\n",
    "# df = pd.read_csv('your_data.csv')  # Load your actual data\n",
    "\n",
    "# Step 1: Log Transaction Amount\n",
    "# Calculate the log of TransactionAmount\n",
    "df['LogTransactionAmount'] = np.log1p(df['TransactionAmount'])  # Use log1p for stability with 0 values\n",
    "\n",
    "# Step 2: Create Transaction Amount Flags\n",
    "# Define a threshold for high-value transactions\n",
    "threshold = 100  # Adjust the threshold based on your data context\n",
    "\n",
    "# Create a flag for high-value transactions\n",
    "df['HighValueTransactionFlag'] = df['TransactionAmount'] > threshold\n",
    "\n",
    "# Verify the new features\n",
    "print(df[['TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag']].head(10))  # Display the first 10 entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217bc43",
   "metadata": {},
   "source": [
    "Behavioral Features:\n",
    "Count of Transactions in Last X Days: Count how many transactions have occurred in the last 7, 14, or 30 days.\n",
    "Average Transaction Amount in Last X Days: Calculate the average transaction amount over the same periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aeefda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'TransactionTime' is already set as the index and in datetime format\n",
    "\n",
    "# Step 1: Count of Transactions in Last X Days\n",
    "for days in [7, 14, 30]:\n",
    "    # Sort data by CreditCardNumber and TransactionTime to ensure rolling works properly\n",
    "    df = df.sort_values(by=['CreditCardNumber', 'TransactionTime'])\n",
    "    \n",
    "    # Apply rolling and count the number of transactions for each card\n",
    "    df[f'TransactionCountLast{days}Days'] = (\n",
    "        df.groupby('CreditCardNumber')['CreditCardNumber']\n",
    "        .rolling(f'{days}D')\n",
    "        .count()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Step 2: Average Transaction Amount in Last X Days\n",
    "for days in [7, 14, 30]:\n",
    "    # Sort data by CreditCardNumber and TransactionTime to ensure rolling works properly\n",
    "    df = df.sort_values(by=['CreditCardNumber', 'TransactionTime'])\n",
    "    \n",
    "    # Calculate the average transaction amount for each credit card in the last X days\n",
    "    df[f'AverageTransactionAmountLast{days}Days'] = (\n",
    "        df.groupby('CreditCardNumber')['TransactionAmount']\n",
    "        .rolling(f'{days}D')\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Verify the new features\n",
    "print(df[['TransactionCountLast7Days', 'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
    "           'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast14Days', 'AverageTransactionAmountLast30Days']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)  # Display all columns in the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc639c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time(\"Part6 - TransactionCountLast_X_Days & AverageTrxAmountLast_X_Days\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a24a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b4aaef3",
   "metadata": {},
   "source": [
    "# Graph Construction with NetworkX:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18793de2",
   "metadata": {},
   "source": [
    "Highlight Fraudulent Nodes: Overlay of fraudulent and non-fraudulent credit cards on this degree distribution to see if there’s a difference in their degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26bd1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges between credit cards and merchants, including transaction amount as an edge attribute\n",
    "for idx, row in df.iterrows():\n",
    "    credit_card = str(row['CreditCardNumber'])\n",
    "    merchant = str(row['merchant'])\n",
    "    transaction_amount = row['TransactionAmount']  # Ensure TransactionAmount exists in your dataframe\n",
    "    \n",
    "    # Add an edge with the transaction amount as an attribute\n",
    "    G.add_edge(credit_card, merchant, transaction_amount=transaction_amount)\n",
    "\n",
    "\n",
    "# Calculate degrees for all nodes in the graph\n",
    "degrees = dict(G.degree())\n",
    "\n",
    "# Filter degrees for credit cards and merchants\n",
    "credit_card_nodes = df['CreditCardNumber'].astype(str).unique()\n",
    "merchant_nodes = df['merchant'].astype(str).unique()\n",
    "\n",
    "credit_card_degrees = {node: degrees[node] for node in credit_card_nodes if node in degrees}\n",
    "merchant_degrees = {node: degrees[node] for node in merchant_nodes if node in degrees}\n",
    "\n",
    "# Debugging: Print counts to ensure correctness\n",
    "print(f\"Number of unique credit card nodes: {len(credit_card_nodes)}\")\n",
    "print(f\"Number of unique merchant nodes: {len(merchant_nodes)}\")\n",
    "print(f\"Number of credit card nodes with degrees: {len(credit_card_degrees)}\")\n",
    "print(f\"Number of merchant nodes with degrees: {len(merchant_degrees)}\")\n",
    "\n",
    "# Create a new DataFrame for easier plotting\n",
    "degree_df = pd.DataFrame({\n",
    "    'CreditCardDegree': pd.Series(credit_card_degrees),\n",
    "    'MerchantDegree': pd.Series(merchant_degrees)\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55584fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add degree information back to the original DataFrame\n",
    "df['degree'] = df['CreditCardNumber'].astype(str).map(credit_card_degrees)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check edges and their attributes\n",
    "#for edge in G.edges(data=True):\n",
    "#    print(edge)\n",
    "\n",
    "#do NOT print this, huge list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c525e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CreditCardNumber'] = df['CreditCardNumber'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_mapping = df.set_index('CreditCardNumber')['is_fraud'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e261bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time(\"Part7 - NetworkX Start Step\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa04d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(fraud_mapping.head(5))\n",
    "#only testing purposes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "167571cb",
   "metadata": {},
   "source": [
    "betweenness_centrality"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0093ce9f",
   "metadata": {},
   "source": [
    "# Calculate betweenness centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac75df",
   "metadata": {},
   "source": [
    "# MULTIPROCESSING : betweenness_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d69fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import time\n",
    "from networkx_graph_betweeness_centrality import parallel_betweenness_centrality\n",
    "\n",
    "# Assuming G is your graph\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate betweenness centrality using parallel processing\n",
    "    betweenness_centrality = parallel_betweenness_centrality(G, num_partitions=4)  # Adjust number of cores if needed\n",
    "\n",
    "    # Log the time taken for betweenness centrality calculation with multiprocessing\n",
    "    log_time(\"Part8 - Betweenness Centrality Calculation with Multiprocessing\", start_time)\n",
    "\n",
    "    # Check a few centrality values\n",
    "    print(list(betweenness_centrality.items())[:10])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e7b4462",
   "metadata": {},
   "source": [
    "import os\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['betweenness_centrality'] = df['CreditCardNumber'].map(betweenness_centrality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8bf772",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['betweenness_centrality'].describe())\n",
    "print(df['betweenness_centrality'].isna().sum())  # Check for missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check betweenness centrality for specific credit card numbers\n",
    "sample_nodes = ['60416207185', 'fraud_Kutch-Ferry']  # Replace with actual nodes\n",
    "for node in sample_nodes:\n",
    "    print(f\"{node}: {betweenness_centrality.get(node)}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a33a0b35",
   "metadata": {},
   "source": [
    "log_time(\"Part8 - Feature Engineering -- NetworkX betweenness_centrality\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439124c6",
   "metadata": {},
   "source": [
    "1. Investigate Nodes with High Betweenness Centrality:\n",
    "\n",
    "Now that you’ve visualized nodes with high betweenness centrality, you can:\n",
    "\n",
    "    Examine if fraudulent nodes tend to have high betweenness centrality. This might indicate that these nodes are acting as \"connectors\" between different parts of the network, which could be a sign of suspicious behavior.\n",
    "    Compare centrality between fraud and non-fraud nodes to see if there's a pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13fdd8e",
   "metadata": {},
   "source": [
    "2. Visualize Communities in the Network:\n",
    "\n",
    "You could apply community detection to uncover fraud rings or clusters of merchants targeted by fraudsters. The Louvain algorithm is great for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f148a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community.community_louvain as community_louvain\n",
    "\n",
    "\n",
    "# Apply Louvain method for community detection\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee872fb",
   "metadata": {},
   "source": [
    "Fraud Node Highlighting:\n",
    "\n",
    "    Fraudulent nodes (from df['is_fraud'] == 1) are colored red to make them stand out. The rest of the nodes are still colored based on their communities.\n",
    "    This should help you easily spot any fraudulent nodes in the network.\n",
    "\n",
    "Top 10 Most Central Nodes:\n",
    "\n",
    "    We calculate betweenness centrality and extract the top 10 most central nodes.\n",
    "    These nodes are visualized with their connections, which should help declutter the graph and focus on the key players in the transaction network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd26394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply Louvain method for community detection\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "# Create positions for nodes using a spring layout\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Add the community information to the DataFrame\n",
    "df['community'] = df['CreditCardNumber'].map(partition)\n",
    "\n",
    "# Highlight fraud nodes separately\n",
    "fraud_nodes = df[df['is_fraud'] == 1]['CreditCardNumber'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95613930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the community information to the DataFrame\n",
    "df['community'] = df['CreditCardNumber'].map(partition)\n",
    "\n",
    "# Calculate the percentage of fraud in each community\n",
    "community_fraud = df.groupby('community')['is_fraud'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9407a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print fraud rate per community\n",
    "print(community_fraud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17572504",
   "metadata": {},
   "outputs": [],
   "source": [
    "community_size = df.groupby('community').size()\n",
    "print(community_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cca311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fraud rates and community sizes into a single DataFrame\n",
    "fraud_vs_size = pd.concat([community_fraud, df.groupby('community').size()], axis=1)\n",
    "fraud_vs_size.columns = ['FraudRate', 'CommunitySize']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accbc68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_fraud_communities = community_fraud.sort_values(ascending=False).head(5)\n",
    "print(top_fraud_communities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a875aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the community labels of the top fraud communities\n",
    "top_community_labels = top_fraud_communities.index.tolist()\n",
    "\n",
    "# Filter the DataFrame for only the top fraud communities\n",
    "top_communities_df = df[df['community'].isin(top_community_labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0bb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Only the Top Merchants by Fraud Rate:\n",
    "# Instead of displaying all merchants, you can filter the plot to show only the top 10 or 20 merchants with the highest fraud rates.\n",
    "\n",
    "# Calculate fraud rate by merchant in the top fraud communities\n",
    "merchant_fraud_rate = top_communities_df.groupby('merchant')['is_fraud'].mean()\n",
    "\n",
    "# Sort merchants by fraud rate in descending order\n",
    "top_merchants = merchant_fraud_rate.sort_values(ascending=False).head(10)\n",
    "\n",
    "# Print top 10 merchants with highest fraud rate\n",
    "print(top_merchants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'category' is a column representing merchant categories\n",
    "merchantcategory_fraud = top_communities_df.groupby('category')['is_fraud'].mean()\n",
    "\n",
    "# Sort the fraud rate by merchant category in descending order\n",
    "merchantcategory_fraud_sorted = merchantcategory_fraud.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_time(\"Part9 - Community & Top Merchants\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154aa31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the density of the graph (a measure of sparsity)\n",
    "density = nx.density(G)\n",
    "print(f\"Graph Density: {density}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e360c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the average degree\n",
    "degree_sequence = [degree for node, degree in G.degree()]\n",
    "average_degree = sum(degree_sequence) / len(degree_sequence)\n",
    "print(f\"Average Degree of Nodes: {average_degree}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa3b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time(\"Part10 - Density\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6bcacc",
   "metadata": {},
   "source": [
    "The results you’ve provided show:\n",
    "\n",
    "    Graph Density: 0.2513\n",
    "        This is a moderate density value. A density of 0 would indicate a completely disconnected graph, while a value close to 1 would indicate a very tightly connected graph (like a clique). A density of 0.25 means about 25% of the possible connections between nodes are present, which suggests the graph isn’t overly sparse, but it’s not densely connected either.\n",
    "\n",
    "    Average Degree: 406.18\n",
    "        This is relatively high, meaning that, on average, each node (credit card or merchant) is connected to about 406 other nodes. This high degree could indicate that nodes, especially credit cards, are interacting with many merchants. However, these connections are likely not forming closed loops or triangles, which is why the clustering coefficient is zero for all nodes.\n",
    "\n",
    "What This Means:\n",
    "\n",
    "    Even though the average degree is high, suggesting that credit cards are interacting with many merchants, the interactions are likely not forming triangles (where connected nodes are also connected to each other). This results in zero clustering coefficients across the board.\n",
    "\n",
    "    The moderate graph density indicates that the network is connected to some extent, but not densely enough to produce high clustering coefficients.\n",
    "\n",
    "Why This Happens:\n",
    "\n",
    "In transaction networks, it’s common for credit cards to interact with different merchants, but merchants don’t typically transact with each other, which means closed triangles (required for a non-zero clustering coefficient) are rare. In fraud detection, this is normal, as fraudsters typically transact with many distinct merchants rather than creating highly connected communities.\n",
    "Next Steps:\n",
    "\n",
    "Given the moderate density and high degree of the nodes, the clustering coefficient might not be the most insightful metric. Instead, you could focus on the following:\n",
    "1. Focus on Betweenness Centrality and Degree:\n",
    "\n",
    "These metrics are more likely to highlight key nodes (e.g., credit cards or merchants) that are crucial in the transaction network. You’ve already calculated betweenness centrality, and the average degree indicates that some credit cards or merchants might have a significant number of connections.\n",
    "2. Look for Key Nodes (High Degree or Centrality):\n",
    "\n",
    "You could identify nodes with high degree or betweenness centrality to see if they’re involved in fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452804a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c450de",
   "metadata": {},
   "source": [
    "Given that you only identified 4 nodes with the lowest betweenness centrality, and they are all disconnected, it seems that focusing on these low-centrality nodes isn’t providing much value for your analysis. This could indicate that these nodes (credit card numbers or merchants) are peripheral and not involved in significant patterns of interaction, and therefore, might not contribute meaningful insights for detecting fraud.\n",
    "Should You Continue with Low Betweenness Centrality Nodes?\n",
    "\n",
    "    Disconnected Nodes: Since the nodes with the lowest betweenness centrality are disconnected and few in number, they don't seem to play a crucial role in the transaction network.\n",
    "    Low Utility: If these nodes don't show fraud or aren't involved in key transactions, they might not be useful for your model or analysis.\n",
    "\n",
    "What to Do Next:\n",
    "\n",
    "    Abandon the Focus on Low Betweenness Centrality:\n",
    "        Since these nodes are disconnected and don't seem to offer useful insights, it might be better to abandon the focus on low betweenness centrality.\n",
    "        Instead, focus on nodes with more centrality (betweenness, degree, etc.) or explore other graph features.\n",
    "\n",
    "    Explore Other Graph Metrics:\n",
    "\n",
    "        You can shift your focus to more meaningful metrics such as pagerank or eigenvector centrality, which may reveal more about the influence or importance of nodes in the network.\n",
    "\n",
    "        Here's how you can calculate pagerank and analyze it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag',\n",
    "    'TransactionCountLast7Days', 'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
    "    'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast14Days', 'AverageTransactionAmountLast30Days',\n",
    "    'Hour', 'HighRiskHour', 'DayOfWeek', 'IsWeekend', 'TransactionFrequency', 'RapidTransactionFlag',\n",
    "    'lat', 'long', 'merch_lat', 'merch_long', 'distance', 'city_pop',\n",
    "    'Age', 'AgeGroup', 'gender', 'state', 'city',\n",
    "    'degree', 'betweenness_centrality', 'community'\n",
    "]\n",
    "\n",
    "df_selected_features = df[selected_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f21f1e",
   "metadata": {},
   "source": [
    "# Page rank as new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fda32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PageRank for each node in the graph\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "# Map the PageRank values to the 'CreditCardNumber' in the DataFrame\n",
    "df['pagerank'] = df['CreditCardNumber'].map(pagerank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53596e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in the pagerank column\n",
    "print(df['pagerank'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e2620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check descriptive statistics of pagerank values\n",
    "print(df['pagerank'].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many nodes have a PageRank of zero\n",
    "zero_pagerank_count = (df['pagerank'] == 0).sum()\n",
    "print(f\"Number of nodes with zero PageRank: {zero_pagerank_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97f80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the average PageRank for fraud and non-fraud transactions\n",
    "fraud_avg_pagerank = df[df['is_fraud'] == 1]['pagerank'].mean()\n",
    "non_fraud_avg_pagerank = df[df['is_fraud'] == 0]['pagerank'].mean()\n",
    "\n",
    "print(f\"Average PageRank for Fraud: {fraud_avg_pagerank}\")\n",
    "print(f\"Average PageRank for Non-Fraud: {non_fraud_avg_pagerank}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47121274",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features.append('pagerank')\n",
    "df_selected_features = df[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_selected_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time(\"Part11 - PageRank\", start_time)\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17cecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56237852",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time(\"END - Feature Engineering .....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac908b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163f30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time(\"START - Random Forest with DASK .....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14312bfe",
   "metadata": {},
   "source": [
    "Close the dASK Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0843139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Start Dask client\n",
    "client = Client()\n",
    "\n",
    "# Define directory paths and other configurations\n",
    "reports_output_dir = \"/path/to/reports\"  # Define the directory to save reports\n",
    "dataset_type = \"train\" if not use_test_data else \"test\"  # Set based on evaluation type\n",
    "\n",
    "# Load and preprocess function for both train and test datasets\n",
    "def load_and_preprocess(data_source):\n",
    "    X = df[selected_features]\n",
    "    y = df['is_fraud']\n",
    "    \n",
    "    # Convert to Dask, categorize, one-hot encode, then back to Pandas\n",
    "    X = dd.from_pandas(X, npartitions=5).categorize()\n",
    "    X = dd.get_dummies(X, drop_first=True).compute()  # Convert Dask DataFrame to Pandas\n",
    "    y = dd.from_pandas(y, npartitions=5).compute()  # Convert Dask Series to Pandas\n",
    "    return X, y\n",
    "\n",
    "# Model training and evaluation\n",
    "if use_test_data:\n",
    "    # Load and preprocess test data\n",
    "    X_test, y_test = load_and_preprocess(f\"{input_src_dir}/fraudTest.csv\")\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(\"Accuracy on test data:\", test_accuracy)\n",
    "    \n",
    "    # Generate classification report for test data\n",
    "    clf_report = classification_report(y_test, y_test_pred)\n",
    "    print(\"Classification Report: \" , clf_report)\n",
    "    \n",
    "else:\n",
    "    # Load and preprocess train data\n",
    "    X_train, y_train = load_and_preprocess(df)\n",
    "\n",
    "    # Train Random Forest model on train data\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on train data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    print(\"Accuracy on train data:\", train_accuracy)\n",
    "    \n",
    "    # Generate classification report for train data\n",
    "    clf_report = classification_report(y_train, y_train_pred)\n",
    "    print(\"Classification Report: \", clf_report)\n",
    "    accuracy = train_accuracy if not use_test_data else test_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8966196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# Calculate AUC-ROC\n",
    "roc_auc = roc_auc_score(y_train, y_train_pred)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n",
    "\n",
    "# Calculate Precision-Recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_train_pred)\n",
    "\n",
    "# Calculate the area under the Precision-Recall curve\n",
    "pr_auc = auc(recall, precision)\n",
    "print(\"Precision-Recall AUC:\", pr_auc)\n",
    "\n",
    "# Display key metrics\n",
    "print(\"Precision at various thresholds:\", precision)\n",
    "print(\"Recall at various thresholds:\", recall)\n",
    "\n",
    "# You can plot these metrics for a clearer understanding\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, marker='.', label='Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# Define the title\n",
    "title = 'Random Forest with DASK - ROC Curve for Test Set'\n",
    "plt.title(title)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Use the title in the filename\n",
    "filename = f\"{dataset_type}_{title.replace(' ', '_').replace(',', '').lower()}.png\"\n",
    "plt.savefig(os.path.join(reports_output_dir, filename), dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the full report content\n",
    "full_report = f\"Accuracy: {accuracy:.4f}\\n\\n{clf_report}\"\n",
    "print(full_report)\n",
    "\n",
    "# Define the report title and output filename\n",
    "classification_report_title = 'Random Forest with DASK Model Classification Report'\n",
    "classification_report_outputfilename = f\"{model_specs}_{dataset_type}_{classification_report_title.replace(' ', '_').replace(',', '').lower()}.txt\"\n",
    "\n",
    "# Save the classification report with accuracy to a text file\n",
    "with open(os.path.join(reports_output_dir, classification_report_outputfilename), 'w') as f:\n",
    "    f.write(full_report)  # Writing the combined report string to the file\n",
    "\n",
    "print(f\"Classification report saved to: {os.path.join(reports_output_dir, classification_report_outputfilename)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d003a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time(\"END - Random Forest with DASK Model .....  \", start_time)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d9134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Close Dask client when done\n",
    "client.close()\n",
    "log_time(\"Closed the DASK Client\", start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4713ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Specify the output directory\n",
    "output_dir_model = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/models'\n",
    "if not os.path.exists(output_dir_model):\n",
    "    os.makedirs(output_dir_model)  # Ensure the directory exists\n",
    "\n",
    "# Create a dynamic filename with .pkl extension\n",
    "model_title = 'model'\n",
    "outputfilename_model = f\"{model_specs}_{model_title.replace(' ', '_').replace(',', '').lower()}.pkl\"\n",
    "\n",
    "# Save the best model obtained from GridSearchCV\n",
    "with open(os.path.join(output_dir_model, outputfilename_model), 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "print(f\"Model saved to {os.path.join(output_dir_model, outputfilename_model)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d57811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Assuming start_time is defined earlier in the notebook\n",
    "end_time_notebook = time.time()\n",
    "elapsed_time = end_time_notebook - start_time_notebook\n",
    "\n",
    "# Print and format the notebook end time and total execution time\n",
    "print(f\"Notebook ended at: {time.ctime(end_time_notebook)}\")\n",
    "print(f\"Total execution time: {elapsed_time // 60:.0f} minutes and {elapsed_time % 60:.2f} seconds\")\n",
    "\n",
    "\n",
    "log_time(f\"{model_specs}_{dataset_type} Notebook Ended at... \", start_time_notebook)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d8d0f14",
   "metadata": {},
   "source": [
    "Results reveal the following:\n",
    "\n",
    "    ROC AUC Score: 0.70, indicating moderate model performance in distinguishing between fraud and non-fraud cases. However, this score shows room for improvement, especially given the critical nature of detecting fraud.\n",
    "\n",
    "    Precision-Recall AUC: 0.70, which aligns with the ROC AUC score. This metric is helpful here as it focuses on the model's performance with regard to the minority class (fraud).\n",
    "\n",
    "    Precision at Various Thresholds: These values suggest high precision at some thresholds, but with a trade-off in recall (detecting fewer fraud cases).\n",
    "        When precision is 1.0, recall drops to 0, meaning the model makes fewer false positive fraud predictions but at the cost of missing actual fraud cases.\n",
    "\n",
    "    Recall at Various Thresholds: The recall values indicate that the model captures only a fraction (about 40%) of actual fraud cases at one threshold, suggesting the need to optimize recall.\n",
    "\n",
    "Suggested Next Steps to Improve Performance:\n",
    "\n",
    "    Adjust Class Weights:\n",
    "        Set class_weight='balanced' in the RandomForestClassifier to help the model pay more attention to fraud cases.\n",
    "\n",
    "    Hyperparameter Tuning:\n",
    "        Experiment with more in-depth hyperparameter tuning, such as adjusting n_estimators, max_depth, and min_samples_split, which may improve the model's performance on fraud cases.\n",
    "\n",
    "    Try a Different Algorithm:\n",
    "        Consider alternative algorithms that may handle class imbalance better, such as Gradient Boosting or XGBoost, which often yield better results in fraud detection.\n",
    "\n",
    "    Custom Threshold Selection:\n",
    "        Use the Precision-Recall curve to determine an optimal classification threshold that balances precision and recall for fraud detection. This approach allows you to improve recall without compromising precision too much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ZHAW_Project)",
   "language": "python",
   "name": "zhaw_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
