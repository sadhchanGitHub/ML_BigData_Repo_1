{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad0bc7a2",
   "metadata": {},
   "source": [
    "# Credit_Card_Fraud_Detection_LogisticRegressionBalanced_CAS_Machine_Intelligence_Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e22053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import geopy\n",
    "from geopy.distance import geodesic\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f710904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook started at: Sun Oct 27 17:53:30 2024\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print(f\"Notebook started at: {time.ctime(start_time)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257eecc3",
   "metadata": {},
   "source": [
    "\n",
    "'''\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf0d2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which dataset to use\n",
    "use_test_data = False  # Set to True when using fraudtest.csv\n",
    "\n",
    "# Determine dataset type based on the variable\n",
    "dataset_type = 'Test' if use_test_data else 'Train'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c56c046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save the figures \n",
    "\n",
    "input_src_dir = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/data/raw'\n",
    "output_dir_figures_train = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/figures/train_figures'\n",
    "output_dir_figures_test = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports/figures/test_figures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23112f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the appropriate dataset\n",
    "\n",
    "if use_test_data:\n",
    "    output_dir_figures = output_dir_figures_test\n",
    "else:\n",
    "    output_dir_figures = output_dir_figures_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99a06471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the appropriate dataset\n",
    "\n",
    "if use_test_data:\n",
    "    df = pd.read_csv(f\"{input_src_dir}/fraudTest.csv\")  # Concatenate the directory with the filename\n",
    "else:\n",
    "    df = pd.read_csv(f\"{input_src_dir}/fraudTrain.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a11b824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category',\n",
      "       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n",
      "       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n",
      "       'merch_lat', 'merch_long', 'is_fraud'],\n",
      "      dtype='object')\n",
      "(1296675, 23)\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81b8283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23ef6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'amt': 'TransactionAmount', 'cc_num': 'CreditCardNumber', 'dob': 'DateOfBirth', 'trans_date_trans_time': 'TransactionTime'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8339c8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a7c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique TransactionID for each row\n",
    "df['TransactionID'] = range(1, len(df) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c847926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0      TransactionTime  CreditCardNumber  \\\n",
      "0           0  2019-01-01 00:00:18  2703186189652095   \n",
      "1           1  2019-01-01 00:00:44      630423337322   \n",
      "2           2  2019-01-01 00:00:51    38859492057661   \n",
      "3           3  2019-01-01 00:01:16  3534093764340240   \n",
      "4           4  2019-01-01 00:03:06   375534208663984   \n",
      "\n",
      "                             merchant       category  TransactionAmount  \\\n",
      "0          fraud_Rippin, Kub and Mann       misc_net               4.97   \n",
      "1     fraud_Heller, Gutmann and Zieme    grocery_pos             107.23   \n",
      "2                fraud_Lind-Buckridge  entertainment             220.11   \n",
      "3  fraud_Kutch, Hermiston and Farrell  gas_transport              45.00   \n",
      "4                 fraud_Keeling-Crist       misc_pos              41.96   \n",
      "\n",
      "       first     last gender                        street  ...      long  \\\n",
      "0   Jennifer    Banks      F                561 Perry Cove  ...  -81.1781   \n",
      "1  Stephanie     Gill      F  43039 Riley Greens Suite 393  ... -118.2105   \n",
      "2     Edward  Sanchez      M      594 White Dale Suite 530  ... -112.2620   \n",
      "3     Jeremy    White      M   9443 Cynthia Court Apt. 038  ... -112.1138   \n",
      "4      Tyler   Garcia      M              408 Bradley Rest  ...  -79.4629   \n",
      "\n",
      "  city_pop                                job  DateOfBirth  \\\n",
      "0     3495          Psychologist, counselling   1988-03-09   \n",
      "1      149  Special educational needs teacher   1978-06-21   \n",
      "2     4154        Nature conservation officer   1962-01-19   \n",
      "3     1939                    Patent attorney   1967-01-12   \n",
      "4       99     Dance movement psychotherapist   1986-03-28   \n",
      "\n",
      "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
      "0  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315   \n",
      "1  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462   \n",
      "2  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481   \n",
      "3  6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071   \n",
      "4  a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459   \n",
      "\n",
      "  is_fraud  TransactionID  \n",
      "0        0              1  \n",
      "1        0              2  \n",
      "2        0              3  \n",
      "3        0              4  \n",
      "4        0              5  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop the 'Unnamed: 0' column\n",
    "#df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Verify it's gone\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3babc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " Unnamed: 0           0\n",
      "TransactionTime      0\n",
      "CreditCardNumber     0\n",
      "merchant             0\n",
      "category             0\n",
      "TransactionAmount    0\n",
      "first                0\n",
      "last                 0\n",
      "gender               0\n",
      "street               0\n",
      "city                 0\n",
      "state                0\n",
      "zip                  0\n",
      "lat                  0\n",
      "long                 0\n",
      "city_pop             0\n",
      "job                  0\n",
      "DateOfBirth          0\n",
      "trans_num            0\n",
      "unix_time            0\n",
      "merch_lat            0\n",
      "merch_long           0\n",
      "is_fraud             0\n",
      "TransactionID        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values)\n",
    "\n",
    "# no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dc29333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_fraud\n",
      "0    1289169\n",
      "1       7506\n",
      "Name: count, dtype: int64\n",
      "is_fraud\n",
      "0    99.421135\n",
      "1     0.578865\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Count of fraud and non-fraud transactions\n",
    "fraud_counts = df['is_fraud'].value_counts()\n",
    "print(fraud_counts)\n",
    "\n",
    "# Optionally, you can get it in percentage terms\n",
    "fraud_percentage = df['is_fraud'].value_counts(normalize=True) * 100\n",
    "print(fraud_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c60dd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many unique credit cards in the data set ??\n",
    "df['CreditCardNumber'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "391577c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'TransactionTime', 'CreditCardNumber', 'merchant',\n",
      "       'category', 'TransactionAmount', 'first', 'last', 'gender', 'street',\n",
      "       'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75b3c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TransactionTime to datetime\n",
    "df['TransactionTime'] = pd.to_datetime(df['TransactionTime'])\n",
    "\n",
    "# Optional: Convert DateOfBirth to datetime, if needed\n",
    "df['DateOfBirth'] = pd.to_datetime(df['DateOfBirth'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52941f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-01 00:00:18', '2019-01-01 00:00:44',\n",
      "               '2019-01-01 00:00:51', '2019-01-01 00:01:16',\n",
      "               '2019-01-01 00:03:06', '2019-01-01 00:04:08',\n",
      "               '2019-01-01 00:04:42', '2019-01-01 00:05:08',\n",
      "               '2019-01-01 00:05:18', '2019-01-01 00:06:01',\n",
      "               ...\n",
      "               '2020-06-21 12:08:42', '2020-06-21 12:09:22',\n",
      "               '2020-06-21 12:10:56', '2020-06-21 12:11:23',\n",
      "               '2020-06-21 12:11:36', '2020-06-21 12:12:08',\n",
      "               '2020-06-21 12:12:19', '2020-06-21 12:12:32',\n",
      "               '2020-06-21 12:13:36', '2020-06-21 12:13:37'],\n",
      "              dtype='datetime64[ns]', name='TransactionTime', length=1296675, freq=None)\n"
     ]
    }
   ],
   "source": [
    "# Set 'TransactionTime' as the index permanently\n",
    "df.set_index('TransactionTime', inplace=True)\n",
    "\n",
    "# Verify the index\n",
    "print(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55397673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Transaction Time: 2019-01-01 00:00:18\n",
      "Maximum Transaction Time: 2020-06-21 12:13:37\n"
     ]
    }
   ],
   "source": [
    "# Get the minimum and maximum transaction times from the index\n",
    "min_time = df.index.min()\n",
    "max_time = df.index.max()\n",
    "\n",
    "print(f\"Minimum Transaction Time: {min_time}\")\n",
    "print(f\"Maximum Transaction Time: {max_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1fc7ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CreditCardNumber</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>TransactionAmount</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>DateOfBirth</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>TransactionID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:18</th>\n",
       "      <td>0</td>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>Moravian Falls</td>\n",
       "      <td>...</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:44</th>\n",
       "      <td>1</td>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>Orient</td>\n",
       "      <td>...</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:51</th>\n",
       "      <td>2</td>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>Malad City</td>\n",
       "      <td>...</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:01:16</th>\n",
       "      <td>3</td>\n",
       "      <td>3534093764340240</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>9443 Cynthia Court Apt. 038</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>...</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>1939</td>\n",
       "      <td>Patent attorney</td>\n",
       "      <td>1967-01-12</td>\n",
       "      <td>6b849c168bdad6f867558c3793159a81</td>\n",
       "      <td>1325376076</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:03:06</th>\n",
       "      <td>4</td>\n",
       "      <td>375534208663984</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>Garcia</td>\n",
       "      <td>M</td>\n",
       "      <td>408 Bradley Rest</td>\n",
       "      <td>Doe Hill</td>\n",
       "      <td>...</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>99</td>\n",
       "      <td>Dance movement psychotherapist</td>\n",
       "      <td>1986-03-28</td>\n",
       "      <td>a41d7549acf90789359a9aa5346dcb46</td>\n",
       "      <td>1325376186</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Unnamed: 0  CreditCardNumber  \\\n",
       "TransactionTime                                     \n",
       "2019-01-01 00:00:18           0  2703186189652095   \n",
       "2019-01-01 00:00:44           1      630423337322   \n",
       "2019-01-01 00:00:51           2    38859492057661   \n",
       "2019-01-01 00:01:16           3  3534093764340240   \n",
       "2019-01-01 00:03:06           4   375534208663984   \n",
       "\n",
       "                                               merchant       category  \\\n",
       "TransactionTime                                                          \n",
       "2019-01-01 00:00:18          fraud_Rippin, Kub and Mann       misc_net   \n",
       "2019-01-01 00:00:44     fraud_Heller, Gutmann and Zieme    grocery_pos   \n",
       "2019-01-01 00:00:51                fraud_Lind-Buckridge  entertainment   \n",
       "2019-01-01 00:01:16  fraud_Kutch, Hermiston and Farrell  gas_transport   \n",
       "2019-01-01 00:03:06                 fraud_Keeling-Crist       misc_pos   \n",
       "\n",
       "                     TransactionAmount      first     last gender  \\\n",
       "TransactionTime                                                     \n",
       "2019-01-01 00:00:18               4.97   Jennifer    Banks      F   \n",
       "2019-01-01 00:00:44             107.23  Stephanie     Gill      F   \n",
       "2019-01-01 00:00:51             220.11     Edward  Sanchez      M   \n",
       "2019-01-01 00:01:16              45.00     Jeremy    White      M   \n",
       "2019-01-01 00:03:06              41.96      Tyler   Garcia      M   \n",
       "\n",
       "                                           street            city  ...  \\\n",
       "TransactionTime                                                    ...   \n",
       "2019-01-01 00:00:18                561 Perry Cove  Moravian Falls  ...   \n",
       "2019-01-01 00:00:44  43039 Riley Greens Suite 393          Orient  ...   \n",
       "2019-01-01 00:00:51      594 White Dale Suite 530      Malad City  ...   \n",
       "2019-01-01 00:01:16   9443 Cynthia Court Apt. 038         Boulder  ...   \n",
       "2019-01-01 00:03:06              408 Bradley Rest        Doe Hill  ...   \n",
       "\n",
       "                         long  city_pop                                job  \\\n",
       "TransactionTime                                                              \n",
       "2019-01-01 00:00:18  -81.1781      3495          Psychologist, counselling   \n",
       "2019-01-01 00:00:44 -118.2105       149  Special educational needs teacher   \n",
       "2019-01-01 00:00:51 -112.2620      4154        Nature conservation officer   \n",
       "2019-01-01 00:01:16 -112.1138      1939                    Patent attorney   \n",
       "2019-01-01 00:03:06  -79.4629        99     Dance movement psychotherapist   \n",
       "\n",
       "                     DateOfBirth                         trans_num  \\\n",
       "TransactionTime                                                      \n",
       "2019-01-01 00:00:18   1988-03-09  0b242abb623afc578575680df30655b9   \n",
       "2019-01-01 00:00:44   1978-06-21  1f76529f8574734946361c461b024d99   \n",
       "2019-01-01 00:00:51   1962-01-19  a1a22d70485983eac12b5b88dad1cf95   \n",
       "2019-01-01 00:01:16   1967-01-12  6b849c168bdad6f867558c3793159a81   \n",
       "2019-01-01 00:03:06   1986-03-28  a41d7549acf90789359a9aa5346dcb46   \n",
       "\n",
       "                      unix_time  merch_lat  merch_long  is_fraud  \\\n",
       "TransactionTime                                                    \n",
       "2019-01-01 00:00:18  1325376018  36.011293  -82.048315         0   \n",
       "2019-01-01 00:00:44  1325376044  49.159047 -118.186462         0   \n",
       "2019-01-01 00:00:51  1325376051  43.150704 -112.154481         0   \n",
       "2019-01-01 00:01:16  1325376076  47.034331 -112.561071         0   \n",
       "2019-01-01 00:03:06  1325376186  38.674999  -78.632459         0   \n",
       "\n",
       "                     TransactionID  \n",
       "TransactionTime                     \n",
       "2019-01-01 00:00:18              1  \n",
       "2019-01-01 00:00:44              2  \n",
       "2019-01-01 00:00:51              3  \n",
       "2019-01-01 00:01:16              4  \n",
       "2019-01-01 00:03:06              5  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10777762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b32d0d",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca771ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31252f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clip outliers if necessary\n",
    "df['TransactionAmount'] = df['TransactionAmount'].clip(upper=df['TransactionAmount'].quantile(0.99))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf9e3c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_9644/1075701793.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['TransactionAmount'].replace([np.inf, -np.inf], np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace inf values with NaN (in case they exist in the 'TransactionAmount' column)\n",
    "df['TransactionAmount'].replace([np.inf, -np.inf], np.nan, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b21308e",
   "metadata": {},
   "source": [
    "# next type of VIZ via transaction id vs transaction count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70594f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour from TransactionTime\n",
    "df['Hour'] = df.index.hour  # Since TransactionTime is already set as the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea729e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Risk Hours: [22, 23, 1, 0, 2, 3]\n",
      "                     Hour  HighRiskHour\n",
      "TransactionTime                        \n",
      "2019-01-01 00:00:18     0             1\n",
      "2019-01-01 00:00:44     0             1\n",
      "2019-01-01 00:00:51     0             1\n",
      "2019-01-01 00:01:16     0             1\n",
      "2019-01-01 00:03:06     0             1\n",
      "...                   ...           ...\n",
      "2020-06-21 12:12:08    12             0\n",
      "2020-06-21 12:12:19    12             0\n",
      "2020-06-21 12:12:32    12             0\n",
      "2020-06-21 12:13:36    12             0\n",
      "2020-06-21 12:13:37    12             0\n",
      "\n",
      "[1296675 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate fraud rate by hour\n",
    "fraud_rate_by_hour = df.groupby('Hour')['is_fraud'].mean()\n",
    "\n",
    "# Sort by fraud rate in descending order\n",
    "fraud_rate_by_hour = fraud_rate_by_hour.sort_values(ascending=False)\n",
    "\n",
    "# Define a threshold for high-risk hours (adjust as needed)\n",
    "threshold = fraud_rate_by_hour.mean()  # Mean fraud rate across all hours\n",
    "\n",
    "# Dynamically identify high-risk hours based on the threshold\n",
    "high_risk_hours = fraud_rate_by_hour[fraud_rate_by_hour > threshold].index.tolist()\n",
    "\n",
    "# Print high-risk hours for reference\n",
    "print(\"High-Risk Hours:\", high_risk_hours)\n",
    "\n",
    "# Create the HighRiskHour flag based on dynamically identified high-risk hours\n",
    "df['HighRiskHour'] = df['Hour'].apply(lambda x: 1 if x in high_risk_hours else 0)\n",
    "\n",
    "# Print a sample of the DataFrame to verify the new column\n",
    "print(df[['Hour', 'HighRiskHour']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4babf06",
   "metadata": {},
   "source": [
    "# Further analysis on data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75736fa",
   "metadata": {},
   "source": [
    "1. Time-Based Analysis:\n",
    "Already explored daily and hourly trends in transaction volumes, but now dive deeper into fraud patterns based on time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a61c0eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Weekday vs. Weekend: Is fraud more common on weekdays or weekends?\n",
    "df['DayOfWeek'] = df.index.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "fraud_by_day = df[df['is_fraud'] == 1]['DayOfWeek'].value_counts().sort_index()\n",
    "non_fraud_by_day = df[df['is_fraud'] == 0]['DayOfWeek'].value_counts().sort_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0d77fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the correct day order\n",
    "day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "\n",
    "\n",
    "df['DayName'] = df.index.day_name()\n",
    "# Convert the 'DayName' column to a categorical type with the correct order\n",
    "df['DayName'] = pd.Categorical(df['DayName'], categories=day_order, ordered=True)\n",
    "\n",
    "fraud_by_day = df[df['is_fraud'] == 1]['DayName'].value_counts().sort_index()\n",
    "non_fraud_by_day = df[df['is_fraud'] == 0]['DayName'].value_counts().sort_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3edc7f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of fraud on weekends: 32.55%\n",
      "Percentage of non-fraud on weekends: 34.84%\n"
     ]
    }
   ],
   "source": [
    "df['IsWeekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "weekend_fraud = df[df['is_fraud'] == 1]['IsWeekend'].mean()\n",
    "weekend_non_fraud = df[df['is_fraud'] == 0]['IsWeekend'].mean()\n",
    "\n",
    "print(f\"Percentage of fraud on weekends: {weekend_fraud * 100:.2f}%\")\n",
    "print(f\"Percentage of non-fraud on weekends: {weekend_non_fraud * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67a62065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#location based analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a8a049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate distance between cardholder and merchant\n",
    "df['distance'] = df.apply(lambda row: geodesic((row['lat'], row['long']), (row['merch_lat'], row['merch_long'])).km, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50a155e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check unique values in the 'is_fraud' column\n",
    "df['is_fraud'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09d464f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud vs Non-Fraud by Merchant Category\n",
    "fraud_by_category = df[df['is_fraud'] == 1]['category'].value_counts().head(10)\n",
    "non_fraud_by_category = df[df['is_fraud'] == 0]['category'].value_counts().head(10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07ec224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Fraudulent Merchant Categories: ['grocery_pos', 'shopping_net', 'misc_net', 'shopping_pos', 'gas_transport']\n"
     ]
    }
   ],
   "source": [
    "# Top 5 categories with the highest fraud counts\n",
    "top_fraud_merchant_categories = df[df['is_fraud'] == 1]['category'].value_counts().head(5).index.tolist()\n",
    "\n",
    "# Print top fraudulent categories\n",
    "print(\"Top Fraudulent Merchant Categories:\", top_fraud_merchant_categories)\n",
    "\n",
    "# Create HighRiskMerchantCategory flag\n",
    "df['HighRiskMerchantCategory'] = df['category'].apply(lambda x: 1 if x in top_fraud_merchant_categories else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7780b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HighRiskMerchantCategory\n",
      "0    763876\n",
      "1    532799\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the count of 1s and 0s in HighRiskMerchantCategory\n",
    "print(df['HighRiskMerchantCategory'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df499da3",
   "metadata": {},
   "source": [
    "# Potential Additional Features:\n",
    "Transaction Frequency:\n",
    "    Feature: How often a credit card has been used within a specific time frame (e.g., last hour or day).\n",
    "    Why: Fraudsters often make rapid successive transactions within short periods. You could create a rolling window to calculate transaction frequency.\n",
    "    How: You could calculate the number of transactions within the past X hours/days using a rolling window on the TransactionTime feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828d3a0e",
   "metadata": {},
   "source": [
    "#age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "221b8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure 'DateOfBirth' is in datetime format\n",
    "df['DateOfBirth'] = pd.to_datetime(df['DateOfBirth'], errors='coerce')  # Handle errors during conversion\n",
    "\n",
    "# Step 1: Calculate Age\n",
    "# Calculate age in years\n",
    "df['Age'] = (pd.Timestamp.now() - df['DateOfBirth']).dt.days // 365  # Age in years\n",
    "\n",
    "# Step 2: Create Age Groups\n",
    "# Define age bins and labels\n",
    "bins = [0, 18, 25, 35, 45, 55, 65, 100]  # Define your age bins, ensuring to cover all possible ages\n",
    "labels = ['0-18', '19-25', '26-35', '36-45', '46-55', '56-65', '66+']  # Corresponding labels\n",
    "\n",
    "# Create age group feature, include NaN values handling\n",
    "df['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "\n",
    "# Verify the new features without truncating DataFrame\n",
    "#print(df[['DateOfBirth', 'Age', 'AgeGroup']].head(10))  # Display the first 10 entries\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19dc4c6f",
   "metadata": {},
   "source": [
    "RapidTransactionFlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b596d059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    TransactionFrequency\n",
      "TransactionTime                         \n",
      "2019-01-01 00:00:18                  NaN\n",
      "2019-01-01 00:00:44                  NaN\n",
      "2019-01-01 00:00:51                  NaN\n",
      "2019-01-01 00:01:16                  NaN\n",
      "2019-01-01 00:03:06                  NaN\n",
      "2019-01-01 00:04:08                  NaN\n",
      "2019-01-01 00:04:42                  NaN\n",
      "2019-01-01 00:05:08                  NaN\n",
      "2019-01-01 00:05:18                  NaN\n",
      "2019-01-01 00:06:01                  NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_9644/2620616198.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['TransactionFrequency'] = df.groupby('CreditCardNumber').apply(count_transactions_within_last_hour).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "def count_transactions_within_last_hour(group):\n",
    "    # Create an empty list to hold the frequencies\n",
    "    frequencies = []\n",
    "    \n",
    "    # Loop over each transaction time in the group\n",
    "    for time in group.index:\n",
    "        # Count the number of transactions within the last hour\n",
    "        count = group[(group.index >= (time - pd.Timedelta(hours=1))) & (group.index <= time)].shape[0]\n",
    "        frequencies.append(count)\n",
    "    \n",
    "    return frequencies\n",
    "\n",
    "# Apply the function to each group\n",
    "df['TransactionFrequency'] = df.groupby('CreditCardNumber').apply(count_transactions_within_last_hour).reset_index(drop=True)\n",
    "print(df[['TransactionFrequency']].head(10))  # Display the first 10 entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dadb160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_9644/1490275857.py:2: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  transaction_counts_hourly = df.resample('H').size()\n",
      "/var/folders/xp/synrlbr15rx4pkqjmwqk80k80000gn/T/ipykernel_9644/1490275857.py:6: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  transaction_counts = df.groupby('CreditCardNumber').resample('H').size().reset_index(name='TransactionCount')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CreditCardNumber     TransactionTime  TransactionCount\n",
      "0       60416207185 2019-01-01 12:00:00                 1\n",
      "1       60416207185 2019-01-01 13:00:00                 0\n",
      "2       60416207185 2019-01-01 14:00:00                 0\n",
      "3       60416207185 2019-01-01 15:00:00                 0\n",
      "4       60416207185 2019-01-01 16:00:00                 0\n",
      "5       60416207185 2019-01-01 17:00:00                 0\n",
      "6       60416207185 2019-01-01 18:00:00                 0\n",
      "7       60416207185 2019-01-01 19:00:00                 0\n",
      "8       60416207185 2019-01-01 20:00:00                 0\n",
      "9       60416207185 2019-01-01 21:00:00                 0\n"
     ]
    }
   ],
   "source": [
    "# Resample the data to count transactions every hour\n",
    "transaction_counts_hourly = df.resample('H').size()\n",
    "transaction_counts_daily = df.resample('D').size()\n",
    "\n",
    "# Combine with CreditCardNumber if necessary\n",
    "transaction_counts = df.groupby('CreditCardNumber').resample('H').size().reset_index(name='TransactionCount')\n",
    "print(transaction_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97c19713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CreditCardNumber  TotalTransactionCount\n",
      "0       60416207185                   1518\n",
      "1       60422928733                   1531\n",
      "2       60423098130                    510\n",
      "3       60427851591                    528\n",
      "4       60487002085                    496\n",
      "5       60490596305                   1010\n",
      "6       60495593109                    518\n",
      "7      501802953619                   1559\n",
      "8      501818133297                      8\n",
      "9      501828204849                    515\n"
     ]
    }
   ],
   "source": [
    "total_transactions = df.groupby('CreditCardNumber').size().reset_index(name='TotalTransactionCount')\n",
    "print(total_transactions.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aef71d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01    1960\n",
      "2019-01-02     606\n",
      "2019-01-03     701\n",
      "2019-01-04     952\n",
      "2019-01-05     879\n",
      "              ... \n",
      "2020-06-17    1327\n",
      "2020-06-18    1547\n",
      "2020-06-19    1966\n",
      "2020-06-20    1910\n",
      "2020-06-21    1142\n",
      "Length: 537, dtype: int64\n",
      "         Unnamed: 0  CreditCardNumber  TransactionAmount           zip  \\\n",
      "count  1.061083e+06      1.061083e+06       1.061083e+06  1.061083e+06   \n",
      "mean   6.541777e+05      4.168739e+17       6.463842e+01  4.881377e+04   \n",
      "min    1.000000e+00      6.041621e+10       1.000000e+00  1.257000e+03   \n",
      "25%    3.399115e+05      1.800429e+14       9.500000e+00  2.623700e+04   \n",
      "50%    6.543810e+05      3.521417e+15       4.635000e+01  4.817400e+04   \n",
      "75%    9.639775e+05      4.642255e+15       8.223000e+01  7.204200e+04   \n",
      "max    1.296674e+06      4.992346e+18       5.459926e+02  9.978300e+04   \n",
      "std    3.688933e+05      1.308438e+18       8.254229e+01  2.689496e+04   \n",
      "\n",
      "                lat          long      city_pop  \\\n",
      "count  1.061083e+06  1.061083e+06  1.061083e+06   \n",
      "mean   3.853009e+01 -9.022821e+01  8.979136e+04   \n",
      "min    2.002710e+01 -1.656723e+02  2.300000e+01   \n",
      "25%    3.459060e+01 -9.679800e+01  7.430000e+02   \n",
      "50%    3.935430e+01 -8.747690e+01  2.470000e+03   \n",
      "75%    4.194040e+01 -8.015800e+01  2.047800e+04   \n",
      "max    6.669330e+01 -6.795030e+01  2.906700e+06   \n",
      "std    5.078796e+00  1.374857e+01  3.041541e+05   \n",
      "\n",
      "                         DateOfBirth     unix_time     merch_lat  \\\n",
      "count                        1061083  1.061083e+06  1.061083e+06   \n",
      "mean   1974-01-07 01:53:43.193284640  1.349452e+09  3.853026e+01   \n",
      "min              1924-10-30 00:00:00  1.325376e+09  1.902779e+01   \n",
      "25%              1962-12-06 00:00:00  1.339269e+09  3.471879e+01   \n",
      "50%              1975-12-28 00:00:00  1.349484e+09  3.935917e+01   \n",
      "75%              1987-05-05 00:00:00  1.359010e+09  4.195454e+01   \n",
      "max              2005-01-29 00:00:00  1.371817e+09  6.751027e+01   \n",
      "std                              NaN  1.264150e+07  5.112969e+00   \n",
      "\n",
      "         merch_long      is_fraud  TransactionID          Hour  HighRiskHour  \\\n",
      "count  1.061083e+06  1.061083e+06   1.061083e+06  1.061083e+06  1.061083e+06   \n",
      "mean  -9.022814e+01  5.652715e-03   6.541787e+05  1.323944e+01  2.301988e-01   \n",
      "min   -1.666701e+02  0.000000e+00   2.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%   -9.689874e+01  0.000000e+00   3.399125e+05  8.000000e+00  0.000000e+00   \n",
      "50%   -8.744732e+01  0.000000e+00   6.543820e+05  1.400000e+01  0.000000e+00   \n",
      "75%   -8.024346e+01  0.000000e+00   9.639785e+05  1.900000e+01  0.000000e+00   \n",
      "max   -6.695090e+01  1.000000e+00   1.296675e+06  2.300000e+01  1.000000e+00   \n",
      "std    1.376070e+01  7.497178e-02   3.688933e+05  6.727723e+00  4.209602e-01   \n",
      "\n",
      "          DayOfWeek     IsWeekend      distance  HighRiskMerchantCategory  \\\n",
      "count  1.061083e+06  1.061083e+06  1.061083e+06              1.061083e+06   \n",
      "mean   3.092014e+00  3.698118e-01  7.611910e+01              3.897716e-01   \n",
      "min    0.000000e+00  0.000000e+00  2.227351e-02              0.000000e+00   \n",
      "25%    1.000000e+00  0.000000e+00  5.535721e+01              0.000000e+00   \n",
      "50%    3.000000e+00  0.000000e+00  7.827691e+01              0.000000e+00   \n",
      "75%    5.000000e+00  1.000000e+00  9.849232e+01              1.000000e+00   \n",
      "max    6.000000e+00  1.000000e+00  1.518682e+02              1.000000e+00   \n",
      "std    2.263057e+00  4.827538e-01  2.909755e+01              4.876986e-01   \n",
      "\n",
      "                Age  \n",
      "count  1.061083e+06  \n",
      "mean   5.034941e+01  \n",
      "min    1.900000e+01  \n",
      "25%    3.700000e+01  \n",
      "50%    4.800000e+01  \n",
      "75%    6.100000e+01  \n",
      "max    1.000000e+02  \n",
      "std    1.738521e+01  \n"
     ]
    }
   ],
   "source": [
    "# Calculate the time difference between consecutive transactions\n",
    "time_diff = df.index.to_series().diff().dt.total_seconds()\n",
    "# Flag rapid transactions (within 5 minutes)\n",
    "df['RapidTransactionFlag'] = time_diff < 60  # For a 1-minute threshold\n",
    "\n",
    "# Create a temporary DataFrame for rapid transactions\n",
    "rapid_transactions = df[df['RapidTransactionFlag']]\n",
    "\n",
    "# Group by date and count the number of rapid transactions\n",
    "rapid_transaction_counts = rapid_transactions.groupby(rapid_transactions.index.date).size()\n",
    "print(rapid_transaction_counts)\n",
    "\n",
    "# Get a summary of the rapid transactions\n",
    "rapid_transactions_summary = rapid_transactions.describe()\n",
    "print(rapid_transactions_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64e05375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'CreditCardNumber', 'merchant', 'category',\n",
      "       'TransactionAmount', 'first', 'last', 'gender', 'street', 'city',\n",
      "       'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID', 'Hour', 'HighRiskHour', 'DayOfWeek', 'DayName',\n",
      "       'IsWeekend', 'distance', 'HighRiskMerchantCategory', 'Age', 'AgeGroup',\n",
      "       'TransactionFrequency', 'RapidTransactionFlag'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5e448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb3b200c",
   "metadata": {},
   "source": [
    "Transaction Amount Features:\n",
    "Log Transaction Amount: Normalize the TransactionAmount by taking its logarithm to reduce skewness.\n",
    "Transaction Amount Flags: Create binary flags for high-value transactions (e.g., if TransactionAmount exceeds a certain threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da75a878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     TransactionAmount  LogTransactionAmount  \\\n",
      "TransactionTime                                                \n",
      "2019-01-01 00:00:18               4.97              1.786747   \n",
      "2019-01-01 00:00:44             107.23              4.684259   \n",
      "2019-01-01 00:00:51             220.11              5.398660   \n",
      "2019-01-01 00:01:16              45.00              3.828641   \n",
      "2019-01-01 00:03:06              41.96              3.760269   \n",
      "2019-01-01 00:04:08              94.63              4.560487   \n",
      "2019-01-01 00:04:42              44.54              3.818591   \n",
      "2019-01-01 00:05:08              71.65              4.285653   \n",
      "2019-01-01 00:05:18               4.27              1.662030   \n",
      "2019-01-01 00:06:01             198.39              5.295263   \n",
      "\n",
      "                     HighValueTransactionFlag  \n",
      "TransactionTime                                \n",
      "2019-01-01 00:00:18                     False  \n",
      "2019-01-01 00:00:44                      True  \n",
      "2019-01-01 00:00:51                      True  \n",
      "2019-01-01 00:01:16                     False  \n",
      "2019-01-01 00:03:06                     False  \n",
      "2019-01-01 00:04:08                     False  \n",
      "2019-01-01 00:04:42                     False  \n",
      "2019-01-01 00:05:08                     False  \n",
      "2019-01-01 00:05:18                     False  \n",
      "2019-01-01 00:06:01                      True  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample DataFrame creation\n",
    "# Assume 'df' is your DataFrame and has a 'TransactionAmount' column\n",
    "# df = pd.read_csv('your_data.csv')  # Load your actual data\n",
    "\n",
    "# Step 1: Log Transaction Amount\n",
    "# Calculate the log of TransactionAmount\n",
    "df['LogTransactionAmount'] = np.log1p(df['TransactionAmount'])  # Use log1p for stability with 0 values\n",
    "\n",
    "# Step 2: Create Transaction Amount Flags\n",
    "# Define a threshold for high-value transactions\n",
    "threshold = 100  # Adjust the threshold based on your data context\n",
    "\n",
    "# Create a flag for high-value transactions\n",
    "df['HighValueTransactionFlag'] = df['TransactionAmount'] > threshold\n",
    "\n",
    "# Verify the new features\n",
    "print(df[['TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag']].head(10))  # Display the first 10 entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217bc43",
   "metadata": {},
   "source": [
    "Behavioral Features:\n",
    "Count of Transactions in Last X Days: Count how many transactions have occurred in the last 7, 14, or 30 days.\n",
    "Average Transaction Amount in Last X Days: Calculate the average transaction amount over the same periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25aeefda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     TransactionCountLast7Days  TransactionCountLast14Days  \\\n",
      "TransactionTime                                                              \n",
      "2019-01-01 12:47:15                        1.0                         1.0   \n",
      "2019-01-02 08:44:57                        2.0                         2.0   \n",
      "2019-01-02 08:47:36                        3.0                         3.0   \n",
      "2019-01-02 12:38:14                        4.0                         4.0   \n",
      "2019-01-02 13:10:46                        5.0                         5.0   \n",
      "2019-01-03 13:56:35                        6.0                         6.0   \n",
      "2019-01-03 17:05:10                        7.0                         7.0   \n",
      "2019-01-04 13:59:55                        8.0                         8.0   \n",
      "2019-01-04 21:17:22                        9.0                         9.0   \n",
      "2019-01-05 00:42:24                       10.0                        10.0   \n",
      "\n",
      "                     TransactionCountLast30Days  \\\n",
      "TransactionTime                                   \n",
      "2019-01-01 12:47:15                         1.0   \n",
      "2019-01-02 08:44:57                         2.0   \n",
      "2019-01-02 08:47:36                         3.0   \n",
      "2019-01-02 12:38:14                         4.0   \n",
      "2019-01-02 13:10:46                         5.0   \n",
      "2019-01-03 13:56:35                         6.0   \n",
      "2019-01-03 17:05:10                         7.0   \n",
      "2019-01-04 13:59:55                         8.0   \n",
      "2019-01-04 21:17:22                         9.0   \n",
      "2019-01-05 00:42:24                        10.0   \n",
      "\n",
      "                     AverageTransactionAmountLast7Days  \\\n",
      "TransactionTime                                          \n",
      "2019-01-01 12:47:15                           7.270000   \n",
      "2019-01-02 08:44:57                          30.105000   \n",
      "2019-01-02 08:47:36                          47.430000   \n",
      "2019-01-02 12:38:14                          44.270000   \n",
      "2019-01-02 13:10:46                          40.852000   \n",
      "2019-01-03 13:56:35                          35.188333   \n",
      "2019-01-03 17:05:10                          31.365714   \n",
      "2019-01-04 13:59:55                          42.083750   \n",
      "2019-01-04 21:17:22                          40.378889   \n",
      "2019-01-05 00:42:24                          46.861000   \n",
      "\n",
      "                     AverageTransactionAmountLast14Days  \\\n",
      "TransactionTime                                           \n",
      "2019-01-01 12:47:15                            7.270000   \n",
      "2019-01-02 08:44:57                           30.105000   \n",
      "2019-01-02 08:47:36                           47.430000   \n",
      "2019-01-02 12:38:14                           44.270000   \n",
      "2019-01-02 13:10:46                           40.852000   \n",
      "2019-01-03 13:56:35                           35.188333   \n",
      "2019-01-03 17:05:10                           31.365714   \n",
      "2019-01-04 13:59:55                           42.083750   \n",
      "2019-01-04 21:17:22                           40.378889   \n",
      "2019-01-05 00:42:24                           46.861000   \n",
      "\n",
      "                     AverageTransactionAmountLast30Days  \n",
      "TransactionTime                                          \n",
      "2019-01-01 12:47:15                            7.270000  \n",
      "2019-01-02 08:44:57                           30.105000  \n",
      "2019-01-02 08:47:36                           47.430000  \n",
      "2019-01-02 12:38:14                           44.270000  \n",
      "2019-01-02 13:10:46                           40.852000  \n",
      "2019-01-03 13:56:35                           35.188333  \n",
      "2019-01-03 17:05:10                           31.365714  \n",
      "2019-01-04 13:59:55                           42.083750  \n",
      "2019-01-04 21:17:22                           40.378889  \n",
      "2019-01-05 00:42:24                           46.861000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'TransactionTime' is already set as the index and in datetime format\n",
    "\n",
    "# Step 1: Count of Transactions in Last X Days\n",
    "for days in [7, 14, 30]:\n",
    "    # Sort data by CreditCardNumber and TransactionTime to ensure rolling works properly\n",
    "    df = df.sort_values(by=['CreditCardNumber', 'TransactionTime'])\n",
    "    \n",
    "    # Apply rolling and count the number of transactions for each card\n",
    "    df[f'TransactionCountLast{days}Days'] = (\n",
    "        df.groupby('CreditCardNumber')['CreditCardNumber']\n",
    "        .rolling(f'{days}D')\n",
    "        .count()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Step 2: Average Transaction Amount in Last X Days\n",
    "for days in [7, 14, 30]:\n",
    "    # Sort data by CreditCardNumber and TransactionTime to ensure rolling works properly\n",
    "    df = df.sort_values(by=['CreditCardNumber', 'TransactionTime'])\n",
    "    \n",
    "    # Calculate the average transaction amount for each credit card in the last X days\n",
    "    df[f'AverageTransactionAmountLast{days}Days'] = (\n",
    "        df.groupby('CreditCardNumber')['TransactionAmount']\n",
    "        .rolling(f'{days}D')\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "# Verify the new features\n",
    "print(df[['TransactionCountLast7Days', 'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
    "           'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast14Days', 'AverageTransactionAmountLast30Days']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebcd77cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'CreditCardNumber', 'merchant', 'category',\n",
      "       'TransactionAmount', 'first', 'last', 'gender', 'street', 'city',\n",
      "       'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID', 'Hour', 'HighRiskHour', 'DayOfWeek', 'DayName',\n",
      "       'IsWeekend', 'distance', 'HighRiskMerchantCategory', 'Age', 'AgeGroup',\n",
      "       'TransactionFrequency', 'RapidTransactionFlag', 'LogTransactionAmount',\n",
      "       'HighValueTransactionFlag', 'TransactionCountLast7Days',\n",
      "       'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
      "       'AverageTransactionAmountLast7Days',\n",
      "       'AverageTransactionAmountLast14Days',\n",
      "       'AverageTransactionAmountLast30Days'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)  # Display all columns in the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc639c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a24a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b4aaef3",
   "metadata": {},
   "source": [
    "# Graph Construction with NetworkX:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18793de2",
   "metadata": {},
   "source": [
    "Highlight Fraudulent Nodes: Overlay of fraudulent and non-fraudulent credit cards on this degree distribution to see if there’s a difference in their degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e26bd1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique credit card nodes: 983\n",
      "Number of unique merchant nodes: 693\n",
      "Number of credit card nodes with degrees: 983\n",
      "Number of merchant nodes with degrees: 693\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges between credit cards and merchants, including transaction amount as an edge attribute\n",
    "for idx, row in df.iterrows():\n",
    "    credit_card = str(row['CreditCardNumber'])\n",
    "    merchant = str(row['merchant'])\n",
    "    transaction_amount = row['TransactionAmount']  # Ensure TransactionAmount exists in your dataframe\n",
    "    \n",
    "    # Add an edge with the transaction amount as an attribute\n",
    "    G.add_edge(credit_card, merchant, transaction_amount=transaction_amount)\n",
    "\n",
    "\n",
    "# Calculate degrees for all nodes in the graph\n",
    "degrees = dict(G.degree())\n",
    "\n",
    "# Filter degrees for credit cards and merchants\n",
    "credit_card_nodes = df['CreditCardNumber'].astype(str).unique()\n",
    "merchant_nodes = df['merchant'].astype(str).unique()\n",
    "\n",
    "credit_card_degrees = {node: degrees[node] for node in credit_card_nodes if node in degrees}\n",
    "merchant_degrees = {node: degrees[node] for node in merchant_nodes if node in degrees}\n",
    "\n",
    "# Debugging: Print counts to ensure correctness\n",
    "print(f\"Number of unique credit card nodes: {len(credit_card_nodes)}\")\n",
    "print(f\"Number of unique merchant nodes: {len(merchant_nodes)}\")\n",
    "print(f\"Number of credit card nodes with degrees: {len(credit_card_degrees)}\")\n",
    "print(f\"Number of merchant nodes with degrees: {len(merchant_degrees)}\")\n",
    "\n",
    "# Create a new DataFrame for easier plotting\n",
    "degree_df = pd.DataFrame({\n",
    "    'CreditCardDegree': pd.Series(credit_card_degrees),\n",
    "    'MerchantDegree': pd.Series(merchant_degrees)\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a55584fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add degree information back to the original DataFrame\n",
    "df['degree'] = df['CreditCardNumber'].astype(str).map(credit_card_degrees)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a8f6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check edges and their attributes\n",
    "#for edge in G.edges(data=True):\n",
    "#    print(edge)\n",
    "\n",
    "#do NOT print this, huge list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9c525e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CreditCardNumber'] = df['CreditCardNumber'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5115524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_mapping = df.set_index('CreditCardNumber')['is_fraud'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e509da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(fraud_mapping.head(5))\n",
    "#only testing purposes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "167571cb",
   "metadata": {},
   "source": [
    "betweenness_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15488913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate betweenness centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85bd6fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['betweenness_centrality'] = df['CreditCardNumber'].map(betweenness_centrality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb8bf772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1.296675e+06\n",
      "mean     4.183005e-04\n",
      "std      1.183054e-04\n",
      "min      2.971179e-08\n",
      "25%      3.695928e-04\n",
      "50%      4.408069e-04\n",
      "75%      5.188969e-04\n",
      "max      5.767604e-04\n",
      "Name: betweenness_centrality, dtype: float64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df['betweenness_centrality'].describe())\n",
    "print(df['betweenness_centrality'].isna().sum())  # Check for missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5e4b947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60416207185: 0.0003969822003208483\n",
      "fraud_Kutch-Ferry: 0.0007037439409292381\n"
     ]
    }
   ],
   "source": [
    "# Check betweenness centrality for specific credit card numbers\n",
    "sample_nodes = ['60416207185', 'fraud_Kutch-Ferry']  # Replace with actual nodes\n",
    "for node in sample_nodes:\n",
    "    print(f\"{node}: {betweenness_centrality.get(node)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439124c6",
   "metadata": {},
   "source": [
    "1. Investigate Nodes with High Betweenness Centrality:\n",
    "\n",
    "Now that you’ve visualized nodes with high betweenness centrality, you can:\n",
    "\n",
    "    Examine if fraudulent nodes tend to have high betweenness centrality. This might indicate that these nodes are acting as \"connectors\" between different parts of the network, which could be a sign of suspicious behavior.\n",
    "    Compare centrality between fraud and non-fraud nodes to see if there's a pattern.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13fdd8e",
   "metadata": {},
   "source": [
    "2. Visualize Communities in the Network:\n",
    "\n",
    "You could apply community detection to uncover fraud rings or clusters of merchants targeted by fraudsters. The Louvain algorithm is great for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5f148a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community.community_louvain as community_louvain\n",
    "\n",
    "\n",
    "# Apply Louvain method for community detection\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee872fb",
   "metadata": {},
   "source": [
    "Fraud Node Highlighting:\n",
    "\n",
    "    Fraudulent nodes (from df['is_fraud'] == 1) are colored red to make them stand out. The rest of the nodes are still colored based on their communities.\n",
    "    This should help you easily spot any fraudulent nodes in the network.\n",
    "\n",
    "Top 10 Most Central Nodes:\n",
    "\n",
    "    We calculate betweenness centrality and extract the top 10 most central nodes.\n",
    "    These nodes are visualized with their connections, which should help declutter the graph and focus on the key players in the transaction network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8fd26394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply Louvain method for community detection\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "# Create positions for nodes using a spring layout\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Add the community information to the DataFrame\n",
    "df['community'] = df['CreditCardNumber'].map(partition)\n",
    "\n",
    "# Highlight fraud nodes separately\n",
    "fraud_nodes = df[df['is_fraud'] == 1]['CreditCardNumber'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95613930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the community information to the DataFrame\n",
    "df['community'] = df['CreditCardNumber'].map(partition)\n",
    "\n",
    "# Calculate the percentage of fraud in each community\n",
    "community_fraud = df.groupby('community')['is_fraud'].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d9407a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community\n",
      "0    0.007648\n",
      "1    0.007615\n",
      "2    0.004217\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print fraud rate per community\n",
    "print(community_fraud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17572504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community\n",
      "0    489023\n",
      "1    105981\n",
      "2    701671\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "community_size = df.groupby('community').size()\n",
    "print(community_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43cca311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fraud rates and community sizes into a single DataFrame\n",
    "fraud_vs_size = pd.concat([community_fraud, df.groupby('community').size()], axis=1)\n",
    "fraud_vs_size.columns = ['FraudRate', 'CommunitySize']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "accbc68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "community\n",
      "0    0.007648\n",
      "1    0.007615\n",
      "2    0.004217\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "top_fraud_communities = community_fraud.sort_values(ascending=False).head(5)\n",
    "print(top_fraud_communities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a875aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the community labels of the top fraud communities\n",
    "top_community_labels = top_fraud_communities.index.tolist()\n",
    "\n",
    "# Filter the DataFrame for only the top fraud communities\n",
    "top_communities_df = df[df['community'].isin(top_community_labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88b0bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merchant\n",
      "fraud_Kozey-Boehm                       0.025723\n",
      "fraud_Herman, Treutel and Dickens       0.025385\n",
      "fraud_Kerluke-Abshire                   0.022307\n",
      "fraud_Brown PLC                         0.022109\n",
      "fraud_Goyette Inc                       0.021616\n",
      "fraud_Terry-Huel                        0.021543\n",
      "fraud_Jast Ltd                          0.021505\n",
      "fraud_Schmeler, Bashirian and Price     0.020833\n",
      "fraud_Boyer-Reichert                    0.019916\n",
      "fraud_Langworth, Boehm and Gulgowski    0.019807\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Show Only the Top Merchants by Fraud Rate:\n",
    "# Instead of displaying all merchants, you can filter the plot to show only the top 10 or 20 merchants with the highest fraud rates.\n",
    "\n",
    "# Calculate fraud rate by merchant in the top fraud communities\n",
    "merchant_fraud_rate = top_communities_df.groupby('merchant')['is_fraud'].mean()\n",
    "\n",
    "# Sort merchants by fraud rate in descending order\n",
    "top_merchants = merchant_fraud_rate.sort_values(ascending=False).head(10)\n",
    "\n",
    "# Print top 10 merchants with highest fraud rate\n",
    "print(top_merchants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bbd1dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'category' is a column representing merchant categories\n",
    "merchantcategory_fraud = top_communities_df.groupby('category')['is_fraud'].mean()\n",
    "\n",
    "# Sort the fraud rate by merchant category in descending order\n",
    "merchantcategory_fraud_sorted = merchantcategory_fraud.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e7c000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the sorted fraud rates\n",
    "#print(merchantcategory_fraud_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc37398",
   "metadata": {},
   "source": [
    "Which Option to Choose:\n",
    "\n",
    "    If you're interested in specific merchants involved in fraud, go with Option 1 (Top 10 merchants by fraud rate).\n",
    "    If you're interested in understanding the overall pattern of fraud rates, choose Option 2 (Distribution) or Option 3 (Box Plot).\n",
    "    If you have merchant categories, Option 4 provides a higher-level view of which categories are more susceptible to fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a15ba4",
   "metadata": {},
   "source": [
    "3. Combine Graph Metrics for Analysis:\n",
    "\n",
    "While you've visualized degree and betweenness centrality separately, you can explore combinations of these metrics:\n",
    "\n",
    "    High betweenness centrality combined with high degree might indicate important fraud hubs.\n",
    "    Low clustering coefficient with high betweenness could signal isolated fraud nodes acting as key connectors.\n",
    "\n",
    "You can calculate clustering coefficient and add it to your analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "154aa31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Density: 0.34130445623909095\n"
     ]
    }
   ],
   "source": [
    "# Check the density of the graph (a measure of sparsity)\n",
    "density = nx.density(G)\n",
    "print(f\"Graph Density: {density}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e360c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Degree of Nodes: 571.6849642004773\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average degree\n",
    "degree_sequence = [degree for node, degree in G.degree()]\n",
    "average_degree = sum(degree_sequence) / len(degree_sequence)\n",
    "print(f\"Average Degree of Nodes: {average_degree}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa3b21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed6bcacc",
   "metadata": {},
   "source": [
    "The results you’ve provided show:\n",
    "\n",
    "    Graph Density: 0.2513\n",
    "        This is a moderate density value. A density of 0 would indicate a completely disconnected graph, while a value close to 1 would indicate a very tightly connected graph (like a clique). A density of 0.25 means about 25% of the possible connections between nodes are present, which suggests the graph isn’t overly sparse, but it’s not densely connected either.\n",
    "\n",
    "    Average Degree: 406.18\n",
    "        This is relatively high, meaning that, on average, each node (credit card or merchant) is connected to about 406 other nodes. This high degree could indicate that nodes, especially credit cards, are interacting with many merchants. However, these connections are likely not forming closed loops or triangles, which is why the clustering coefficient is zero for all nodes.\n",
    "\n",
    "What This Means:\n",
    "\n",
    "    Even though the average degree is high, suggesting that credit cards are interacting with many merchants, the interactions are likely not forming triangles (where connected nodes are also connected to each other). This results in zero clustering coefficients across the board.\n",
    "\n",
    "    The moderate graph density indicates that the network is connected to some extent, but not densely enough to produce high clustering coefficients.\n",
    "\n",
    "Why This Happens:\n",
    "\n",
    "In transaction networks, it’s common for credit cards to interact with different merchants, but merchants don’t typically transact with each other, which means closed triangles (required for a non-zero clustering coefficient) are rare. In fraud detection, this is normal, as fraudsters typically transact with many distinct merchants rather than creating highly connected communities.\n",
    "Next Steps:\n",
    "\n",
    "Given the moderate density and high degree of the nodes, the clustering coefficient might not be the most insightful metric. Instead, you could focus on the following:\n",
    "1. Focus on Betweenness Centrality and Degree:\n",
    "\n",
    "These metrics are more likely to highlight key nodes (e.g., credit cards or merchants) that are crucial in the transaction network. You’ve already calculated betweenness centrality, and the average degree indicates that some credit cards or merchants might have a significant number of connections.\n",
    "2. Look for Key Nodes (High Degree or Centrality):\n",
    "\n",
    "You could identify nodes with high degree or betweenness centrality to see if they’re involved in fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "452804a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'CreditCardNumber', 'merchant', 'category',\n",
      "       'TransactionAmount', 'first', 'last', 'gender', 'street', 'city',\n",
      "       'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'DateOfBirth',\n",
      "       'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'is_fraud',\n",
      "       'TransactionID', 'Hour', 'HighRiskHour', 'DayOfWeek', 'DayName',\n",
      "       'IsWeekend', 'distance', 'HighRiskMerchantCategory', 'Age', 'AgeGroup',\n",
      "       'TransactionFrequency', 'RapidTransactionFlag', 'LogTransactionAmount',\n",
      "       'HighValueTransactionFlag', 'TransactionCountLast7Days',\n",
      "       'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
      "       'AverageTransactionAmountLast7Days',\n",
      "       'AverageTransactionAmountLast14Days',\n",
      "       'AverageTransactionAmountLast30Days', 'degree',\n",
      "       'betweenness_centrality', 'community'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c450de",
   "metadata": {},
   "source": [
    "Given that you only identified 4 nodes with the lowest betweenness centrality, and they are all disconnected, it seems that focusing on these low-centrality nodes isn’t providing much value for your analysis. This could indicate that these nodes (credit card numbers or merchants) are peripheral and not involved in significant patterns of interaction, and therefore, might not contribute meaningful insights for detecting fraud.\n",
    "Should You Continue with Low Betweenness Centrality Nodes?\n",
    "\n",
    "    Disconnected Nodes: Since the nodes with the lowest betweenness centrality are disconnected and few in number, they don't seem to play a crucial role in the transaction network.\n",
    "    Low Utility: If these nodes don't show fraud or aren't involved in key transactions, they might not be useful for your model or analysis.\n",
    "\n",
    "What to Do Next:\n",
    "\n",
    "    Abandon the Focus on Low Betweenness Centrality:\n",
    "        Since these nodes are disconnected and don't seem to offer useful insights, it might be better to abandon the focus on low betweenness centrality.\n",
    "        Instead, focus on nodes with more centrality (betweenness, degree, etc.) or explore other graph features.\n",
    "\n",
    "    Explore Other Graph Metrics:\n",
    "\n",
    "        You can shift your focus to more meaningful metrics such as pagerank or eigenvector centrality, which may reveal more about the influence or importance of nodes in the network.\n",
    "\n",
    "        Here's how you can calculate pagerank and analyze it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d5be3c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag',\n",
    "    'TransactionCountLast7Days', 'TransactionCountLast14Days', 'TransactionCountLast30Days',\n",
    "    'AverageTransactionAmountLast7Days', 'AverageTransactionAmountLast14Days', 'AverageTransactionAmountLast30Days',\n",
    "    'Hour', 'HighRiskHour', 'DayOfWeek', 'IsWeekend', 'TransactionFrequency', 'RapidTransactionFlag',\n",
    "    'lat', 'long', 'merch_lat', 'merch_long', 'distance', 'city_pop',\n",
    "    'Age', 'AgeGroup', 'gender', 'state', 'city',\n",
    "    'degree', 'betweenness_centrality', 'community'\n",
    "]\n",
    "\n",
    "df_selected_features = df[selected_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f21f1e",
   "metadata": {},
   "source": [
    "# Page rank as new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6fda32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PageRank for each node in the graph\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "# Map the PageRank values to the 'CreditCardNumber' in the DataFrame\n",
    "df['pagerank'] = df['CreditCardNumber'].map(pagerank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "53596e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values in the pagerank column\n",
    "print(df['pagerank'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f5e2620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1.296675e+06\n",
      "mean     5.971670e-04\n",
      "std      7.856303e-05\n",
      "min      9.478567e-05\n",
      "25%      5.812104e-04\n",
      "50%      6.195727e-04\n",
      "75%      6.566711e-04\n",
      "max      6.834763e-04\n",
      "Name: pagerank, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check descriptive statistics of pagerank values\n",
    "print(df['pagerank'].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a525047f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with zero PageRank: 0\n"
     ]
    }
   ],
   "source": [
    "# Check how many nodes have a PageRank of zero\n",
    "zero_pagerank_count = (df['pagerank'] == 0).sum()\n",
    "print(f\"Number of nodes with zero PageRank: {zero_pagerank_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c97f80b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PageRank for Fraud: 0.000505843602201983\n",
      "Average PageRank for Non-Fraud: 0.0005976986758972222\n"
     ]
    }
   ],
   "source": [
    "# Compare the average PageRank for fraud and non-fraud transactions\n",
    "fraud_avg_pagerank = df[df['is_fraud'] == 1]['pagerank'].mean()\n",
    "non_fraud_avg_pagerank = df[df['is_fraud'] == 0]['pagerank'].mean()\n",
    "\n",
    "print(f\"Average PageRank for Fraud: {fraud_avg_pagerank}\")\n",
    "print(f\"Average PageRank for Non-Fraud: {non_fraud_avg_pagerank}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "47121274",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features.append('pagerank')\n",
    "df_selected_features = df[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "439c0ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TransactionAmount', 'LogTransactionAmount', 'HighValueTransactionFlag',\n",
      "       'TransactionCountLast7Days', 'TransactionCountLast14Days',\n",
      "       'TransactionCountLast30Days', 'AverageTransactionAmountLast7Days',\n",
      "       'AverageTransactionAmountLast14Days',\n",
      "       'AverageTransactionAmountLast30Days', 'Hour', 'HighRiskHour',\n",
      "       'DayOfWeek', 'IsWeekend', 'TransactionFrequency',\n",
      "       'RapidTransactionFlag', 'lat', 'long', 'merch_lat', 'merch_long',\n",
      "       'distance', 'city_pop', 'Age', 'AgeGroup', 'gender', 'state', 'city',\n",
      "       'degree', 'betweenness_centrality', 'community', 'pagerank'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_selected_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ae9a67f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1296675, 46)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b08a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e17cecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "56237852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define target and features\n",
    "y = df['is_fraud'] \n",
    "X = df[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c2f5bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the below code snippet is for the first run of fraudtrain.csv"
   ]
  },
  {
   "cell_type": "raw",
   "id": "096cd393",
   "metadata": {},
   "source": [
    "import time\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Start timer\n",
    "start_time_model = time.time()\n",
    "\n",
    "# One-hot encode fraudTrain data\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "fraudTrain_columns_balanced = X_encoded.columns  # Save the columns after one-hot encoding\n",
    "\n",
    "# Save the fraudTrain columns\n",
    "fraudTrain_columns_balanced_df = pd.DataFrame(fraudTrain_columns_balanced)\n",
    "data_processed_dir = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/data/processed'\n",
    "fraudTrain_columns_balanced_df.to_csv(os.path.join(data_processed_dir, 'fraudTrain_columns_balanced.csv'), index=False)\n",
    "\n",
    "# Impute missing values and train the Logistic Regression model\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X_encoded)\n",
    "\n",
    "# Train Logistic Regression with class_weight='balanced'\n",
    "credit_card_fraud_detection_model_balanced = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "credit_card_fraud_detection_model_balanced.fit(X_imputed, y)\n",
    "\n",
    "# Evaluate the model on fraudTrain\n",
    "y_pred_train_balanced = credit_card_fraud_detection_model_balanced.predict(X_imputed)\n",
    "\n",
    "# End timer\n",
    "end_time_model = time.time()\n",
    "print(f\"Time taken to run model: {end_time_model - start_time_model:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "42eaa42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.4\n"
     ]
    }
   ],
   "source": [
    "import imblearn\n",
    "print(imblearn.__version__)  # This should print 0.12.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c2bd42d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE:\n",
      "is_fraud\n",
      "0    1289169\n",
      "1    1289169\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming X_imputed and y are already defined from fraudTrain.csv\n",
    "# Apply SMOTE to the imputed data (X_imputed) and target (y)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_imputed, y)\n",
    "\n",
    "# Check the class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(pd.Series(y_resampled).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b6124a36",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train Logistic Regression with class_weight='balanced'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m log_reg_smote \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mlog_reg_smote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_resampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Make predictions on the original training data (fraudTrain.csv)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m y_pred_train_smote \u001b[38;5;241m=\u001b[39m log_reg_smote\u001b[38;5;241m.\u001b[39mpredict(X_imputed)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1233\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1231\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1233\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1258\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:436\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    432\u001b[0m l2_reg_strength \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C\n\u001b[1;32m    433\u001b[0m iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[1;32m    434\u001b[0m     np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[1;32m    435\u001b[0m ]\n\u001b[0;32m--> 436\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg_strength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[1;32m    445\u001b[0m     solver,\n\u001b[1;32m    446\u001b[0m     opt_res,\n\u001b[1;32m    447\u001b[0m     max_iter,\n\u001b[1;32m    448\u001b[0m     extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    450\u001b[0m w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/scipy/optimize/_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 713\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    716\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    717\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py:369\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    363\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:296\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/scipy/optimize/_optimize.py:78\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/scipy/optimize/_optimize.py:72\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 72\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:202\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads)\u001b[0m\n\u001b[1;32m    200\u001b[0m     grad[:n_features] \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m grad_per_sample \u001b[38;5;241m+\u001b[39m l2_reg_strength \u001b[38;5;241m*\u001b[39m weights\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept:\n\u001b[0;32m--> 202\u001b[0m         grad[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_per_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m l2_reg_strength \u001b[38;5;241m*\u001b[39m squared_norm(weights)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ZHAW_Project/lib/python3.10/site-packages/numpy/core/_methods.py:47\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_minimum(a, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out, keepdims, initial, where)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression with class_weight='balanced'\n",
    "log_reg_smote = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "log_reg_smote.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions on the original training data (fraudTrain.csv)\n",
    "y_pred_train_smote = log_reg_smote.predict(X_imputed)\n",
    "\n",
    "# Evaluate the model on the training data\n",
    "print(\"Classification report for fraudTrain.csv after SMOTE (Logistic regression):\")\n",
    "print(classification_report(y, y_pred_train_smote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd391bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess fraudTest.csv the same way as before (e.g., encoding, imputation)\n",
    "X_imputed_fraudTest = imputer.transform(X_encoded_fraudTest)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_test_smote = log_reg_smote.predict(X_imputed_fraudTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Generate the classification report\n",
    "logistic_regression_smote_report_train = classification_report(y_test, y_pred_test_smote)\n",
    "\n",
    "print(\"Classification report for fraudTest.csv after SMOTE:\")\n",
    "\n",
    "print(logistic_regression_smote_report_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "\n",
    "print(classification_report(y_test, y_pred_test_smote))\n",
    "\n",
    "\n",
    "# Specify the output directory\n",
    "reports_output_dir = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports'\n",
    "\n",
    "# Create a dynamic filename\n",
    "classification_report_title = 'logistic regression smote Classification Report Train '\n",
    "classification_report_outputfilename = f\"{dataset_type}_{classification_report_title.replace(' ', '_').replace(',', '').lower()}.txt\"\n",
    "\n",
    "# Save the report to a text file\n",
    "with open(os.path.join(reports_output_dir, classification_report_outputfilename), 'w') as f:\n",
    "    f.write(logistic_regression_smote_report_train)  # Writing the classification report string to the file\n",
    "\n",
    "print(f\"Classification report saved to: {os.path.join(reports_output_dir, classification_report_outputfilename)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4c0203d",
   "metadata": {},
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Generate the classification report\n",
    "report_train_balanced = classification_report(y, y_pred_train_balanced)\n",
    "print(report_train_balanced)\n",
    "\n",
    "# Specify the output directory\n",
    "reports_output_dir = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports'\n",
    "\n",
    "# Create a dynamic filename\n",
    "classification_report_title = 'Credit Fraud Detection Classification Report Balanced '\n",
    "classification_report_outputfilename = f\"{dataset_type}_{classification_report_title.replace(' ', '_').replace(',', '').lower()}.txt\"\n",
    "\n",
    "# Save the report to a text file\n",
    "with open(os.path.join(reports_output_dir, classification_report_outputfilename), 'w') as f:\n",
    "    f.write(report_train_balanced)  # Writing the classification report string to the file\n",
    "\n",
    "print(f\"Classification report saved to: {os.path.join(reports_output_dir, classification_report_outputfilename)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5ff88617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code snippet is used for second run of fraudtest.csv to read fraud_train columns, has if else"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20af2aa2",
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the previously saved feature columns from fraudTrain.csv\n",
    "data_processed_dir = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/data/processed'\n",
    "fraudTrain_columns_balanced = pd.read_csv(os.path.join(data_processed_dir, 'fraudTrain_columns_balanced.csv')).values.flatten()\n",
    "\n",
    "# Load fraudTest.csv (ensure it’s preprocessed the same way)\n",
    "X_fraudTest = df[selected_features]  # Select the same features as fraudTrain\n",
    "\n",
    "# One-hot encode fraudTest\n",
    "X_encoded_fraudTest = pd.get_dummies(X_fraudTest, drop_first=True)\n",
    "\n",
    "# Reindex fraudTest to ensure it has the same columns as fraudTrain\n",
    "X_encoded_fraudTest = X_encoded_fraudTest.reindex(columns=fraudTrain_columns_balanced, fill_value=0)\n",
    "\n",
    "# Impute missing values for fraudTest using the imputer trained on fraudTrain\n",
    "X_imputed_fraudTest = imputer.transform(X_encoded_fraudTest)\n",
    "\n",
    "# Use the already trained Logistic Regression Balanced model to make predictions on fraudTest\n",
    "y_pred_fraudTest = credit_card_fraud_detection_model_balanced.predict(X_imputed_fraudTest)\n",
    "\n",
    "# Evaluate the model on fraudTest\n",
    "y_true_fraudTest = df['is_fraud']  # True labels from fraudTest\n",
    "report_fraudTest_balanced = classification_report(y_true_fraudTest, y_pred_fraudTest)\n",
    "\n",
    "# Print the classification report\n",
    "print(report_fraudTest_balanced)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "950753b9",
   "metadata": {},
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Specify the output directory\n",
    "reports_output_dir = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/reports'\n",
    "\n",
    "# Create a dynamic filename\n",
    "classification_report_title = 'Logisitic Regression Balanced Classification Report'\n",
    "classification_report_outputfilename = f\"{classification_report_title.replace(' ', '_').replace(',', '')_{dataset_type}.lower()}.txt\"\n",
    "\n",
    "# Save the report to a text file\n",
    "with open(os.path.join(reports_output_dir, classification_report_outputfilename), 'w') as f:\n",
    "    f.write(report_fraudTest_balanced)  # Writing the classification report string to the file\n",
    "\n",
    "print(f\"Classification report saved to: {os.path.join(reports_output_dir, classification_report_outputfilename)}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "279d7784",
   "metadata": {},
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Specify the output directory\n",
    "output_dir_model = '/Users/sadhvichandragiri/desktop/coding/ZHAW_Project/ML_BigData_Repo_1/models'\n",
    "if not os.path.exists(output_dir_model):\n",
    "    os.makedirs(output_dir_model)  # Ensure the directory exists\n",
    "\n",
    "# Create a dynamic filename with .pkl extension\n",
    "model_title = 'Credit Fraud Detection Model Logistic Regression Balanced'\n",
    "outputfilename_model = f\"{dataset_type}_{model_title.replace(' ', '_').replace(',', '').lower()}.pkl\"\n",
    "\n",
    "# Assuming 'model' is your trained model object (e.g., Logistic Regression)\n",
    "with open(os.path.join(output_dir_model, outputfilename_model), 'wb') as model_file:\n",
    "    pickle.dump(credit_card_fraud_detection_model_balanced, model_file)\n",
    "\n",
    "print(f\"Model saved to {os.path.join(output_dir_model, outputfilename_model)}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c876626",
   "metadata": {},
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you've already trained Logistic Regression Balanced model (credit_card_fraud_detection_model_balanced)\n",
    "# and X_imputed contains the processed input features from fraudTrain.csv\n",
    "# Also assuming y contains the true labels (is_fraud) from fraudTrain.csv\n",
    "\n",
    "# Get predicted probabilities for the positive class (fraud)\n",
    "y_prob_train = credit_card_fraud_detection_model_balanced.predict_proba(X_imputed)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC for fraudTrain.csv\n",
    "roc_auc_train = roc_auc_score(y, y_prob_train)\n",
    "print(f\"ROC AUC for fraudTrain.csv: {roc_auc_train}\")\n",
    "\n",
    "# Plot ROC curve for fraudTrain.csv\n",
    "fpr, tpr, thresholds = roc_curve(y, y_prob_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"Logistic Regression Balanced (AUC = {roc_auc_train:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for fraudTrain.csv')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Save the plot to a file\n",
    "plot_filename = 'logistic regression_balanced_roc_curve_fraudTrain.png'\n",
    "plt.savefig(f\"{reports_output_dir}/{plot_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "38cb776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook ended at: Mon Oct 28 03:38:43 2024\n",
      "Total execution time: 585 minutes and 13.00 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Notebook ended at: {time.ctime(end_time)}\")\n",
    "print(f\"Total execution time: {elapsed_time // 60:.0f} minutes and {elapsed_time % 60:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bea59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ZHAW_Project)",
   "language": "python",
   "name": "zhaw_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
